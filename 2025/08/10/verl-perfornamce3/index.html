<!DOCTYPE html>
<html lang="zh-Hans">

<head>
  <meta http-equiv="content-type" content="text/html; charset=utf-8">
<meta name="x5-fullscreen" content="true">
<meta name="full-screen" content="yes">
<meta name="theme-color" content="#317EFB" />
<meta content="width=device-width, initial-scale=1.0, maximum-scale=5.0, user-scalable=0" name="viewport">
<meta name="description" content="目的对RL框架（Verl）中的LLM做性能分析.  统计MoE专家之间all-to-all通信耗时在推理\训练时间中占比，是否存在通信-计算重叠加速的空间。 统计多模态LLM训练时,ray中driver process和各rank之间dispatch与collect数据的时间占比.不过,跨&#x2F;同节点分开Ray 传输数据的方式不同,在同节点是共享内存传输，跨节点是 TCP，差异很大,目前仅为">
<meta property="og:type" content="article">
<meta property="og:title" content="verl性能分析（三）-实验分析">
<meta property="og:url" content="http://example.com/2025/08/10/verl-perfornamce3/index.html">
<meta property="og:site_name" content="Hexo">
<meta property="og:description" content="目的对RL框架（Verl）中的LLM做性能分析.  统计MoE专家之间all-to-all通信耗时在推理\训练时间中占比，是否存在通信-计算重叠加速的空间。 统计多模态LLM训练时,ray中driver process和各rank之间dispatch与collect数据的时间占比.不过,跨&#x2F;同节点分开Ray 传输数据的方式不同,在同节点是共享内存传输，跨节点是 TCP，差异很大,目前仅为">
<meta property="og:locale">
<meta property="og:image" content="http://example.com/img/404.jpg">
<meta property="og:image" content="http://example.com/img/404.jpg">
<meta property="og:image" content="http://example.com/img/404.jpg">
<meta property="og:image" content="http://example.com/img/404.jpg">
<meta property="article:published_time" content="2025-08-10T02:19:03.000Z">
<meta property="article:modified_time" content="2025-10-27T11:34:22.134Z">
<meta property="article:author" content="John Doe">
<meta property="article:tag" content="MOE">
<meta property="article:tag" content="Verl框架">
<meta property="article:tag" content="性能分析">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="http://example.com/img/404.jpg">

    <meta name="keywords" content="RLHF,Verl框架,性能分析">


<title >verl性能分析（三）-实验分析</title>

<!-- Favicon -->

    <link href='/img/favicon.svg?v=2.2.6' rel='icon' type='image/png' sizes='16x16' ></link>


    <link href='/img/favicon.svg?v=2.2.6' rel='icon' type='image/png' sizes='32x32' ></link>




<!-- Plugin -->




    
<link rel="stylesheet" href="/css/plugins/bootstrap.row.css">

    
<link rel="stylesheet" href="https://unpkg.com/@fancyapps/ui@4.0/dist/fancybox.css">

    
    




<!-- Icon -->

    
<link rel="stylesheet" href="/css/plugins/font-awesome.min.css">




<!-- Variable -->
<script>window.ASYNC_CONFIG = {"hostname":"example.com","author":"John Doe","root":"/","typed_text":null,"theme_version":"2.2.6","theme":{"switch":true,"default":"style-light"},"favicon":{"logo":"/img/favicon.svg","icon16":"/img/favicon.svg","icon32":"/img/favicon.svg","apple_touch_icon":null,"webmanifest":null,"visibilitychange":false,"hidden":"/failure.ico","show_text":"(/≧▽≦/)咦！又好了！","hide_text":"(●—●)喔哟，崩溃啦！"},"i18n":{"placeholder":"搜索文章...","empty":"找不到您查询的内容: ${query}","hits":"找到 ${hits} 条结果","hits_time":"找到 ${hits} 条结果（用时 ${time} 毫秒）","author":"本文作者：","copyright_link":"本文链接：","copyright_license_title":"版权声明：","copyright_license_content":"本博客所有文章除特别声明外，均默认采用 undefined 许可协议。","copy_success":"复制成功","copy_failure":"复制失败","open_read_mode":"进入阅读模式","exit_read_mode":"退出阅读模式","notice_outdate_message":"距离上次更新已经 undefined 天了, 文章内容可能已经过时。","sticky":"置顶","just":"刚刚","min":"分钟前","hour":"小时前","day":"天前","month":"个月前"},"swup":false,"plugin":{"flickr_justified_gallery":"https://unpkg.com/flickr-justified-gallery@latest/dist/fjGallery.min.js"},"icons":{"sun":"far fa-sun","moon":"far fa-moon","play":"fas fa-play","email":"far fa-envelope","next":"fas fa-arrow-right","calendar":"far fa-calendar-alt","clock":"far fa-clock","user":"far fa-user","back_top":"fas fa-arrow-up","close":"fas fa-times","search":"fas fa-search","reward":"fas fa-hand-holding-usd","toc_tag":"fas fa-th-list","read":"fas fa-book-reader","arrows":"fas fa-arrows-alt-h","double_arrows":"fas fa-angle-double-down","copy":"fas fa-copy"},"icontype":"font","highlight":{"plugin":"highlighjs","theme":true,"copy":true,"lang":true,"title":"default","height_limit":false},"toc":{"post_title":true},"live_time":{"start_time":"","prefix":"博客已萌萌哒运行 undefined 天"},"danmu":{"enable":false,"el":".trm-banner"}};</script>
<script id="async-page-config">window.PAGE_CONFIG = {"isPost":true,"isHome":false,"postUpdate":"2025-10-27 19:34:22"};</script>

<!-- Theme mode css -->
<link data-swup-theme rel="stylesheet" href="/css/index.css?v=2.2.6" id="trm-switch-style">
<script>
    let defaultMode = ASYNC_CONFIG.theme.default !=='auto' ?  ASYNC_CONFIG.theme.default : (window.matchMedia("(prefers-color-scheme: light)").matches ? 'style-light' : 'style-dark')
    let catchMode = localStorage.getItem('theme-mode') || defaultMode;
    let type = catchMode === 'style-dark' ? 'add' : 'remove';
    document.documentElement.classList[type]('dark')
</script>

<!-- CDN -->


    
    



<!-- Site Analytics -->

 
<meta name="generator" content="Hexo 7.3.0"></head>

<body>

  <!-- app wrapper -->
  <div class="trm-app-frame">

    <!-- page preloader -->
    <div class="trm-preloader">
    <div class="trm-holder">
        <div class="preloader">
            <div></div>
            <div></div>
            <div></div>
            <div></div>
            <div></div>
            <div></div>
            <div></div>
            <div></div>
            <div></div>
            <div></div>
        </div>
    </div>
</div>
    <!-- page preloader end -->

    <!-- change mode preloader -->
    <div class="trm-mode-swich-animation-frame">
    <div class="trm-mode-swich-animation">
        <i class="i-sun"><i class="iconfont far fa-sun"></i></i>
        <div class="trm-horizon"></div>
        <i class="i-moon"><i class="iconfont far fa-moon"></i></i>
    </div>
</div>
    <!-- change mode preloader end -->

      <!-- scroll container -->
      <div id="trm-dynamic-content" class="trm-swup-animation">
        <div id="trm-scroll-container" class="trm-scroll-container" style="opacity: 0">
            <!-- top bar -->
            <header class="trm-top-bar">
	<div class="container">
		<div class="trm-left-side">
			<!-- logo -->
<a href="/" class="trm-logo-frame trm-anima-link">
    
        <img alt="logo" src="/img/favicon.svg">
    
    
        <div class="trm-logo-text">
            Async<span>zhiqiang</span>
        </div>
    
</a>
<!-- logo end -->
		</div>
		<div class="trm-right-side">
			<!-- menu -->
<div class="trm-menu">
    <nav>
        <ul>
            
            <li class="menu-item-has-children ">
                <a  href="/" target="">
                    首页
                </a>
                
            </li>
            
            <li class="menu-item-has-children ">
                <a  href="/archives/" target="">
                    时间线
                </a>
                
            </li>
            
            <li class="menu-item-has-children ">
                <a  href="/tags/" target="">
                    标签
                </a>
                
            </li>
            
            <li class="menu-item-has-children ">
                <a  href="/categories" target="">
                    分类
                </a>
                
            </li>
            
        </ul>
    </nav>
</div>
<!-- menu end -->
			
    <!-- mode switcher place -->
    <div class="trm-mode-switcher-place">
        <div class="trm-mode-switcher">
            <i class="iconfont far fa-sun"></i>
            <input class="tgl tgl-light" id="trm-swich" type="checkbox">
            <label class="trm-swich" for="trm-swich"></label>
            <i class="iconfont far fa-moon"></i>
        </div>
    </div>
    <!-- mode switcher place end -->

			
		</div>
		<div class="trm-menu-btn">
			<span></span>
		</div>
	</div>
</header>
            <!-- top bar end -->

            <!-- body -->
            
<div class="trm-content-start">
    <!-- banner -->
    <div class="trm-banner">
    
    <!-- banner cover -->
    <img style="object-position:top;object-fit:cover;" alt="banner" class="trm-banner-cover" src="https://pic1.zhimg.com/v2-b3c2c6745b9421a13a3c4706b19223b3_r.jpg">
    <!-- banner cover end -->
    

    <!-- banner content -->
    <div class="trm-banner-content trm-overlay">
        <div class="container">
            <div class="row">
                
                <div class="col-lg-4"></div>
                
                <div class="col-lg-8">

                    <!-- banner title -->
                    <div class="trm-banner-text ">
                        <div class="trm-label trm-mb-20">
                            more paper, more fun
                        </div>
                        <h1 class="trm-mb-30 trm-hsmb-font">
                            verl性能分析（三）-实验分析
                        </h1>

                        
                            <ul class="trm-breadcrumbs trm-label">
                                <li>
                                    <a href="/" class="trm-anima-link">Home</a>
                                </li>
                                <li>
                                    <span>
                                        2025
                                    </span>
                                </li>
                            </ul>
                        
                    </div>
                    <!-- banner title end -->

                    <!-- scroll hint -->
                    <span id="scroll-triger" class="trm-scroll-hint-frame">
                        <div class="trm-scroll-hint"></div>
                        <span class="trm-label">Scroll down</span>
                    </span>
                    <!-- scroll hint end -->

                </div>
            </div>
        </div>
    </div>
    <!-- banner content end -->
</div>
    <!-- banner end -->
    <div class="container">
        <div class="row">
            
                <div class="trm-page-sidebar col-lg-4 hidden-sm">
                    <!-- main card -->
                    <div class="trm-main-card-frame trm-sidebar">
    <div class="trm-main-card"> 
        <!-- card header -->
<div class="trm-mc-header">
    <div class="trm-avatar-frame trm-mb-20">
        <img alt="Avatar" class="trm-avatar" src="/img/avatar.jpg">
    </div>
    <h5 class="trm-name trm-mb-15">
        看雪
    </h5>
    
</div>
<!-- card header end -->
        <!-- sidebar social -->

<div class="trm-divider trm-mb-40 trm-mt-40"></div>
<div class="trm-social">
    
        <a href="https://github.com" title="Github" rel="nofollow" target="_blank">
            <i class="iconfont fab fa-github"></i>
        </a>
    
</div>

<!-- sidebar social end -->
        <!-- info -->
<div class="trm-divider trm-mb-40 trm-mt-40"></div>
<ul class="trm-table trm-mb-20">
    
        <li>
            <div class="trm-label">
                Residence:
            </div>
            <div class="trm-label trm-label-light">
                Mars
            </div>
        </li>
    
</ul>
<!-- info end -->

        
    </div>
</div>
                    <!-- main card end -->
                </div>
            
            <div class="trm-page-content col-lg-8">
                <div id="trm-content" class="trm-content">
                    <div class="trm-post-info row hidden-sm">
    <div class="col-sm-4">
        <div class="trm-card trm-label trm-label-light text-center">
            <i class="iconfont far fa-calendar-alt trm-icon"></i><br>
            08/10
        </div>
    </div>
    <div class="col-sm-4">
        <div class="trm-card trm-label trm-label-light text-center">
            <i class="iconfont far fa-clock trm-icon"></i><br>
            10:19
        </div>
    </div>
    <div class="col-sm-4">
        <div id="post-author" class="trm-card trm-label trm-label-light text-center">
            <i class="iconfont far fa-user trm-icon"></i><br>
            John Doe
        </div>
    </div>
</div>
<div class="trm-card ">
    <article id="article-container" class="trm-publication">
    <h1 id="目的"><a href="#目的" class="headerlink" title="目的"></a>目的</h1><p>对RL框架（Verl）中的LLM做性能分析.</p>
<ol>
<li>统计MoE专家之间all-to-all通信耗时在推理\训练时间中占比，是否存在通信-计算重叠加速的空间。</li>
<li>统计多模态LLM训练时,ray中driver process和各rank之间dispatch与collect数据的时间占比.<br>不过,跨&#x2F;同节点分开Ray 传输数据的方式不同,在同节点是共享内存传输，跨节点是 TCP，差异很大,目前仅为单机8卡实验.</li>
</ol>
<h1 id="前置工作"><a href="#前置工作" class="headerlink" title="前置工作"></a>前置工作</h1><ol>
<li>选择合适的MoE与多模态模型.</li>
<li>wrap VLLM和Megatron中Export之间通信函数;wrap ray dispatch和collect数据的函数.</li>
<li>nsys-rep文件转sqlite或json,脚本分析wrap后插桩获取的数据.</li>
</ol>
<h1 id="实验分析"><a href="#实验分析" class="headerlink" title="实验分析"></a>实验分析</h1><h2 id="多模态"><a href="#多模态" class="headerlink" title="多模态"></a>多模态</h2><h3 id="简短结论"><a href="#简短结论" class="headerlink" title="简短结论"></a>简短结论</h3><p>与纯文本数据相比,多模态(本实验为image+text)显然会造成driver process与各rank的通信时间增大.</p>
<h3 id="实验环境"><a href="#实验环境" class="headerlink" title="实验环境"></a>实验环境</h3><ul>
<li>硬件:8*A6000-t2(49G)</li>
<li>框架:Verl0.5</li>
<li>模型:<strong>Qwen2.5-VL-7B-Instruct</strong></li>
<li>数据集:geo3k</li>
<li>推理：actor: VLLM，(dp,tp,pp)&#x3D;(2,4,1)</li>
<li>训练：actor: FSDP,(dp,tp,pp)&#x3D;(8,1,1)</li>
<li>batch size: 256</li>
<li>放置方式：actor、reference model和rollout model共置, 一起放在8张卡上</li>
</ul>
<h3 id="timeline分析"><a href="#timeline分析" class="headerlink" title="timeline分析"></a>timeline分析</h3><p>记录driver process在step2的timeline:</p>
<table>
<thead>
<tr>
<th>阶段</th>
<th>耗时(s)</th>
<th>占比 (%)</th>
</tr>
</thead>
<tbody><tr>
<td><strong>step</strong></td>
<td>277.9</td>
<td><strong>100.00</strong></td>
</tr>
<tr>
<td>├─ <strong>rollout</strong></td>
<td>83.00</td>
<td><strong>29.87</strong></td>
</tr>
<tr>
<td>│  ├─ dispatch</td>
<td>4e-3</td>
<td>忽略</td>
</tr>
<tr>
<td>│  ├─ submit_execute 即(ray.remote())</td>
<td>0.36</td>
<td>0.13</td>
</tr>
<tr>
<td>│  ├─ <strong>get_output</strong> 即(ray.get())</td>
<td>82.48</td>
<td><strong>29.68</strong></td>
</tr>
<tr>
<td>│  └─ collect</td>
<td>0.16</td>
<td>0.06</td>
</tr>
<tr>
<td>│     reward</td>
<td>1.31</td>
<td>0.5</td>
</tr>
<tr>
<td>├─ old_log_prob</td>
<td>30.01</td>
<td>10.8</td>
</tr>
<tr>
<td>│  ├─ dispatch</td>
<td>6e-3</td>
<td>忽略</td>
</tr>
<tr>
<td>│  ├─ submit_execute</td>
<td>4.25</td>
<td>1.5</td>
</tr>
<tr>
<td>│  ├─ get_output</td>
<td>25.72</td>
<td>9.3</td>
</tr>
<tr>
<td>│  └─ collect</td>
<td>3e-3</td>
<td>忽略</td>
</tr>
<tr>
<td>├─ ref</td>
<td>30.63</td>
<td>11.02</td>
</tr>
<tr>
<td>│  ├─ dispatch</td>
<td>7e-3</td>
<td>忽略</td>
</tr>
<tr>
<td>│  ├─ submit_execute</td>
<td>4.69</td>
<td>1.69</td>
</tr>
<tr>
<td>│  ├─ get_output</td>
<td>25.93</td>
<td>9.33</td>
</tr>
<tr>
<td>│  └─ collect</td>
<td>2e-3</td>
<td>忽略</td>
</tr>
<tr>
<td>├─ adv</td>
<td>0.55</td>
<td>0.2</td>
</tr>
<tr>
<td>├─ <strong>actor_train</strong></td>
<td>132.76</td>
<td><strong>47.78</strong></td>
</tr>
<tr>
<td>│  ├─ dispatch</td>
<td>8e-3</td>
<td>忽略</td>
</tr>
<tr>
<td>│  ├─ submit_execute</td>
<td>4.53</td>
<td>1.6</td>
</tr>
<tr>
<td>│  ├─ <strong>get_output</strong></td>
<td>128.20</td>
<td><strong>46.13</strong></td>
</tr>
<tr>
<td>│  └─ collect</td>
<td>4e-3</td>
<td>忽略</td>
</tr>
</tbody></table>
<p>driver process中dispatch和collect的时间都非常小，不过他们只表示实际数据发生传输前后driver process对数据split和concat的时间，整体数据流向如下：</p>
<p><img src="/2025/08/10/verl-perfornamce3/Users\25490\Desktop\blog\source_posts\verl-perfornamce3\call_generate_sequences.png" alt="call_generate_sequences"  data-tag='post-image' onload='this.onload=null;this.style.opacity=1;' loading="lazy" onerror='this.onerror=null;this.src="/img/404.jpg"'></p>
<p>如果把每个stage整体分成计算和数据传输两部分，那么数据传输时间是driver process上记录各stage执行时间和各rank本身所记录该阶段执行时间的差值.</p>
<p>则以rank1,rank 3和rank 7在step2为例(batch_size&#x3D;256)，多模态rank与driver process通信耗时如下:</p>
<table>
<thead>
<tr>
<th>阶段</th>
<th>rank 1计算(s)</th>
<th>rank 3计算(s)</th>
<th>rank 7计算(s)</th>
<th>rank 1通信(s)</th>
<th>rank 3 通信(s)</th>
<th>rank 7 通信(s)</th>
<th>平均通信(s)</th>
</tr>
</thead>
<tbody><tr>
<td>rollout</td>
<td>82.37</td>
<td>82.33</td>
<td>82.15</td>
<td>0.63</td>
<td>0.67</td>
<td>0.85</td>
<td><strong>0.71</strong></td>
</tr>
<tr>
<td>reward</td>
<td>null</td>
<td>null</td>
<td>null</td>
<td>null</td>
<td>null</td>
<td>null</td>
<td>null</td>
</tr>
<tr>
<td>old_log_prob</td>
<td>28.61</td>
<td>27.64</td>
<td>25.40</td>
<td>1.40</td>
<td>2.37</td>
<td>4.61</td>
<td><strong>2.79</strong></td>
</tr>
<tr>
<td>ref_prob</td>
<td>28.79</td>
<td>27.95</td>
<td>25.58</td>
<td>1.84</td>
<td>2.68</td>
<td>4.43</td>
<td><strong>2.98</strong></td>
</tr>
<tr>
<td>adv</td>
<td>null</td>
<td>null</td>
<td>null</td>
<td>null</td>
<td>null</td>
<td>null</td>
<td>null</td>
</tr>
<tr>
<td>actor_train</td>
<td>131.27</td>
<td>130.28</td>
<td>129.31</td>
<td>1.01</td>
<td>2.48</td>
<td>3.45</td>
<td><strong>2.31</strong></td>
</tr>
</tbody></table>
<h3 id="与纯文本耗时比较"><a href="#与纯文本耗时比较" class="headerlink" title="与纯文本耗时比较"></a>与纯文本耗时比较</h3><h4 id="Qwen2-5-VL-7B-Instruct"><a href="#Qwen2-5-VL-7B-Instruct" class="headerlink" title="Qwen2.5-VL-7B-Instruct"></a>Qwen2.5-VL-7B-Instruct</h4><ul>
<li>硬件:8*A6000-t2(49G)</li>
<li>框架:Verl0.5</li>
<li>数据集:gsm8k</li>
<li>推理：VLLM，(dp,tp,pp)&#x3D;(2,4,1)</li>
<li>训练：FSDP,(dp,tp,pp)&#x3D;(1,8,1)</li>
<li>batch size: 256</li>
<li>放置方式：actor、reference model和rollout model共置, 一起放在8张卡上</li>
</ul>
<p>纯文本通信耗时如下):</p>
<table>
<thead>
<tr>
<th>阶段</th>
<th>rank 1计算(s)</th>
<th>rank 3计算(s)</th>
<th>rank 7计算(s)</th>
<th>rank 1通信(s)</th>
<th>rank 3 通信(s)</th>
<th>rank 7 通信(s)</th>
<th>平均通信(s)</th>
</tr>
</thead>
<tbody><tr>
<td>rollout</td>
<td>44.88</td>
<td>44.87</td>
<td>44.84</td>
<td>0.21</td>
<td>0.22</td>
<td>0.25</td>
<td><strong>0.23</strong></td>
</tr>
<tr>
<td>reward</td>
<td>null</td>
<td>null</td>
<td>null</td>
<td>null</td>
<td>null</td>
<td>null</td>
<td>null</td>
</tr>
<tr>
<td>old_log_prob</td>
<td>14.55</td>
<td>14.53</td>
<td>14.46</td>
<td>0.09</td>
<td>0.11</td>
<td>0.18</td>
<td><strong>0.13</strong></td>
</tr>
<tr>
<td>ref_prob</td>
<td>15.22</td>
<td>15.17</td>
<td>15.05</td>
<td>0.15</td>
<td>0.20</td>
<td>0.32</td>
<td><strong>0.22</strong></td>
</tr>
<tr>
<td>adv</td>
<td>null</td>
<td>null</td>
<td>null</td>
<td>null</td>
<td>null</td>
<td>null</td>
<td>null</td>
</tr>
<tr>
<td>actor_train</td>
<td>84.50</td>
<td>84.49</td>
<td>84.37</td>
<td>0.15</td>
<td>0.16</td>
<td>0.28</td>
<td><strong>0.20</strong></td>
</tr>
</tbody></table>
<ul>
<li><p>gsm8k数据集:各stage平均通信时间约<strong>0.2</strong>s.</p>
</li>
<li><p>geo3k数据集:,各stage平均通信时间约<strong>2.6</strong>s.</p>
</li>
<li></li>
</ul>
<h4 id="Deepseek-7b-chat"><a href="#Deepseek-7b-chat" class="headerlink" title="Deepseek-7b-chat"></a>Deepseek-7b-chat</h4><ul>
<li>硬件:8*A6000-t2(49G)</li>
<li>框架:Verl0.5</li>
<li>数据集:gsm8k</li>
<li>推理：VLLM，(dp,tp,pp)&#x3D;(4,2,1)</li>
<li>训练：FSDP,(dp,tp,pp)&#x3D;(8,1,1)</li>
<li>batch size: 1024</li>
<li>放置方式：actor、reference model和rollout model共置, 一起放在8张卡上</li>
</ul>
<p>纯文本通信耗时如下(batch_size&#x3D;1024):</p>
<table>
<thead>
<tr>
<th>阶段</th>
<th>rank 1计算(s)</th>
<th>rank 3计算(s)</th>
<th>rank 7计算(s)</th>
<th>rank 1通信(s)</th>
<th>rank 3 通信(s)</th>
<th>rank 7 通信(s)</th>
<th>平均通信(s)</th>
</tr>
</thead>
<tbody><tr>
<td>rollout</td>
<td>90.43</td>
<td>90.39</td>
<td>90.33</td>
<td>0.56</td>
<td>0.60</td>
<td>0.66</td>
<td><strong>0.61</strong></td>
</tr>
<tr>
<td>reward</td>
<td>null</td>
<td>null</td>
<td>null</td>
<td>null</td>
<td>null</td>
<td>null</td>
<td>null</td>
</tr>
<tr>
<td>old_log_prob</td>
<td>30.08</td>
<td>29.90</td>
<td>29.49</td>
<td>0.43</td>
<td>0.61</td>
<td>1.02</td>
<td><strong>0.69</strong></td>
</tr>
<tr>
<td>ref_prob</td>
<td>31.02</td>
<td>30.83</td>
<td>30.34</td>
<td>0.41</td>
<td>0.60</td>
<td>1.09</td>
<td><strong>0.70</strong></td>
</tr>
<tr>
<td>adv</td>
<td>null</td>
<td>null</td>
<td>null</td>
<td>null</td>
<td>null</td>
<td>null</td>
<td>null</td>
</tr>
<tr>
<td>actor_train</td>
<td>122.67</td>
<td>122.40</td>
<td>121.92</td>
<td>0.38</td>
<td>0.65</td>
<td>1.13</td>
<td><strong>0.72</strong></td>
</tr>
</tbody></table>
<ul>
<li><p>gsm8k数据集:在batch_size&#x3D;1024下,各stage平均通信时间约<strong>0.7</strong>s.</p>
</li>
<li><p>geo3k数据集:在batch_size&#x3D;256下,各stage平均通信时间约<strong>2.6</strong>s.</p>
</li>
</ul>
<h3 id="实验日志"><a href="#实验日志" class="headerlink" title="实验日志"></a><del>实验日志</del></h3><ul>
<li><del>2025&#x2F;08&#x2F;9:分析verl中ray交互数据原理.</del></li>
<li><del>2025&#x2F;08&#x2F;10:直接跑官方样例不通,降低tansformers版本到4.52.4试一试(4.53是不行的,不支持),降低版本(4.52.4)后可以跑了,但是ray 的actor莫名其妙挂掉了,尝试减少batch size看看是不是OOM的问题,看起来是的,但是还是跑不成功,内存太小了.换了500G内存后就OK了.</del></li>
<li><del>2025&#x2F;08&#x2F;11:使用Nsight计算时间,计算传输数据时间占比,是否为bottleneck，写文档.</del></li>
</ul>
<h2 id="MoE"><a href="#MoE" class="headerlink" title="MoE"></a>MoE</h2><h3 id="问题"><a href="#问题" class="headerlink" title="问题"></a>问题</h3><p>实验测得的时间偏小：<br>单机：平均0.x ms，耗时总占比0.1%，占moe层前向计算3.7%~7.7%时间。<br>多机：平均比单机长15%，耗时总占比0.1%，占moe层前向计算3.2%时间。</p>
<h3 id="实验1"><a href="#实验1" class="headerlink" title="实验1"></a>实验1</h3><ul>
<li>硬件:<strong><em>8</em>A6000</strong></li>
<li>框架:Verl0.5</li>
<li>模型:<strong>qwen1.5-moe-a2.7b-chat</strong>，隐藏层仅保留2层</li>
<li>数据集:gsm8k</li>
<li>推理：VLLM (dp,tp,pp)&#x3D;(2,4,1)</li>
<li>训练：Megatron (dp,tp,ep,pp,cp)&#x3D;(1,2,4,1,1)</li>
<li>batch size: 1024</li>
<li>placement：colocate</li>
</ul>
<table>
<thead>
<tr>
<th>Time (%)</th>
<th>Total Time (ns)</th>
<th>Instances</th>
<th>Avg (ns)</th>
<th>Med (ns)</th>
<th>Min (ns)</th>
<th>Max (ns)</th>
<th>StdDev (ns)</th>
<th>Range</th>
</tr>
</thead>
<tbody><tr>
<td>31.5</td>
<td>395646330645</td>
<td>2</td>
<td>197823165322.5</td>
<td>197823165322.5</td>
<td>193885690017</td>
<td>201760640628</td>
<td>5568430978.5</td>
<td>:generate_sequences_6</td>
</tr>
<tr>
<td>30.5</td>
<td>382438458508</td>
<td>2</td>
<td>191219229254.0</td>
<td>191219229254.0</td>
<td>187562464316</td>
<td>194875994192</td>
<td>5171446569.7</td>
<td>:inference_engine</td>
</tr>
<tr>
<td>14.5</td>
<td>181888980677</td>
<td>768</td>
<td>236834610.3</td>
<td>228848954.0</td>
<td>150249507</td>
<td>712077406</td>
<td>70626783.4</td>
<td>:forward_step</td>
</tr>
<tr>
<td>8.2</td>
<td>102610332737</td>
<td>2</td>
<td>51305166368.5</td>
<td>51305166368.5</td>
<td>51242657680</td>
<td>51367675057</td>
<td>88400635.0</td>
<td>:update_actor_6</td>
</tr>
<tr>
<td>5.2</td>
<td>65872338896</td>
<td>2</td>
<td>32936169448.0</td>
<td>32936169448.0</td>
<td>32871899089</td>
<td>33000439807</td>
<td>90892013.4</td>
<td>:compute_log_prob_6</td>
</tr>
<tr>
<td>3.7</td>
<td>45889879653</td>
<td>2</td>
<td>22944939826.5</td>
<td>22944939826.5</td>
<td>22838427151</td>
<td>23051452502</td>
<td>150631670.3</td>
<td>:compute_ref_log_prob_6</td>
</tr>
<tr>
<td>2.8</td>
<td>34774562967</td>
<td>1536</td>
<td>22639689.4</td>
<td>29509380.5</td>
<td>7971534</td>
<td>231777397</td>
<td>15465139.2</td>
<td>:fwd_attention</td>
</tr>
<tr>
<td>1.3</td>
<td>16311177012</td>
<td>1536</td>
<td>10619255.9</td>
<td>7856696.0</td>
<td>6325167</td>
<td>487785488</td>
<td>24982474.1</td>
<td>:fwd_mlp</td>
</tr>
<tr>
<td>1.1</td>
<td>13644285403</td>
<td>256</td>
<td>53297989.9</td>
<td>52714961.0</td>
<td>40110961</td>
<td>114295853</td>
<td>6262934.9</td>
<td>:backward_step</td>
</tr>
<tr>
<td>0.6</td>
<td>7876402942</td>
<td>1536</td>
<td>5127866.5</td>
<td>2116981.0</td>
<td>1780915</td>
<td>481800619</td>
<td>24910408.6</td>
<td>:fwd_router_and_preprocess</td>
</tr>
<tr>
<td>0.5</td>
<td>5740326034</td>
<td>1536</td>
<td></td>
<td>3589240.0</td>
<td>2938000</td>
<td>16967258</td>
<td>707158.8</td>
<td>:fwd_experts_compute</td>
</tr>
<tr>
<td>0.1</td>
<td>1454034748</td>
<td>1536</td>
<td>946637.2</td>
<td>927889.5</td>
<td>815089</td>
<td>4490748</td>
<td>143396.2</td>
<td>:fwd_combine</td>
</tr>
<tr>
<td>0.1</td>
<td>994471923</td>
<td>6144</td>
<td>161860.7</td>
<td>155357.5</td>
<td>106186</td>
<td>3540048</td>
<td>62969.4</td>
<td>:all_to_all</td>
</tr>
<tr>
<td>0.0</td>
<td>545049826</td>
<td>1536</td>
<td>354850.1</td>
<td>349921.5</td>
<td>305206</td>
<td>1367220</td>
<td>50939.2</td>
<td>:fwd_dispatch</td>
</tr>
</tbody></table>
<ul>
<li>max_prompt_length&#x3D;512，max_response_length&#x3D;1024</li>
<li>ppo_micro_batch_size_per_gpu&#x3D;10</li>
<li>hidden_size: 2048</li>
<li>num_experts: 60</li>
<li>num_experts_per_tok: 4</li>
<li>ep&#x3D;4 （专家个数为60，ep最大可设置4）</li>
</ul>
<p>平均每个rank需要接收和发送的数据量为((512+1024))$\times$10)$\times$2048)$\times$4)$\times$2)$\times$(1-1&#x2F;ep)&#x3D;180MB。<br>nvlink4带宽大概300GB&#x2F;s，则每次耗时约0.6ms，实际测得平均0.17ms，总耗时占比0.1%，mlp耗时占比7.7%</p>
<h3 id="实验2"><a href="#实验2" class="headerlink" title="实验2"></a>实验2</h3><ul>
<li>硬件:8*A6000</li>
<li>框架:Verl0.5</li>
<li>模型:<strong>Qwen3-30B-A3B-Instruct-2507</strong>，隐藏层仅保留2层</li>
<li>数据集:gsm8k</li>
<li>推理：VLLM (dp,tp,pp)&#x3D;(2,4,1)</li>
<li>训练：Megatron (dp,tp,ep,pp,cp)&#x3D;(1,1,8,1,1)</li>
<li>batch size: 1024</li>
<li>placement：colocate</li>
</ul>
<table>
<thead>
<tr>
<th>Time (%)</th>
<th>Total Time (ns)</th>
<th>Instances</th>
<th>Avg (ns)</th>
<th>Med (ns)</th>
<th>Min (ns)</th>
<th>Max (ns)</th>
<th>StdDev (ns)</th>
<th>Range</th>
</tr>
</thead>
<tbody><tr>
<td>27.3</td>
<td>183931773529</td>
<td>2</td>
<td>91965886764.5</td>
<td>91965886764.5</td>
<td>89021277229</td>
<td>94910496300</td>
<td>4164306741.0</td>
<td>:generate_sequences_4</td>
</tr>
<tr>
<td>23.9</td>
<td>160554373024</td>
<td>2</td>
<td>80277186512.0</td>
<td>80277186512.0</td>
<td>80013204041</td>
<td>80541168983</td>
<td>373327590.7</td>
<td>:inference_engine</td>
</tr>
<tr>
<td>17.1</td>
<td>115144190522</td>
<td>384</td>
<td>299854662.8</td>
<td>284892385.5</td>
<td>141471850</td>
<td>747238401</td>
<td>99946826.6</td>
<td>:forward_step</td>
</tr>
<tr>
<td>10.8</td>
<td>72604010163</td>
<td>2</td>
<td>36302005081.5</td>
<td>36302005081.5</td>
<td>35970823362</td>
<td>36633186801</td>
<td>468361679.3</td>
<td>:update_actor_4</td>
</tr>
<tr>
<td>6.8</td>
<td>45930349980</td>
<td>2</td>
<td>22965174990.0</td>
<td>22965174990.0</td>
<td>22317069885</td>
<td>23613280095</td>
<td>916559029.3</td>
<td>:compute_log_prob_4</td>
</tr>
<tr>
<td>4.5</td>
<td>30518993851</td>
<td>2</td>
<td>15259496925.5</td>
<td>15259496925.5</td>
<td>15087346599</td>
<td>15431647252</td>
<td>243457326.5</td>
<td>:compute_ref_log_prob_4</td>
</tr>
<tr>
<td>3.3</td>
<td>21948225162</td>
<td>768</td>
<td>28578418.2</td>
<td>26472111.0</td>
<td>6847250</td>
<td>57825589</td>
<td>20513042.6</td>
<td>:fwd_attention</td>
</tr>
<tr>
<td>2.7</td>
<td>17940871362</td>
<td>768</td>
<td>23360509.6</td>
<td>8758587.0</td>
<td>4786855</td>
<td>501779971</td>
<td>52617223.5</td>
<td>:fwd_mlp</td>
</tr>
<tr>
<td>2.2</td>
<td>14832590836</td>
<td>768</td>
<td>19313269.3</td>
<td>4739424.5</td>
<td>1455464</td>
<td>496123484</td>
<td>52420456.9</td>
<td>:fwd_router_and_preprocess</td>
</tr>
<tr>
<td>1.0</td>
<td>6423416035</td>
<td>128</td>
<td>50182937.8</td>
<td>48544096.0</td>
<td>29980927</td>
<td>253409675</td>
<td>19474015.2</td>
<td>:backward_step</td>
</tr>
<tr>
<td>0.3</td>
<td>2203125754</td>
<td>768</td>
<td>2868653.3</td>
<td>2783709.0</td>
<td>2212154</td>
<td>13441155</td>
<td>580323.9</td>
<td>:fwd_experts_compute</td>
</tr>
<tr>
<td>0.1</td>
<td>573465484</td>
<td>3072</td>
<td>186675.0</td>
<td>176568.0</td>
<td>110583</td>
<td>3137688</td>
<td>81770.7</td>
<td>:all_to_all</td>
</tr>
<tr>
<td>0.0</td>
<td>295239274</td>
<td>768</td>
<td>384426.1</td>
<td>371658.0</td>
<td>319277</td>
<td>1481166</td>
<td>82422.6</td>
<td>:fwd_dispatch</td>
</tr>
<tr>
<td>0.0</td>
<td>250625077</td>
<td>768</td>
<td>326334.7</td>
<td>321104.5</td>
<td>260302</td>
<td>3279347</td>
<td>109431.7</td>
<td>:fwd_combine</td>
</tr>
</tbody></table>
<ul>
<li>max_prompt_length&#x3D;512，max_response_length&#x3D;1024</li>
<li>ppo_micro_batch_size_per_gpu&#x3D;10</li>
<li>hidden_size: 2048</li>
<li>num_experts: 128</li>
<li>num_experts_per_tok: 8</li>
<li>ep&#x3D;8</li>
</ul>
<p>总数据量约为((512+1024)$\times$10)$\times$2048)$\times$8)$\times$2)$\times$(1-1&#x2F;ep)&#x3D;420MB<br>nvlink4带宽大概300GB&#x2F;s，则每次耗时约1.4ms，实际测得平均0.19ms，总耗时占比0.1%，mlp耗时占比3.7%</p>
<h3 id="实验3"><a href="#实验3" class="headerlink" title="实验3"></a>实验3</h3><ul>
<li>硬件:16<em>A6000,2</em>nodes</li>
<li>框架:Verl0.5</li>
<li>模型:<strong>Qwen3-30B-A3B-Instruct-2507</strong>，隐藏层仅保留2层</li>
<li>数据集:gsm8k</li>
<li>推理：VLLM (dp,tp,pp)&#x3D;(4,4,1)</li>
<li>训练：Megatron (dp,tp,ep,pp,cp)&#x3D;(1,1,16,1,1)</li>
<li>batch size: 1024</li>
<li>placement：colocate</li>
</ul>
<table>
<thead>
<tr>
<th>Time (%)</th>
<th>Total Time (ns)</th>
<th>Instances</th>
<th>Avg (ns)</th>
<th>Med (ns)</th>
<th>Min (ns)</th>
<th>Max (ns)</th>
<th>StdDev (ns)</th>
<th>Range</th>
</tr>
</thead>
<tbody><tr>
<td>25.1</td>
<td>97456412468</td>
<td>2</td>
<td>48728206234.0</td>
<td>48728206234.0</td>
<td>48138730440</td>
<td>49317682028</td>
<td>833644662.6</td>
<td>:generate_sequences_13</td>
</tr>
<tr>
<td>20.5</td>
<td>79564308142</td>
<td>2</td>
<td>39782154071.0</td>
<td>39782154071.0</td>
<td>38985379873</td>
<td>40578928269</td>
<td>1126808877.0</td>
<td>:inference_engine</td>
</tr>
<tr>
<td>18.0</td>
<td>69824887209</td>
<td>192</td>
<td>363671287.5</td>
<td>334106747.0</td>
<td>209434312</td>
<td>782124379</td>
<td>117800779.4</td>
<td>:forward_step</td>
</tr>
<tr>
<td>12.6</td>
<td>48908072135</td>
<td>2</td>
<td>24454036067.5</td>
<td>24454036067.5</td>
<td>24415895488</td>
<td>24492176647</td>
<td>53938924.8</td>
<td>:update_actor_13</td>
</tr>
<tr>
<td>7.9</td>
<td>30746822962</td>
<td>2</td>
<td>15373411481.0</td>
<td>15373411481.0</td>
<td>14943811298</td>
<td>15803011664</td>
<td>607546405.2</td>
<td>:compute_log_prob_13</td>
</tr>
<tr>
<td>5.1</td>
<td>19698532686</td>
<td>2</td>
<td>9849266343.0</td>
<td>9849266343.0</td>
<td>9829265553</td>
<td>9869267133</td>
<td>28285388.5</td>
<td>:compute_ref_log_prob_13</td>
</tr>
<tr>
<td>3.8</td>
<td>14605295656</td>
<td>384</td>
<td>38034624.1</td>
<td>36246496.5</td>
<td>7100670</td>
<td>76476830</td>
<td>29830573.6</td>
<td>:fwd_attention</td>
</tr>
<tr>
<td>3.1</td>
<td>12227337036</td>
<td>384</td>
<td>31842023.5</td>
<td>14573901.0</td>
<td>4690706</td>
<td>494206604</td>
<td>61375832.9</td>
<td>:fwd_mlp</td>
</tr>
<tr>
<td>2.8</td>
<td>10747121868</td>
<td>384</td>
<td>27987296.5</td>
<td>10773530.0</td>
<td>1481889</td>
<td>489889950</td>
<td>61242039.6</td>
<td>:fwd_router_and_preprocess</td>
</tr>
<tr>
<td>0.8</td>
<td>3111567213</td>
<td>64</td>
<td>48618237.7</td>
<td>47743969.0</td>
<td>42087545</td>
<td>69646649</td>
<td>4001517.2</td>
<td>:backward_step</td>
</tr>
<tr>
<td>0.3</td>
<td>989585055</td>
<td>384</td>
<td>2577044.4</td>
<td>2437471.5</td>
<td>1983947</td>
<td>9939923</td>
<td>727446.5</td>
<td>:fwd_experts_compute</td>
</tr>
<tr>
<td>0.1</td>
<td>339264004</td>
<td>1536</td>
<td>220875.0</td>
<td>200823.0</td>
<td>131437</td>
<td>1022218</td>
<td>80212.6</td>
<td>:all_to_all</td>
</tr>
<tr>
<td>0.0</td>
<td>170317337</td>
<td>384</td>
<td>443534.7</td>
<td>423148.0</td>
<td>359405</td>
<td>1321812</td>
<td>97124.4</td>
<td>:fwd_dispatch</td>
</tr>
<tr>
<td>0.0</td>
<td>137372603</td>
<td>384</td>
<td>357741.2</td>
<td>344486.5</td>
<td>291652</td>
<td>1116278</td>
<td>79440.5</td>
<td>:fwd_combine</td>
</tr>
</tbody></table>
<ul>
<li>max_prompt_length&#x3D;512，max_response_length&#x3D;1024</li>
<li>ppo_micro_batch_size_per_gpu&#x3D;10</li>
<li>hidden_size: 2048</li>
<li>num_experts: 128</li>
<li>num_experts_per_tok: 8</li>
<li>ep&#x3D;16</li>
</ul>
<p>总数据量约为((512+1024)$\times$10)$\times$2048)$\times$8)$\times$2)$\times$(1-1&#x2F;ep)&#x3D;450MB<br>nvlink4带宽大概300GB&#x2F;s，IB带宽9~16GB&#x2F;s，则每次耗时约22.5ms，实际测得平均0.22ms，总耗时占比0.1%，mlp耗时占比3.2%</p>
<h3 id="实验4"><a href="#实验4" class="headerlink" title="实验4"></a>实验4</h3><ul>
<li>硬件:16<em>A6000,2</em>nodes</li>
<li>框架:Verl0.5</li>
<li>模型:<strong>qwen1.5-moe-a2.7b-chat</strong>，隐藏层仅保留2层</li>
<li>数据集:gsm8k</li>
<li>推理：VLLM (dp,tp,pp)&#x3D;(4,4,1)</li>
<li>训练：Megatron (dp,tp,ep,pp,cp)&#x3D;(1,4,4,1,1)</li>
<li>batch size: 1024</li>
<li>placement：colocate</li>
</ul>
<table>
<thead>
<tr>
<th>Time (%)</th>
<th>Total Time (ns)</th>
<th>Instances</th>
<th>Avg (ns)</th>
<th>Med (ns)</th>
<th>Min (ns)</th>
<th>Max (ns)</th>
<th>StdDev (ns)</th>
<th>Range</th>
</tr>
</thead>
<tbody><tr>
<td>27.1</td>
<td>202588819762</td>
<td>2</td>
<td>101294409881.0</td>
<td>101294409881.0</td>
<td>99414899608</td>
<td>103173920154</td>
<td>2658028918.7</td>
<td>:generate_sequences_15</td>
</tr>
<tr>
<td>25.5</td>
<td>190411641677</td>
<td>2</td>
<td>95205820838.5</td>
<td>95205820838.5</td>
<td>94441406174</td>
<td>95970235503</td>
<td>1081045585.8</td>
<td>:inference_engine</td>
</tr>
<tr>
<td>16.7</td>
<td>125029407224</td>
<td>768</td>
<td>162798707.3</td>
<td>156371393.0</td>
<td>111778707</td>
<td>243407500</td>
<td>33373805.6</td>
<td>:forward_step</td>
</tr>
<tr>
<td>10.1</td>
<td>75791956861</td>
<td>2</td>
<td>37895978430.5</td>
<td>37895978430.5</td>
<td>37808544227</td>
<td>37983412634</td>
<td>123650636.4</td>
<td>:update_actor_15</td>
</tr>
<tr>
<td>5.8</td>
<td>43302815735</td>
<td>2</td>
<td>21651407867.5</td>
<td>21651407867.5</td>
<td>21553779123</td>
<td>21749036612</td>
<td>138067894.5</td>
<td>:compute_log_prob_15</td>
</tr>
<tr>
<td>4.7</td>
<td>34782074906</td>
<td>2</td>
<td>17391037453.0</td>
<td>17391037453.0</td>
<td>17316844374</td>
<td>17465230532</td>
<td>104924858.6</td>
<td>:compute_ref_log_prob_15</td>
</tr>
<tr>
<td>3.9</td>
<td>29104729426</td>
<td>1536</td>
<td>18948391.6</td>
<td>25354800.0</td>
<td>7561542</td>
<td>89553828</td>
<td>10416410.4</td>
<td>:fwd_attention</td>
</tr>
<tr>
<td>2.1</td>
<td>16032874723</td>
<td>1536</td>
<td>10438069.5</td>
<td>9592203.0</td>
<td>7410043</td>
<td>45278896</td>
<td>3123988.8</td>
<td>:fwd_mlp</td>
</tr>
<tr>
<td>1.8</td>
<td>13646407003</td>
<td>256</td>
<td>53306277.4</td>
<td>52192236.5</td>
<td>38475759</td>
<td>88677588</td>
<td>5026936.9</td>
<td>:backward_step</td>
</tr>
<tr>
<td>0.9</td>
<td>6757944332</td>
<td>1536</td>
<td>4399703.3</td>
<td>4291218.5</td>
<td>3458584</td>
<td>9049253</td>
<td>592927.3</td>
<td>:fwd_experts_compute</td>
</tr>
<tr>
<td>0.9</td>
<td>6678846331</td>
<td>1536</td>
<td>4348207.2</td>
<td>3617350.5</td>
<td>1858482</td>
<td>39291003</td>
<td>2960388.3</td>
<td>:fwd_router_and_preprocess</td>
</tr>
<tr>
<td>0.2</td>
<td>1390332157</td>
<td>1536</td>
<td>905164.2</td>
<td>887759.0</td>
<td>807811</td>
<td>5811081</td>
<td>176811.6</td>
<td>:fwd_combine</td>
</tr>
<tr>
<td>0.1</td>
<td>988520111</td>
<td>6144</td>
<td>160891.9</td>
<td>152971.0</td>
<td>109053</td>
<td>3188642</td>
<td>66937.5</td>
<td>:all_to_all</td>
</tr>
<tr>
<td>0.1</td>
<td>548037263</td>
<td>1536</td>
<td>356795.1</td>
<td>351149.0</td>
<td>313244</td>
<td>1535052</td>
<td>44303.8</td>
<td>:fwd_dispatch</td>
</tr>
</tbody></table>
<ul>
<li>max_prompt_length&#x3D;512，max_response_length&#x3D;1024</li>
<li>ppo_micro_batch_size_per_gpu&#x3D;10</li>
<li>hidden_size: 2048</li>
<li>num_experts: 60</li>
<li>num_experts_per_tok: 4</li>
<li>ep&#x3D;4 （专家个数为60，ep最大可设置4）</li>
</ul>
<p>总数据量约为((512+1024)$\times$10)$\times$2048)$\times$4)$\times$2)$\times$(1-1&#x2F;ep)&#x3D;225MB<br>nvlink4带宽大概300GB&#x2F;s，IB带宽9~16GB&#x2F;s，则每次耗时约10.25ms，实际测得平均0.16ms。</p>
<h3 id="实验日志-1"><a href="#实验日志-1" class="headerlink" title="实验日志"></a><del>实验日志</del></h3><p><del>2025&#x2F;&#x2F;08&#x2F;08: 找到了VLLM中moe通信的函数，调用的deepep c++实现，同步调用.</del><br><del>2025&#x2F;08&#x2F;09:MoE一直OOM,调整并行参数不奏效.</del><br><del>2025&#x2F;08&#x2F;10:仍然在调试MoE,或许我需要自动调参的工具;或许可以尝试使用lora,lora看起来也不行,说参数不识别,因为目前lora只支持fsdp,而且qwen2MoE也不支持lora;使用deepseek-vl2-tiny,模型verl不支持</del><br><del>2025&#x2F;08&#x2F;11:尝试使用fsdp跑lora的实验,不行，排队; 今天尝试解析timeline文件,要多读论文</del><br><del>2025&#x2F;08&#x2F;12:精度文章streamRL，准备看看RLHFuse，把Magatron和VLLM的EP通信dispatch和collect函数用nvtx包一下统计时间.</del><br><del>2025&#x2F;08&#x2F;13: 缩小Moe模型尺寸，与老师讨论idea。<br>2025&#x2F;08&#x2F;14: wrap deep中关于Experts通信的实现，统计时间占比。<br>2025&#x2F;08&#x2F;15: 昨天找错函数了，没成功打点，重新一点点打点，通过插件ray debugger找到真正执行通信的地方，但是backward没找到。<br>2025&#x2F;08&#x2F;16: 成功打点，看起来一次all-to-all大概100~500us之间，总体耗时占比大约5%。<br>2025&#x2F;08&#x2F;17: 生病。<br>2025&#x2F;08&#x2F;18：两台机器16卡a6000-t2，总体耗时没怎么变，生病。<br>2025&#x2F;08&#x2F;19：下载新的模型，deepseek-16b-moe，megatron不支持。</del></p>
<h1 id="下一步工作"><a href="#下一步工作" class="headerlink" title="下一步工作"></a>下一步工作</h1><ul>
<li><del>统计多模态多机情况看看耗时占比.</del></li>
<li><del>统计MoE模型中EP all-to-all的时间占比.</del></li>
<li><del>读文章（在精读streamRL，准备读RLHFuse).</del></li>
</ul>
<h1 id="一些想法"><a href="#一些想法" class="headerlink" title="一些想法"></a>一些想法</h1><h2 id="多模态-1"><a href="#多模态-1" class="headerlink" title="多模态"></a>多模态</h2><p>~~去掉driver process这个数据集中的bottleneck在大规模多模态后训练中是有提速的，这部分工作在DistFlow中也有体现，目前还不知道怎么进一步和DistFlow做出差异。</p>
<h2 id="MoE-1"><a href="#MoE-1" class="headerlink" title="MoE"></a>MoE</h2><p>基本思路是如果Experts之间dispatch和colloect的all-to-all通信时间较大（需要实验预估收益），可<br>以利用它们掩盖一部分RLHF中的计算。如果可以的话，掩盖哪部分计算呢？需要根据是否on-policy讨论。<br>切成更小的mini-batch或stream，保持on-policy同时使rollout和training时间重叠（但是会同时占据显存，那么和分置有什么区别？）<br>~~- on-policy： 目前的主要做法，但是严格的依赖关系使其他stage的计算任务不能同时执行。如果做sample-level的调度，像RLHFuse$^2$是可以重叠部分stage的(如reward和adv)。</p>
<ul>
<li>off-policy：依据算法的不同，依赖关系更宽松，可以直接做各种stage之间的重叠。</li>
</ul>
<p>但是我不太理解，不管是否on-policy，是否all-to-all是必须要重叠的那部分？感觉以上场景都不必局限于all-to-all，而是更大颗粒度的stage之间的overlap，RL框架中针对all-to-all的计算-通信重叠还是不大理解。答：主要看重的是all-to-all导致的计算空闲，事实上，all-to-all多机的时候会有30%到50%的时间占比，那这部分就可以用来：</p>
<ol>
<li>training的all-to-all拿来做generate，当然需要把generate的batch变得更小才行。</li>
<li>training的expert imbanlace, 那空闲的expert可以塞reward等</li>
</ol>
<p>可能的问题：</p>
<ol>
<li>显存管理：colocate情况下，rollout 生成会占用 KV cache &#x2F; 激活缓存，训练的 all-to-all 阶段也要占显存，可能爆显存。</li>
<li>时间粒度：MoE 的 all-to-all 往往是毫秒级的（甚至更短），而一次完整 rollout 推理通常是几十到几百毫秒，粒度不匹配。</li>
<li>带宽资源分配：通信和推理生成都占用 NVLink&#x2F;IB 带宽，可能互相抢资源，反而降低吞吐。</li>
<li>通信堵塞：rollout推理也需要占据通信资源。</li>
<li>baseline对比：与”推理与训练直接并行“的区别？换句话说，不局限于all-to-all时间内计算会有什么后果？</li>
</ol>
<h1 id="附录"><a href="#附录" class="headerlink" title="附录"></a>附录</h1><h2 id="多模态实验"><a href="#多模态实验" class="headerlink" title="多模态实验"></a>多模态实验</h2><h3 id="Verl中ray-dispatch和collect数据理解"><a href="#Verl中ray-dispatch和collect数据理解" class="headerlink" title="Verl中ray dispatch和collect数据理解"></a>Verl中ray dispatch和collect数据理解</h3><p>在worker的方法中,ray通过dispatch_mode的方式简洁地规定了数据dispatch的方式:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">ActorRolloutRefWorker</span>(<span class="title class_ inherited__">Worker</span>):</span><br><span class="line">    ...</span><br><span class="line"><span class="meta">    @register(<span class="params">dispatch_mode=Dispatch.DP_COMPUTE_PROTO</span>)</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">generate_sequences</span>(<span class="params">self, prompts: DataProto</span>):</span><br><span class="line">        prompts = prompts.to(torch.cuda.current_device())</span><br><span class="line">        ...</span><br></pre></td></tr></table></figure>
<p>register会设置一个魔法key MAGIC_ATTR,包括attrs &#x3D; {“dispatch_mode”: dispatch_mode, “execute_mode”: execute_mode, “blocking”: blocking}.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">register</span>(<span class="params">dispatch_mode=Dispatch.ALL_TO_ALL, execute_mode=Execute.ALL, blocking=<span class="literal">True</span>, materialize_futures=<span class="literal">True</span></span>):</span><br><span class="line">    ...</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">decorator</span>(<span class="params">func</span>):</span><br><span class="line"><span class="meta">        @wraps(<span class="params">func</span>)</span></span><br><span class="line">        <span class="keyword">def</span> <span class="title function_">inner</span>(<span class="params">*args, **kwargs</span>):</span><br><span class="line">            <span class="keyword">if</span> materialize_futures:</span><br><span class="line">                args, kwargs = _materialize_futures(*args, **kwargs) <span class="comment">#注意这里通过arg.get()获取参数</span></span><br><span class="line">            <span class="keyword">return</span> func(*args, **kwargs)</span><br><span class="line"></span><br><span class="line">        attrs = &#123;<span class="string">&quot;dispatch_mode&quot;</span>: dispatch_mode, <span class="string">&quot;execute_mode&quot;</span>: execute_mode, <span class="string">&quot;blocking&quot;</span>: blocking&#125;</span><br><span class="line">        <span class="built_in">setattr</span>(inner, MAGIC_ATTR, attrs)</span><br><span class="line">        <span class="keyword">return</span> inner</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> decorator</span><br></pre></td></tr></table></figure>
<p>worker(ray actor)会通过RayClassWithInitArgs包一层,最终与resource_pool绑定一起为RayWorkerGroup:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">ray_cls_with_init = RayClassWithInitArgs(cls=ray.remote(ActorRolloutRefWorker), config=config, role=<span class="string">&quot;rollout&quot;</span>)</span><br><span class="line">resource_pool = RayResourcePool(process_on_nodes=[config.trainer.n_gpus_per_node] * config.trainer.nnodes)</span><br><span class="line">wg = RayWorkerGroup(resource_pool=resource_pool, ray_cls_with_init=ray_cls_with_init)</span><br></pre></td></tr></table></figure>
<p>worker通过RayWorkerGroup._init_with_resource_pool创建,被register修饰的方法就会绑定至RayWorkerGroup:</p>
<img src="/2025/08/10/verl-perfornamce3/Users\25490\Desktop\blog\source\_posts\verl-perfornamce3\worker_group_init.png" alt="worker_group_init" style="zoom: 50%;"  data-tag='post-image' onload='this.onload=null;this.style.opacity=1;' loading="lazy" onerror='this.onerror=null;this.src="/img/404.jpg"'>

<p>代码中是这样绑定的,首先判断是否可调用,然后如果发现存在MAGIC_ATTR,</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">if</span> <span class="built_in">hasattr</span>(method, MAGIC_ATTR):</span><br><span class="line">    attribute = <span class="built_in">getattr</span>(method, MAGIC_ATTR)</span><br><span class="line">    dispatch_mode = attribute[<span class="string">&quot;dispatch_mode&quot;</span>]</span><br><span class="line">    execute_mode = attribute[<span class="string">&quot;execute_mode&quot;</span>]</span><br><span class="line">    blocking = attribute[<span class="string">&quot;blocking&quot;</span>]</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p>dispatch_mode等是预定义的,如:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">       Dispatch.DP_COMPUTE_PROTO: &#123;</span><br><span class="line">           <span class="string">&quot;dispatch_fn&quot;</span>: dispatch_dp_compute_data_proto,</span><br><span class="line">           <span class="string">&quot;collect_fn&quot;</span>: collect_dp_compute_data_proto,</span><br><span class="line">       &#125;,</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">dispatch_dp_compute_data_proto</span>(<span class="params">worker_group, *args, **kwargs</span>):</span><br><span class="line">   <span class="keyword">from</span> verl.single_controller.base.worker_group <span class="keyword">import</span> WorkerGroup</span><br><span class="line"></span><br><span class="line">   <span class="keyword">assert</span> <span class="built_in">isinstance</span>(worker_group, WorkerGroup)</span><br><span class="line">   <span class="comment"># Note: enable auto padding for dp compute DatapProto</span></span><br><span class="line">   splitted_args, splitted_kwargs = _split_args_kwargs_data_proto_with_auto_padding(</span><br><span class="line">       worker_group.world_size,</span><br><span class="line">       *args,</span><br><span class="line">       **kwargs,</span><br><span class="line">   )</span><br><span class="line">   <span class="keyword">return</span> splitted_args, splitted_kwargs</span><br></pre></td></tr></table></figure>
<p>接着通过func_generator讲这些MAGIC_ATTR中的方法绑定:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># bind a new method to the RayWorkerGroup</span></span><br><span class="line">func = func_generator(</span><br><span class="line">   <span class="variable language_">self</span>,</span><br><span class="line">   method_name,</span><br><span class="line">   dispatch_fn=dispatch_fn,</span><br><span class="line">   collect_fn=collect_fn,</span><br><span class="line">   execute_fn=execute_fn,</span><br><span class="line">   blocking=blocking,</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"><span class="keyword">try</span>:</span><br><span class="line">   <span class="built_in">setattr</span>(<span class="variable language_">self</span>, method_name, func)</span><br><span class="line">   method_names.append(method_name)</span><br><span class="line"><span class="keyword">except</span> Exception <span class="keyword">as</span> e:</span><br><span class="line">   <span class="keyword">raise</span> ValueError(<span class="string">f&quot;Fail to set method_name <span class="subst">&#123;method_name&#125;</span>&quot;</span>) <span class="keyword">from</span> e</span><br></pre></td></tr></table></figure>
<p>func_generator的作用就是在原来的函数外头,把dispatch_fn和collect_fn的逻辑加上:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">func_generator</span>(<span class="params">self, method_name, dispatch_fn, collect_fn, execute_fn, blocking</span>):</span><br><span class="line">    <span class="keyword">class</span> <span class="title class_">Functor</span>:</span><br><span class="line">        <span class="keyword">def</span> <span class="title function_">__call__</span>(<span class="params">this, *args, **kwargs</span>):</span><br><span class="line">            args, kwargs = dispatch_fn(<span class="variable language_">self</span>, *args, **kwargs)</span><br><span class="line">            padding_count = kwargs.pop(_padding_size_key, <span class="number">0</span>)</span><br><span class="line">            output = execute_fn(method_name, *args, **kwargs)</span><br><span class="line">            <span class="keyword">if</span> blocking:</span><br><span class="line">                output = ray.get(output)</span><br><span class="line">            output = collect_fn(<span class="variable language_">self</span>, output)</span><br><span class="line">            <span class="keyword">if</span> padding_count &gt; <span class="number">0</span>:</span><br><span class="line">                <span class="keyword">if</span> <span class="built_in">isinstance</span>(output, DataProto):</span><br><span class="line">                    indices = [i <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(output))][:-padding_count]</span><br><span class="line">                    output = output.select_idxs(indices)</span><br><span class="line">                <span class="keyword">elif</span> <span class="built_in">isinstance</span>(output, <span class="built_in">list</span>):</span><br><span class="line">                    output = output[:-padding_count]</span><br><span class="line">            <span class="keyword">return</span> output</span><br></pre></td></tr></table></figure>
<p>至于这个excute_fn,和dispatch与collect逻辑类似,定义在rayWorkerGroup中:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">execute_all_async</span>(<span class="params">self, method_name: <span class="built_in">str</span>, *args, **kwargs</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;Execute a method on all workers asynchronously.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Args:</span></span><br><span class="line"><span class="string">        method_name: Name of the method to execute</span></span><br><span class="line"><span class="string">        *args: Positional arguments for the method</span></span><br><span class="line"><span class="string">        **kwargs: Keyword arguments for the method</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">        List of remote object references to the method executions</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    <span class="comment"># Here, we assume that if all arguments in args and kwargs are lists,</span></span><br><span class="line">    <span class="comment"># and their lengths match len(self._workers), we&#x27;ll distribute each</span></span><br><span class="line">    <span class="comment"># element in these lists to the corresponding worker</span></span><br><span class="line">    <span class="comment"># print(f&quot;execute_all_async: method &#123;method_name&#125;(&#123;args&#125;, &#123;kwargs&#125;)&quot;)</span></span><br><span class="line">    length = <span class="built_in">len</span>(<span class="variable language_">self</span>._workers)</span><br><span class="line">    <span class="keyword">if</span> <span class="built_in">all</span>(<span class="built_in">isinstance</span>(arg, <span class="built_in">list</span>) <span class="keyword">for</span> arg <span class="keyword">in</span> args) <span class="keyword">and</span> <span class="built_in">all</span>(<span class="built_in">isinstance</span>(kwarg, <span class="built_in">list</span>) <span class="keyword">for</span> kwarg <span class="keyword">in</span> kwargs.values()):</span><br><span class="line">        <span class="keyword">if</span> <span class="built_in">all</span>(<span class="built_in">len</span>(arg) == length <span class="keyword">for</span> arg <span class="keyword">in</span> args) <span class="keyword">and</span> <span class="built_in">all</span>(<span class="built_in">len</span>(kwarg) == length <span class="keyword">for</span> kwarg <span class="keyword">in</span> kwargs.values()):</span><br><span class="line">            <span class="comment"># print(f&quot;splitting args and kwargs into &#123;length&#125; shards&quot;)</span></span><br><span class="line">            result = []</span><br><span class="line">            <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(length):</span><br><span class="line">                sliced_args = <span class="built_in">tuple</span>(arg[i] <span class="keyword">for</span> arg <span class="keyword">in</span> args)</span><br><span class="line">                sliced_kwargs = &#123;k: v[i] <span class="keyword">for</span> k, v <span class="keyword">in</span> kwargs.items()&#125;</span><br><span class="line">                result.append(</span><br><span class="line">                    <span class="variable language_">self</span>._execute_remote_single_worker(<span class="variable language_">self</span>._workers[i], method_name, *sliced_args, **sliced_kwargs)</span><br><span class="line">                )</span><br><span class="line">            <span class="keyword">return</span> result</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> [<span class="variable language_">self</span>._execute_remote_single_worker(worker, method_name, *args, **kwargs) <span class="keyword">for</span> worker <span class="keyword">in</span> <span class="variable language_">self</span>._workers]</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">_execute_remote_single_worker</span>(<span class="params">self, worker, method_name: <span class="built_in">str</span>, *args, **kwargs</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;Execute a method on a single worker remotely.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Args:</span></span><br><span class="line"><span class="string">        worker: The worker actor handle</span></span><br><span class="line"><span class="string">        method_name: Name of the method to execute</span></span><br><span class="line"><span class="string">        *args: Positional arguments for the method</span></span><br><span class="line"><span class="string">        **kwargs: Keyword arguments for the method</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">        Remote object reference to the method execution</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">if</span> <span class="variable language_">self</span>.fused_worker_used <span class="keyword">and</span> method_name <span class="keyword">not</span> <span class="keyword">in</span> <span class="variable language_">self</span>.method_names:</span><br><span class="line">        remote_call = <span class="built_in">getattr</span>(worker, <span class="variable language_">self</span>.fused_worker_execute_fn_name)</span><br><span class="line">        <span class="keyword">return</span> remote_call.remote(<span class="string">f&quot;<span class="subst">&#123;self.sub_cls_name&#125;</span>_fwmn_<span class="subst">&#123;method_name&#125;</span>&quot;</span>, *args, **kwargs)</span><br><span class="line">    <span class="comment"># fused worker not used</span></span><br><span class="line">    remote_call = <span class="built_in">getattr</span>(worker, method_name)</span><br><span class="line">    <span class="keyword">return</span> remote_call.remote(*args, **kwargs)</span><br></pre></td></tr></table></figure>

<p>所以,如果以Rollout process上为例,实际数据流向是这样的:</p>
<p><img src="/2025/08/10/verl-perfornamce3/Users\25490\Desktop\blog\source_posts\verl-perfornamce3\call_generate_sequences.png" alt="call_generate_sequences"  data-tag='post-image' onload='this.onload=null;this.style.opacity=1;' loading="lazy" onerror='this.onerror=null;this.src="/img/404.jpg"'></p>
<p>因此,我们需要测量driver process中把数据传输至rank并执行的总时间,把这些时间与rank上实际执行的时间对比,即可知道传输数据所用时长.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">func_generator</span>(<span class="params">self, method_name, dispatch_fn, collect_fn, execute_fn, blocking</span>):</span><br><span class="line">    <span class="keyword">class</span> <span class="title class_">Functor</span>:</span><br><span class="line">        <span class="keyword">def</span> <span class="title function_">__call__</span>(<span class="params">this, *args, **kwargs</span>):</span><br><span class="line">            <span class="keyword">with</span> marked_timer(<span class="string">&quot;dispatch&quot;</span>, color=<span class="string">&quot;green&quot;</span>):</span><br><span class="line">                args, kwargs = dispatch_fn(<span class="variable language_">self</span>, *args, **kwargs)</span><br><span class="line">                padding_count = kwargs.pop(_padding_size_key, <span class="number">0</span>)</span><br><span class="line">            <span class="keyword">with</span> marked_timer(<span class="string">&quot;submit_execute&quot;</span>, color=<span class="string">&quot;red&quot;</span>):</span><br><span class="line">                output = execute_fn(method_name, *args, **kwargs)</span><br><span class="line">            <span class="keyword">if</span> blocking:</span><br><span class="line">                <span class="keyword">with</span> marked_timer(<span class="string">&quot;get_output&quot;</span>, color=<span class="string">&quot;green&quot;</span>):</span><br><span class="line">                    output = ray.get(output)</span><br><span class="line">            <span class="keyword">with</span> marked_timer(<span class="string">&quot;collect&quot;</span>, color=<span class="string">&quot;black&quot;</span>):</span><br><span class="line">                output = collect_fn(<span class="variable language_">self</span>, output)</span><br><span class="line">                <span class="keyword">if</span> padding_count &gt; <span class="number">0</span>:</span><br><span class="line">                    <span class="keyword">if</span> <span class="built_in">isinstance</span>(output, DataProto):</span><br><span class="line">                        indices = [i <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(output))][:-padding_count]</span><br><span class="line">                        output = output.select_idxs(indices)</span><br><span class="line">                    <span class="keyword">elif</span> <span class="built_in">isinstance</span>(output, <span class="built_in">list</span>):</span><br><span class="line">                        output = output[:-padding_count]</span><br><span class="line">            <span class="keyword">return</span> output</span><br></pre></td></tr></table></figure>

<h3 id="实验脚本"><a href="#实验脚本" class="headerlink" title="实验脚本"></a>实验脚本</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Nsight profiling configuration</span></span><br><span class="line">PROFILE_STEPS=<span class="string">&quot;[2,3]&quot;</span> <span class="comment"># or [] or null</span></span><br><span class="line">PROFILE_RANKS_ALL=<span class="literal">True</span> <span class="comment"># or True</span></span><br><span class="line">PROFILE_RANKS=[<span class="number">0</span>,<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>,<span class="number">5</span>,<span class="number">6</span>,<span class="number">7</span>]</span><br><span class="line">DISCRETE=<span class="literal">False</span> <span class="comment"># or True</span></span><br><span class="line">PROFILE_CONTINUOUS_STEPS=<span class="literal">True</span></span><br><span class="line"></span><br><span class="line"><span class="built_in">set</span> -x</span><br><span class="line">ENGINE=$&#123;<span class="number">1</span>:-vllm&#125;</span><br><span class="line"></span><br><span class="line">python3 -m verl.trainer.main_ppo \</span><br><span class="line">    algorithm.adv_estimator=grpo \</span><br><span class="line">    data.train_files=/workspace/dataset/geo3k/train.parquet \</span><br><span class="line">    data.val_files=/workspace/dataset/geo3k/test.parquet \</span><br><span class="line">    data.train_batch_size=<span class="number">256</span> \</span><br><span class="line">    data.max_prompt_length=<span class="number">1024</span> \</span><br><span class="line">    data.max_response_length=<span class="number">2048</span> \</span><br><span class="line">    data.filter_overlong_prompts=<span class="literal">True</span> \</span><br><span class="line">    data.truncation=<span class="string">&#x27;error&#x27;</span> \</span><br><span class="line">    data.image_key=images \</span><br><span class="line">    actor_rollout_ref.model.path=/workspace/model/Qwen2<span class="number">.5</span>-VL-7B-Instruct \</span><br><span class="line">    actor_rollout_ref.actor.optim.lr=<span class="number">1e-6</span> \</span><br><span class="line">    actor_rollout_ref.model.use_remove_padding=<span class="literal">True</span> \</span><br><span class="line">    actor_rollout_ref.actor.ppo_mini_batch_size=<span class="number">128</span> \</span><br><span class="line">    actor_rollout_ref.actor.ppo_micro_batch_size_per_gpu=<span class="number">10</span> \</span><br><span class="line">    actor_rollout_ref.actor.use_kl_loss=<span class="literal">True</span> \</span><br><span class="line">    actor_rollout_ref.actor.kl_loss_coef=<span class="number">0.01</span> \</span><br><span class="line">    actor_rollout_ref.actor.kl_loss_type=low_var_kl \</span><br><span class="line">    actor_rollout_ref.actor.entropy_coeff=<span class="number">0</span> \</span><br><span class="line">    actor_rollout_ref.model.enable_gradient_checkpointing=<span class="literal">True</span> \</span><br><span class="line">    actor_rollout_ref.actor.fsdp_config.param_offload=<span class="literal">False</span> \</span><br><span class="line">    actor_rollout_ref.actor.fsdp_config.optimizer_offload=<span class="literal">False</span> \</span><br><span class="line">    actor_rollout_ref.rollout.log_prob_micro_batch_size_per_gpu=<span class="number">20</span> \</span><br><span class="line">    actor_rollout_ref.rollout.tensor_model_parallel_size=<span class="number">4</span> \</span><br><span class="line">    actor_rollout_ref.rollout.name=$ENGINE \</span><br><span class="line">    actor_rollout_ref.rollout.engine_kwargs.vllm.disable_mm_preprocessor_cache=<span class="literal">True</span> \</span><br><span class="line">    actor_rollout_ref.rollout.gpu_memory_utilization=<span class="number">0.6</span> \</span><br><span class="line">    actor_rollout_ref.rollout.enable_chunked_prefill=<span class="literal">False</span> \</span><br><span class="line">    actor_rollout_ref.rollout.enforce_eager=<span class="literal">False</span> \</span><br><span class="line">    actor_rollout_ref.rollout.free_cache_engine=<span class="literal">True</span> \</span><br><span class="line">    actor_rollout_ref.rollout.n=<span class="number">5</span> \</span><br><span class="line">    actor_rollout_ref.ref.log_prob_micro_batch_size_per_gpu=<span class="number">20</span> \</span><br><span class="line">    actor_rollout_ref.ref.fsdp_config.param_offload=<span class="literal">True</span> \</span><br><span class="line">    actor_rollout_ref.profiler.ranks=$PROFILE_RANKS \</span><br><span class="line">    actor_rollout_ref.profiler.all_ranks=$PROFILE_RANKS_ALL \</span><br><span class="line">    actor_rollout_ref.profiler.discrete=$DISCRETE \</span><br><span class="line">    algorithm.use_kl_in_reward=<span class="literal">False</span> \</span><br><span class="line">    trainer.critic_warmup=<span class="number">0</span> \</span><br><span class="line">    trainer.logger=<span class="string">&#x27;[&quot;console&quot;]&#x27;</span> \</span><br><span class="line">    trainer.project_name=<span class="string">&#x27;verl_grpo_example_geo3k&#x27;</span> \</span><br><span class="line">    trainer.experiment_name=<span class="string">&#x27;qwen2_5_vl_7b_function_rm&#x27;</span> \</span><br><span class="line">    trainer.n_gpus_per_node=<span class="number">8</span> \</span><br><span class="line">    trainer.nnodes=<span class="number">1</span> \</span><br><span class="line">    trainer.save_freq=<span class="number">20</span> \</span><br><span class="line">    trainer.test_freq=<span class="number">100</span> \</span><br><span class="line">    ray_init.timeline_json_file=/workspace/record/qwenvl_ray_timeline.json \</span><br><span class="line">    trainer.profile_steps=$PROFILE_STEPS \</span><br><span class="line">    trainer.profile_continuous_steps=$PROFILE_CONTINUOUS_STEPS \</span><br><span class="line">    trainer.total_epochs=<span class="number">1</span> <span class="number">2</span>&gt;&amp;<span class="number">1</span> | tee verl_demo.log</span><br></pre></td></tr></table></figure>

<h2 id="MoE-2"><a href="#MoE-2" class="headerlink" title="MoE"></a>MoE</h2><h3 id="MoE模型减少体积"><a href="#MoE模型减少体积" class="headerlink" title="MoE模型减少体积"></a>MoE模型减少体积</h3><p>以Qwen1.5-MoE-A2.7B-Chat为例，该模型有24层，希望仅保留两层：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> inspect</span><br><span class="line"><span class="keyword">from</span> transformers <span class="keyword">import</span> AutoModelForCausalLM</span><br><span class="line"><span class="keyword">import</span> sys</span><br><span class="line">sys.path.append(<span class="string">&quot;/workspace&quot;</span>)</span><br><span class="line"></span><br><span class="line">model = AutoModelForCausalLM.from_pretrained(<span class="string">&quot;/workspace/model/Qwen1.5-MoE-A2.7B-Chat&quot;</span>, trust_remote_code=<span class="literal">True</span>)</span><br><span class="line"><span class="built_in">print</span>(model.model.layers)</span><br><span class="line">new_layers = torch.nn.ModuleList([layers <span class="keyword">for</span> i,layers <span class="keyword">in</span> <span class="built_in">enumerate</span>(model.model.layers) <span class="keyword">if</span> i % <span class="number">12</span> == <span class="number">0</span>]) <span class="comment"># 设置新层</span></span><br><span class="line">model.model.layers = new_layers <span class="comment">#赋值</span></span><br><span class="line">model.model.config.num_hidden_layers = <span class="built_in">len</span>(new_layers) <span class="comment">#修改config</span></span><br><span class="line">model.save_pretrained(<span class="string">&quot;/workspace/model/Qwen1.5-MoE-A2.7B_1_4-Chat&quot;</span>, safe_serialization=<span class="literal">True</span>)</span><br></pre></td></tr></table></figure>
<p>save_pretrained会保存模型权重safetensors格式、描述模型配置的config.json以及映射关系configuration.json，其他token等文件拷贝原模型所在文件夹即可。</p>
<p><img src="/2025/08/10/verl-perfornamce3/Users\25490\Desktop\blog\source_posts\verl-perfornamce3\reduce_moe_size.png" alt="image-20250814161646104"  data-tag='post-image' onload='this.onload=null;this.style.opacity=1;' loading="lazy" onerror='this.onerror=null;this.src="/img/404.jpg"'></p>
<p>使用VLLM和megatron加载模型即可。</p>
<p>如果是自定义的模型还需要映射模型结构，即在config.json中定义：</p>
<figure class="highlight json"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">......  </span><br><span class="line"><span class="attr">&quot;attn_implementation&quot;</span><span class="punctuation">:</span> <span class="string">&quot;eager&quot;</span><span class="punctuation">,</span></span><br><span class="line">  <span class="attr">&quot;auto_map&quot;</span><span class="punctuation">:</span> <span class="punctuation">&#123;</span></span><br><span class="line">    <span class="attr">&quot;AutoConfig&quot;</span><span class="punctuation">:</span> <span class="string">&quot;configuration_gatemodel.GateModelConfig&quot;</span><span class="punctuation">,</span>     </span><br><span class="line">    <span class="attr">&quot;AutoModel&quot;</span><span class="punctuation">:</span> <span class="string">&quot;modeling_gatemodel.GateModelForCausalLM&quot;</span><span class="punctuation">,</span></span><br><span class="line">    <span class="attr">&quot;AutoModelForCausalLM&quot;</span><span class="punctuation">:</span> <span class="string">&quot;modeling_gatemodel.GateModelForCausalLM&quot;</span></span><br><span class="line">  <span class="punctuation">&#125;</span><span class="punctuation">,</span></span><br><span class="line">  <span class="attr">&quot;bos_token_id&quot;</span><span class="punctuation">:</span> <span class="number">151643</span><span class="punctuation">,</span></span><br><span class="line">  <span class="attr">&quot;eos_token_id&quot;</span><span class="punctuation">:</span> <span class="number">151645</span><span class="punctuation">,</span></span><br><span class="line">......</span><br></pre></td></tr></table></figure>

<p>详情参考：</p>
<ol>
<li>[VLLM加载自定义模型]([<a target="_blank" rel="noopener" href="https://github.com/vllm-project/vllm/issues/16826">Bug]: The Transformers implementation of My Model is not compatible with vLLM. · Issue #16826 · vllm-project&#x2F;vllm</a>)</li>
<li>[VLLM支持自定义模型说明](<a target="_blank" rel="noopener" href="https://docs.vllm.ai/en/latest/models/supported_models.html#custom-models">Supported Models - vLLM</a>)</li>
</ol>
<h3 id="Megatron的dispatch和collect实现"><a href="#Megatron的dispatch和collect实现" class="headerlink" title="Megatron的dispatch和collect实现"></a>Megatron的dispatch和collect实现</h3><p>通过ray debugger插件打点breakpoint()找到：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># path: megatron\core\transformer\moe\moe_layer.py</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">MoELayer</span>(<span class="title class_ inherited__">BaseMoELayer</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;Mixture of Experts layer.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    This layer implements a Mixture of Experts model, where each token is routed to a</span></span><br><span class="line"><span class="string">    subset of experts. This implementation supports different token dispatching</span></span><br><span class="line"><span class="string">    strategies such as All-to-All and All-Gather.</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    ......</span><br><span class="line">        <span class="comment"># Initialize token dispatcher</span></span><br><span class="line">    <span class="keyword">if</span> config.moe_token_dispatcher_type == <span class="string">&quot;allgather&quot;</span>:</span><br><span class="line">        <span class="variable language_">self</span>.token_dispatcher = MoEAllGatherTokenDispatcher(</span><br><span class="line">            <span class="variable language_">self</span>.num_local_experts,</span><br><span class="line">            <span class="variable language_">self</span>.local_expert_indices,</span><br><span class="line">            config=<span class="variable language_">self</span>.config,</span><br><span class="line">            model_comm_pgs=model_comm_pgs,</span><br><span class="line">        )</span><br><span class="line">    <span class="keyword">elif</span> config.moe_token_dispatcher_type == <span class="string">&quot;alltoall&quot;</span>:</span><br><span class="line">        <span class="variable language_">self</span>.token_dispatcher = MoEAlltoAllTokenDispatcher(</span><br><span class="line">            <span class="variable language_">self</span>.num_local_experts,</span><br><span class="line">            <span class="variable language_">self</span>.local_expert_indices,</span><br><span class="line">            config=<span class="variable language_">self</span>.config,</span><br><span class="line">            model_comm_pgs=model_comm_pgs,</span><br><span class="line">        )</span><br><span class="line">    <span class="keyword">elif</span> config.moe_token_dispatcher_type == <span class="string">&quot;flex&quot;</span>:</span><br><span class="line">        <span class="variable language_">self</span>.token_dispatcher = MoEFlexTokenDispatcher(</span><br><span class="line">            <span class="variable language_">self</span>.num_local_experts,</span><br><span class="line">            <span class="variable language_">self</span>.local_expert_indices,</span><br><span class="line">            config=<span class="variable language_">self</span>.config,</span><br><span class="line">            model_comm_pgs=model_comm_pgs,</span><br><span class="line">        )</span><br><span class="line">       ......</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, hidden_states: torch.Tensor</span>):</span><br><span class="line">        <span class="string">&quot;&quot;&quot;Forward pass for the MoE layer.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        The forward pass comprises four main steps:</span></span><br><span class="line"><span class="string">        1. Routing &amp; Preprocessing: Route tokens to the assigned experts and prepare for dispatch.</span></span><br><span class="line"><span class="string">        2. Dispatch: Tokens are sent to the expert devices using communication collectives.</span></span><br><span class="line"><span class="string">        3. Expert Computation: Experts process the dispatched tokens.</span></span><br><span class="line"><span class="string">        4. Combine: The outputs from the experts are combined and returned.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        Args:</span></span><br><span class="line"><span class="string">            hidden_states (torch.Tensor): The input tensor to the MoE layer.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        Returns:</span></span><br><span class="line"><span class="string">            A tuple containing the output tensor and the MLP bias, if any.</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        <span class="keyword">if</span> <span class="variable language_">self</span>.training <span class="keyword">and</span> <span class="variable language_">self</span>.attn_tp_group.size() &gt; <span class="number">1</span> <span class="keyword">and</span> <span class="keyword">not</span> <span class="variable language_">self</span>.config.sequence_parallel:</span><br><span class="line">            <span class="keyword">raise</span> ValueError(</span><br><span class="line">                <span class="string">&quot;During training, performance may degrade if MoE and tensor parallelism&quot;</span></span><br><span class="line">                <span class="string">&quot;are enabled without also enabling sequence parallelism.&quot;</span></span><br><span class="line">            )</span><br><span class="line"></span><br><span class="line">        <span class="comment"># MoE forward: route -&gt; dispatch -&gt; compute -&gt; combine</span></span><br><span class="line">        <span class="keyword">def</span> <span class="title function_">custom_forward</span>(<span class="params">hidden_states</span>):</span><br><span class="line">            <span class="comment"># mark_id = nvtx.start_range(message=&quot;router_and_preprocess&quot;, color=&quot;red&quot;)</span></span><br><span class="line">            hidden_states, probs, residual = <span class="variable language_">self</span>.router_and_preprocess(hidden_states)</span><br><span class="line">            <span class="comment"># nvtx.end_range(mark_id)</span></span><br><span class="line">            <span class="comment"># mark_id = nvtx.start_range(message=&quot;dispatch&quot;, color=&quot;blue&quot;)</span></span><br><span class="line">            dispatched_input, probs = <span class="variable language_">self</span>.dispatch(hidden_states, probs)</span><br><span class="line">            <span class="comment"># nvtx.end_range(mark_id)</span></span><br><span class="line">            <span class="comment"># mark_id = nvtx.start_range(message=&quot;experts_compute&quot;, color=&quot;green&quot;)</span></span><br><span class="line">            output, shared_expert_output, mlp_bias = <span class="variable language_">self</span>.experts_compute(</span><br><span class="line">                dispatched_input, probs, residual</span><br><span class="line">            )</span><br><span class="line">            <span class="comment"># nvtx.end_range(mark_id)</span></span><br><span class="line">            <span class="comment"># mark_id = nvtx.start_range(message=&quot;combine&quot;, color=&quot;yellow&quot;)</span></span><br><span class="line">            output = <span class="variable language_">self</span>.combine(output, shared_expert_output)</span><br><span class="line">            <span class="comment"># nvtx.end_range(mark_id)</span></span><br><span class="line">            <span class="keyword">return</span> output, mlp_bias</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> <span class="variable language_">self</span>.moe_layer_recompute:</span><br><span class="line">            <span class="keyword">if</span> <span class="variable language_">self</span>.config.fp8:</span><br><span class="line">                output, mlp_bias = te_checkpoint(</span><br><span class="line">                    custom_forward,</span><br><span class="line">                    <span class="literal">False</span>,</span><br><span class="line">                    tensor_parallel.random.get_cuda_rng_tracker,</span><br><span class="line">                    parallel_state.get_tensor_model_parallel_group(),</span><br><span class="line">                    hidden_states,</span><br><span class="line">                )</span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                output, mlp_bias = tensor_parallel.checkpoint(custom_forward, <span class="literal">False</span>, hidden_states)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            output, mlp_bias = custom_forward(hidden_states)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> output, mlp_bias</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">backward_dw</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="string">&quot;&quot;&quot;Compute weight gradients for experts and shared experts.&quot;&quot;&quot;</span></span><br><span class="line">        <span class="variable language_">self</span>.experts.backward_dw()</span><br><span class="line">        <span class="keyword">if</span> <span class="variable language_">self</span>.use_shared_expert <span class="keyword">and</span> <span class="keyword">not</span> <span class="variable language_">self</span>.shared_expert_overlap:</span><br><span class="line">            <span class="variable language_">self</span>.shared_experts.backward_dw()</span><br></pre></td></tr></table></figure>
<p>在MoEAlltoAllTokenDispatcher最里面调用的</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># path: megatron\core\transformer\tensor_parallel.py\mapping.py\_AllToAll</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">_AllToAll</span>(torch.autograd.Function):</span><br><span class="line"><span class="meta">    @staticmethod</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">ctx, group, <span class="built_in">input</span>, output_split_sizes, input_split_sizes</span>):</span><br><span class="line">        mark_range = nvtx.start_range(message=<span class="string">&quot;AllToAll&quot;</span>, color=<span class="string">&quot;blue&quot;</span>)</span><br><span class="line">        <span class="string">&quot;&quot;&quot;Forward function.&quot;&quot;&quot;</span></span><br><span class="line">        ctx.group = group</span><br><span class="line">        ctx.output_split_sizes = output_split_sizes</span><br><span class="line">        ctx.input_split_sizes = input_split_sizes</span><br><span class="line"></span><br><span class="line">        world_size = group.size()</span><br><span class="line">        <span class="comment"># Bypass the function if we are using only 1 GPU.</span></span><br><span class="line">        <span class="keyword">if</span> world_size == <span class="number">1</span>:</span><br><span class="line">            <span class="keyword">return</span> <span class="built_in">input</span></span><br><span class="line"></span><br><span class="line">        <span class="built_in">input</span> = <span class="built_in">input</span>.contiguous()</span><br><span class="line">        <span class="keyword">if</span> output_split_sizes <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">            <span class="comment"># Equal split (all2all)</span></span><br><span class="line">            output = torch.empty_like(<span class="built_in">input</span>)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="comment"># Unequal split (all2all-v)</span></span><br><span class="line">            output = <span class="built_in">input</span>.new_empty(</span><br><span class="line">                size=[<span class="built_in">sum</span>(output_split_sizes)] + <span class="built_in">list</span>(<span class="built_in">input</span>.size()[<span class="number">1</span>:]),</span><br><span class="line">                dtype=<span class="built_in">input</span>.dtype,</span><br><span class="line">                device=torch.cuda.current_device(),</span><br><span class="line">            )</span><br><span class="line">        torch.distributed.all_to_all_single(</span><br><span class="line">            output,</span><br><span class="line">            <span class="built_in">input</span>,</span><br><span class="line">            output_split_sizes=output_split_sizes,</span><br><span class="line">            input_split_sizes=input_split_sizes,</span><br><span class="line">            group=group,</span><br><span class="line">        )</span><br><span class="line">        nvtx.end_range(mark_range)</span><br><span class="line">        <span class="keyword">return</span> output</span><br><span class="line"></span><br><span class="line"><span class="meta">    @staticmethod</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">backward</span>(<span class="params">ctx, *grad_output</span>):</span><br><span class="line">        <span class="string">&quot;&quot;&quot;Backward function.&quot;&quot;&quot;</span></span><br><span class="line">        <span class="keyword">return</span> (</span><br><span class="line">            <span class="literal">None</span>,</span><br><span class="line">            _AllToAll.apply(ctx.group, *grad_output, ctx.input_split_sizes, ctx.output_split_sizes),</span><br><span class="line">            <span class="literal">None</span>,</span><br><span class="line">            <span class="literal">None</span>,</span><br><span class="line">        )</span><br></pre></td></tr></table></figure>
<h3 id="wrap"><a href="#wrap" class="headerlink" title="wrap"></a>wrap</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 仅为例子，最外层import</span></span><br><span class="line"><span class="keyword">from</span> deep_ep <span class="keyword">import</span> Buffer</span><br><span class="line"><span class="keyword">from</span> .nvtx_profile <span class="keyword">import</span> marked_timer</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">wrap</span>(<span class="params">func</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    A decorator to wrap a function with NVTX profiling.</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">wrapper</span>(<span class="params">*args, **kwargs</span>):</span><br><span class="line">        <span class="keyword">with</span> marked_timer(name=func.__name__, color=<span class="string">&quot;blue&quot;</span>, domain=<span class="string">&quot;megatron&quot;</span>):</span><br><span class="line">            result = func(*args, **kwargs)</span><br><span class="line">        <span class="keyword">return</span> result</span><br><span class="line">    <span class="keyword">return</span> wrapper</span><br><span class="line"></span><br><span class="line">Buffer.dispatch = wrap(Buffer.dispatch)</span><br><span class="line">Buffer.combine = wrap(Buffer.combine)</span><br></pre></td></tr></table></figure>
<h3 id="统计信息"><a href="#统计信息" class="headerlink" title="统计信息"></a>统计信息</h3><p>首先生成sqlite数据库文件：<br>nsys stats -r nvtx_startend_sum -f csv -o worker_process_105183 worker_process_105183.nsys-rep<br>这会生成相应的统计信息：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">Time (%),Total Time (ns),Instances,Avg (ns),Med (ns),Min (ns),Max (ns),StdDev (ns),Range</span><br><span class="line">32.1,223763692191,2,111881846095.5,111881846095.5,111087627097,112676065094,1123195279.2,:generate_sequences_7</span><br><span class="line">30.1,209793820677,2,104896910338.5,104896910338.5,104515626528,105278194149,539216735.9,:inference_engine</span><br><span class="line">16.7,116746518185,2,58373259092.5,58373259092.5,58281325873,58465192312,130013205.8,:update_actor_7</span><br><span class="line">10.4,72423041196,2,36211520598.0,36211520598.0,35559228826,36863812370,922479870.6,:compute_log_prob_7</span><br><span class="line">7.5,52581291573,2,26290645786.5,26290645786.5,25816871758,26764419815,670017656.6,:compute_ref_log_prob_7</span><br><span class="line">1.8,12516333138,1536,8148654.4,3333891.0,2423059,661870189,38331856.6,:router_and_preprocess</span><br><span class="line">0.9,6319498196,1536,4114256.6,3410866.5,2790738,353038771,11825450.4,:experts_compute</span><br><span class="line">0.2,1407082188,1536,916069.1,899008.0,785984,3238252,147826.4,:combine</span><br><span class="line">0.1,983480419,6144,160071.7,145750.0,102032,1328357,62962.8,:AllToAll</span><br><span class="line">0.1,532784649,1536,346865.0,339328.5,296435,1316957,59627.4,:dispatch</span><br></pre></td></tr></table></figure>
<p>当然也可以使用python sqlite3更灵活地统计：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> os, sqlite3</span><br><span class="line"></span><br><span class="line">path = <span class="string">&quot;/workspace/record/moe/moe_tj/worker_process_499551.sqlite&quot;</span>   <span class="comment"># 改成你的绝对路径</span></span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;python sqlite3 version =&quot;</span>, sqlite3.sqlite_version)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 1) 只读 + URI 方式，避免误写/锁冲突；路径里有冒号/问号时也更稳</span></span><br><span class="line">uri = <span class="string">f&quot;file:<span class="subst">&#123;path&#125;</span>?mode=ro&quot;</span></span><br><span class="line"><span class="keyword">assert</span> os.path.exists(path), <span class="string">f&quot;DB file not found: <span class="subst">&#123;path&#125;</span>&quot;</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">con = sqlite3.connect(uri, uri=<span class="literal">True</span>, timeout=<span class="number">10.0</span>)  <span class="comment"># timeout 以防短暂锁</span></span><br><span class="line">cur = con.cursor()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 2) 快速健康检查</span></span><br><span class="line">cur.execute(<span class="string">&quot;PRAGMA integrity_check;&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;integrity_check =&quot;</span>, cur.fetchone()[<span class="number">0</span>])</span><br><span class="line">query = <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">select SUM(end - start) AS total_duration_ns FROM NVTX_EVENTS WHERE text=&#x27;AllToAll&#x27;;</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="comment"># 3) 看看都有啥表</span></span><br><span class="line">cur.execute(query)</span><br><span class="line"><span class="comment"># 3. 取结果</span></span><br><span class="line">rows = cur.fetchall()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 4. 打印结果</span></span><br><span class="line"><span class="keyword">for</span> row <span class="keyword">in</span> rows:</span><br><span class="line">    <span class="built_in">print</span>(row)</span><br><span class="line"></span><br><span class="line">con.close()</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;OK: connected and queried successfully.&quot;</span>)</span><br></pre></td></tr></table></figure>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">root@job-h60e208f-5lr9k:/workspace# /bin/python /workspace/record/qurey.py</span><br><span class="line">python sqlite3 version = 3.37.2</span><br><span class="line">integrity_check = ok</span><br><span class="line">(988939166,)</span><br><span class="line">OK: connected and queried successfully.</span><br></pre></td></tr></table></figure>
<p>但是统计结果都是约1ms，ALL-TO-ALL占总时间0.1%，占update_actor、ref和old_prob计算时间的0.3%，不得不说令人沮丧。</p>
<h3 id="多机配置"><a href="#多机配置" class="headerlink" title="多机配置"></a>多机配置</h3><ol>
<li>申请容器：选择use IB，并在Capability中添加IPC_LOCK。</li>
<li>查看驱动：ls &#x2F;sys&#x2F;class&#x2F;infiniband&#x2F;，如果有mlx5_0，说明有 Infiniband 网卡。</li>
<li>看网卡端口状态（可选）：apt-get install -y infiniband-diags，可以用ibstat等命令。</li>
<li><a target="_blank" rel="noopener" href="https://verl.readthedocs.io/en/latest/start/multinode.html">启动集群</a>：<ul>
<li>Host: ray start –head –dashboard-host&#x3D;0.0.0.0，注意一个dashboard address，一个gcs address</li>
<li>other node: ray start –address&#x3D;gcs address</li>
<li>ray status，查看状态</li>
</ul>
</li>
<li>修改环境变量：</li>
</ol>
 <figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># working_dir: ./</span></span><br><span class="line"><span class="comment"># excludes: [&quot;/.git/&quot;]</span></span><br><span class="line"><span class="attr">env_vars:</span></span><br><span class="line">  <span class="attr">TORCH_NCCL_AVOID_RECORD_STREAMS:</span> <span class="string">&quot;1&quot;</span></span><br><span class="line">  <span class="attr">CUDA_DEVICE_MAX_CONNECTIONS:</span> <span class="string">&quot;1&quot;</span></span><br><span class="line">  <span class="comment"># 配置python路径</span></span><br><span class="line">  <span class="attr">PYTHONPATH:</span> <span class="string">&quot;/workspace/verl-main&quot;</span></span><br><span class="line">  <span class="comment"># —— 数据面：走 InfiniBand RDMA ——</span></span><br><span class="line">  <span class="attr">NCCL_IB_DISABLE:</span> <span class="string">&quot;0&quot;</span></span><br><span class="line">  <span class="attr">NCCL_IB_CUDA_SUPPORT:</span> <span class="string">&quot;1&quot;</span></span><br><span class="line">  <span class="comment"># 如需绑定到特定 HCA/端口（按实际改；可省略）</span></span><br><span class="line">  <span class="comment"># 查看：ls /sys/class/infiniband/</span></span><br><span class="line">  <span class="comment"># cat /sys/class/infiniband/mlx5_0/ports/1/state</span></span><br><span class="line">  <span class="comment"># cat /sys/class/infiniband/mlx5_0/ports/1/phys_state</span></span><br><span class="line">  <span class="comment"># 例：mlx5_0 端口1,</span></span><br><span class="line">  <span class="attr">NCCL_IB_HCA:</span> <span class="string">&quot;mlx5_0:1&quot;</span></span><br><span class="line"></span><br><span class="line">  <span class="comment"># —— 控制面/握手：用 IPoIB 接口 ——</span></span><br><span class="line">  <span class="attr">NCCL_SOCKET_IFNAME:</span> <span class="string">&quot;ibs0&quot;</span></span><br><span class="line">  <span class="attr">GLOO_SOCKET_IFNAME:</span> <span class="string">&quot;ibs0&quot;</span></span><br><span class="line"></span><br><span class="line">  <span class="comment"># 可选稳定性/诊断</span></span><br><span class="line">  <span class="attr">NCCL_DEBUG:</span> <span class="string">&quot;INFO&quot;</span></span><br><span class="line">  <span class="attr">NCCL_ASYNC_ERROR_HANDLING:</span> <span class="string">&quot;1&quot;</span></span><br><span class="line">  <span class="attr">CUDA_DEVICE_MAX_CONNECTIONS:</span> <span class="string">&quot;1&quot;</span></span><br><span class="line">  <span class="attr">TORCH_NCCL_AVOID_RECORD_STREAMS:</span> <span class="string">&quot;1&quot;</span></span><br></pre></td></tr></table></figure>
<ol start="6">
<li>启动脚本：</li>
</ol>
 <figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">- ray job submit --address=<span class="string">&quot;http://127.0.0.1:8265&quot;</span> \</span><br><span class="line">--runtime-env=verl/trainer/runtime_env.yaml \</span><br><span class="line"> --no-wait \</span><br><span class="line"> -- \</span><br><span class="line"> python3 -m verl.trainer.main_ppo \</span><br><span class="line"> trainer.n_gpus_per_node=8 \</span><br><span class="line"> trainer.nnodes=2 \</span><br><span class="line"> ...</span><br></pre></td></tr></table></figure>
<h3 id="ib带宽测试"><a href="#ib带宽测试" class="headerlink" title="ib带宽测试"></a>ib带宽测试</h3><p>apt install perftest</p>
<h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><ol>
<li><a target="_blank" rel="noopener" href="https://arxiv.org/abs/2504.15930">StreamRL: Scalable, Heterogeneous, and Elastic RL for LLMs with Disaggregated Stream Generation</a></li>
<li><a target="_blank" rel="noopener" href="https://arxiv.org/abs/2409.13221">Optimizing RLHF Training for Large Language Models with Stage Fusion</a></li>
</ol>

</article>
    
    

</div>
<div class="trm-post-next-prev row">
    <div class="col-lg-12">
        <!-- title -->
        <h5 class="trm-title-with-divider">
            其他文章
            <span data-number="02"></span>
        </h5>
    </div>
    
        <div class="col-lg-6">
    <div class="trm-blog-card trm-scroll-animation">
        <a href="/2025/08/14/lifetrace/" class="trm-cover-frame trm-anima-link">
            
            
                <img alt="cover" class="no-fancybox" src="https://www.logosc.cn/uploads/resources/2018/11/29/1543459457_thumb.jpg">
            
        </a>
        
        <div class="trm-card-descr">
            <div class="trm-label trm-category trm-mb-20">
                <a href=" /categories/%E8%AF%BB%E5%8D%9A%E8%AE%B0%E5%BD%95/">
                    读博记录
                </a>
            </div>
            <h5>
                <a href="/2025/08/14/lifetrace/" class="trm-anima-link">
                    生活小记
                </a>
            </h5>
            <div class="trm-divider trm-mb-20 trm-mt-20"></div>
            <ul class="trm-card-data trm-label">
                <li>25/08/14</li>
                <li>20:11</li>
                
                
            </ul>
        </div>
    </div>
</div>
    
    
        <div class="col-lg-6">
    <div class="trm-blog-card trm-scroll-animation">
        <a href="/2025/08/04/verl-perfornamce2/" class="trm-cover-frame trm-anima-link">
            
            
                <img alt="cover" class="no-fancybox" src="https://www.logosc.cn/uploads/resources/2018/11/29/1543459457_thumb.jpg">
            
        </a>
        
        <div class="trm-card-descr">
            <div class="trm-label trm-category trm-mb-20">
                <a href=" /categories/RLHF/">
                    RLHF
                </a>
            </div>
            <h5>
                <a href="/2025/08/04/verl-perfornamce2/" class="trm-anima-link">
                    verl性能分析（二）-实验分析
                </a>
            </h5>
            <div class="trm-divider trm-mb-20 trm-mt-20"></div>
            <ul class="trm-card-data trm-label">
                <li>25/08/04</li>
                <li>20:35</li>
                
                
            </ul>
        </div>
    </div>
</div>
    
</div>

    



                    <div class="trm-divider footer-divider"></div>

                    <!-- footer -->
                    <footer class="trm-footer-card trm-scroll-animation">

    

    

    
        <div class="trm-footer-item">
            <span>
                由 <a href="https://hexo.io" target="_blank" rel="noopener">Hexo</a> 驱动 v7.3.0
            </span>
            <span class="footer-separator" data-separator=" | "></span>
            <span> 
                主题 - 
                <a rel="noopener" href='https://github.com/MaLuns/hexo-theme-async' target='_blank'>Async</a>
                v2.2.6
            </span>
        </div>
      

     

     
</footer>
                    <!-- footer end -->

                </div>
            </div>
        </div>
    </div>
</div>
            <!-- body end -->

            

            
<div class="trm-fixed-container">
    
    
        <div class="trm-fixed-btn" data-title="阅读模式" onclick="asyncFun.switchReadMode()">
            <i class="iconfont fas fa-book-reader"></i>
        </div>
    
    
    <div id="trm-back-top" class="trm-fixed-btn" data-title="回到顶部">
        <i class="iconfont fas fa-arrow-up"></i>
    </div>
</div>
        </div>
      </div>
      <!-- scroll container end -->
  </div>
  <!-- app wrapper end -->

  
  <!-- Plugin -->




    
    
<script src="https://unpkg.com/@fancyapps/ui@4.0/dist/fancybox.umd.js"></script>

    

    

    

    <!-- 数学公式 -->
    

    <!-- 评论插件 -->
    
        

        
    

		




    <!-- Service Worker -->
    
    <!-- baidu push -->
    


<script id="async-script" src="/js/main.js?v=2.2.6"></script>

<!-- CDN -->


    

    

    



</body>

</html>