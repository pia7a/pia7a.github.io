<!DOCTYPE html>
<html lang="zh-Hans">

<head>
  <meta http-equiv="content-type" content="text/html; charset=utf-8">
<meta name="x5-fullscreen" content="true">
<meta name="full-screen" content="yes">
<meta name="theme-color" content="#317EFB" />
<meta content="width=device-width, initial-scale=1.0, maximum-scale=5.0, user-scalable=0" name="viewport">
<meta name="description" content="目的实现colocate的minibatch pipline，然后做资源调度，实现rollout和其他stage的计算通信重叠。9&#x2F;17：minibatch pipline pipline是实现了，调度还没想好怎么做，有些复杂。 实验all-to-all Qwen1.5-MoE-A2.7B-Chat VLLM: TP&#x3D;2 node:4  GPU per node: 6 **tr">
<meta property="og:type" content="article">
<meta property="og:title" content="verl改进（一）colocated minibatch实现">
<meta property="og:url" content="http://example.com/2025/09/04/verl-moe-all2all-2/index.html">
<meta property="og:site_name" content="Hexo">
<meta property="og:description" content="目的实现colocate的minibatch pipline，然后做资源调度，实现rollout和其他stage的计算通信重叠。9&#x2F;17：minibatch pipline pipline是实现了，调度还没想好怎么做，有些复杂。 实验all-to-all Qwen1.5-MoE-A2.7B-Chat VLLM: TP&#x3D;2 node:4  GPU per node: 6 **tr">
<meta property="og:locale">
<meta property="og:image" content="http://example.com/img/404.jpg">
<meta property="og:image" content="http://example.com/img/404.jpg">
<meta property="og:image" content="http://example.com/img/404.jpg">
<meta property="og:image" content="http://example.com/img/404.jpg">
<meta property="og:image" content="http://example.com/img/404.jpg">
<meta property="og:image" content="http://example.com/img/404.jpg">
<meta property="og:image" content="http://example.com/img/404.jpg">
<meta property="og:image" content="http://example.com/img/404.jpg">
<meta property="og:image" content="http://example.com/img/404.jpg">
<meta property="og:image" content="http://example.com/img/404.jpg">
<meta property="og:image" content="http://example.com/img/404.jpg">
<meta property="og:image" content="http://example.com/img/404.jpg">
<meta property="article:published_time" content="2025-09-04T01:26:24.000Z">
<meta property="article:modified_time" content="2025-09-19T01:09:52.157Z">
<meta property="article:author" content="John Doe">
<meta property="article:tag" content="MOE">
<meta property="article:tag" content="Verl框架">
<meta property="article:tag" content="性能分析">
<meta property="article:tag" content="all-to-all">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="http://example.com/img/404.jpg">

    <meta name="keywords" content="RLHF,Verl框架,性能分析,all-to-all">


<title >verl改进（一）colocated minibatch实现</title>

<!-- Favicon -->

    <link href='/img/favicon.svg?v=2.2.6' rel='icon' type='image/png' sizes='16x16' ></link>


    <link href='/img/favicon.svg?v=2.2.6' rel='icon' type='image/png' sizes='32x32' ></link>




<!-- Plugin -->




    
<link rel="stylesheet" href="/css/plugins/bootstrap.row.css">

    
<link rel="stylesheet" href="https://unpkg.com/@fancyapps/ui@4.0/dist/fancybox.css">

    
    




<!-- Icon -->

    
<link rel="stylesheet" href="/css/plugins/font-awesome.min.css">




<!-- Variable -->
<script>window.ASYNC_CONFIG = {"hostname":"example.com","author":"John Doe","root":"/","typed_text":null,"theme_version":"2.2.6","theme":{"switch":true,"default":"style-light"},"favicon":{"logo":"/img/favicon.svg","icon16":"/img/favicon.svg","icon32":"/img/favicon.svg","apple_touch_icon":null,"webmanifest":null,"visibilitychange":false,"hidden":"/failure.ico","show_text":"(/≧▽≦/)咦！又好了！","hide_text":"(●—●)喔哟，崩溃啦！"},"i18n":{"placeholder":"搜索文章...","empty":"找不到您查询的内容: ${query}","hits":"找到 ${hits} 条结果","hits_time":"找到 ${hits} 条结果（用时 ${time} 毫秒）","author":"本文作者：","copyright_link":"本文链接：","copyright_license_title":"版权声明：","copyright_license_content":"本博客所有文章除特别声明外，均默认采用 undefined 许可协议。","copy_success":"复制成功","copy_failure":"复制失败","open_read_mode":"进入阅读模式","exit_read_mode":"退出阅读模式","notice_outdate_message":"距离上次更新已经 undefined 天了, 文章内容可能已经过时。","sticky":"置顶","just":"刚刚","min":"分钟前","hour":"小时前","day":"天前","month":"个月前"},"swup":false,"plugin":{"flickr_justified_gallery":"https://unpkg.com/flickr-justified-gallery@latest/dist/fjGallery.min.js"},"icons":{"sun":"far fa-sun","moon":"far fa-moon","play":"fas fa-play","email":"far fa-envelope","next":"fas fa-arrow-right","calendar":"far fa-calendar-alt","clock":"far fa-clock","user":"far fa-user","back_top":"fas fa-arrow-up","close":"fas fa-times","search":"fas fa-search","reward":"fas fa-hand-holding-usd","toc_tag":"fas fa-th-list","read":"fas fa-book-reader","arrows":"fas fa-arrows-alt-h","double_arrows":"fas fa-angle-double-down","copy":"fas fa-copy"},"icontype":"font","highlight":{"plugin":"highlighjs","theme":true,"copy":true,"lang":true,"title":"default","height_limit":false},"toc":{"post_title":true},"live_time":{"start_time":"","prefix":"博客已萌萌哒运行 undefined 天"},"danmu":{"enable":false,"el":".trm-banner"}};</script>
<script id="async-page-config">window.PAGE_CONFIG = {"isPost":true,"isHome":false,"postUpdate":"2025-09-19 09:09:52"};</script>

<!-- Theme mode css -->
<link data-swup-theme rel="stylesheet" href="/css/index.css?v=2.2.6" id="trm-switch-style">
<script>
    let defaultMode = ASYNC_CONFIG.theme.default !=='auto' ?  ASYNC_CONFIG.theme.default : (window.matchMedia("(prefers-color-scheme: light)").matches ? 'style-light' : 'style-dark')
    let catchMode = localStorage.getItem('theme-mode') || defaultMode;
    let type = catchMode === 'style-dark' ? 'add' : 'remove';
    document.documentElement.classList[type]('dark')
</script>

<!-- CDN -->


    
    



<!-- Site Analytics -->

 
<meta name="generator" content="Hexo 7.3.0"></head>

<body>

  <!-- app wrapper -->
  <div class="trm-app-frame">

    <!-- page preloader -->
    <div class="trm-preloader">
    <div class="trm-holder">
        <div class="preloader">
            <div></div>
            <div></div>
            <div></div>
            <div></div>
            <div></div>
            <div></div>
            <div></div>
            <div></div>
            <div></div>
            <div></div>
        </div>
    </div>
</div>
    <!-- page preloader end -->

    <!-- change mode preloader -->
    <div class="trm-mode-swich-animation-frame">
    <div class="trm-mode-swich-animation">
        <i class="i-sun"><i class="iconfont far fa-sun"></i></i>
        <div class="trm-horizon"></div>
        <i class="i-moon"><i class="iconfont far fa-moon"></i></i>
    </div>
</div>
    <!-- change mode preloader end -->

      <!-- scroll container -->
      <div id="trm-dynamic-content" class="trm-swup-animation">
        <div id="trm-scroll-container" class="trm-scroll-container" style="opacity: 0">
            <!-- top bar -->
            <header class="trm-top-bar">
	<div class="container">
		<div class="trm-left-side">
			<!-- logo -->
<a href="/" class="trm-logo-frame trm-anima-link">
    
        <img alt="logo" src="/img/favicon.svg">
    
    
        <div class="trm-logo-text">
            Async<span>zhiqiang</span>
        </div>
    
</a>
<!-- logo end -->
		</div>
		<div class="trm-right-side">
			<!-- menu -->
<div class="trm-menu">
    <nav>
        <ul>
            
            <li class="menu-item-has-children ">
                <a  href="/" target="">
                    首页
                </a>
                
            </li>
            
            <li class="menu-item-has-children ">
                <a  href="/archives/" target="">
                    时间线
                </a>
                
            </li>
            
            <li class="menu-item-has-children ">
                <a  href="/tags/" target="">
                    标签
                </a>
                
            </li>
            
            <li class="menu-item-has-children ">
                <a  href="/categories" target="">
                    分类
                </a>
                
            </li>
            
        </ul>
    </nav>
</div>
<!-- menu end -->
			
    <!-- mode switcher place -->
    <div class="trm-mode-switcher-place">
        <div class="trm-mode-switcher">
            <i class="iconfont far fa-sun"></i>
            <input class="tgl tgl-light" id="trm-swich" type="checkbox">
            <label class="trm-swich" for="trm-swich"></label>
            <i class="iconfont far fa-moon"></i>
        </div>
    </div>
    <!-- mode switcher place end -->

			
		</div>
		<div class="trm-menu-btn">
			<span></span>
		</div>
	</div>
</header>
            <!-- top bar end -->

            <!-- body -->
            
<div class="trm-content-start">
    <!-- banner -->
    <div class="trm-banner">
    
    <!-- banner cover -->
    <img style="object-position:top;object-fit:cover;" alt="banner" class="trm-banner-cover" src="https://pic1.zhimg.com/v2-b3c2c6745b9421a13a3c4706b19223b3_r.jpg">
    <!-- banner cover end -->
    

    <!-- banner content -->
    <div class="trm-banner-content trm-overlay">
        <div class="container">
            <div class="row">
                
                <div class="col-lg-4"></div>
                
                <div class="col-lg-8">

                    <!-- banner title -->
                    <div class="trm-banner-text ">
                        <div class="trm-label trm-mb-20">
                            more paper, more fun
                        </div>
                        <h1 class="trm-mb-30 trm-hsmb-font">
                            verl改进（一）colocated minibatch实现
                        </h1>

                        
                            <ul class="trm-breadcrumbs trm-label">
                                <li>
                                    <a href="/" class="trm-anima-link">Home</a>
                                </li>
                                <li>
                                    <span>
                                        2025
                                    </span>
                                </li>
                            </ul>
                        
                    </div>
                    <!-- banner title end -->

                    <!-- scroll hint -->
                    <span id="scroll-triger" class="trm-scroll-hint-frame">
                        <div class="trm-scroll-hint"></div>
                        <span class="trm-label">Scroll down</span>
                    </span>
                    <!-- scroll hint end -->

                </div>
            </div>
        </div>
    </div>
    <!-- banner content end -->
</div>
    <!-- banner end -->
    <div class="container">
        <div class="row">
            
                <div class="trm-page-sidebar col-lg-4 hidden-sm">
                    <!-- main card -->
                    <div class="trm-main-card-frame trm-sidebar">
    <div class="trm-main-card"> 
        <!-- card header -->
<div class="trm-mc-header">
    <div class="trm-avatar-frame trm-mb-20">
        <img alt="Avatar" class="trm-avatar" src="/img/avatar.jpg">
    </div>
    <h5 class="trm-name trm-mb-15">
        看雪
    </h5>
    
</div>
<!-- card header end -->
        <!-- sidebar social -->

<div class="trm-divider trm-mb-40 trm-mt-40"></div>
<div class="trm-social">
    
        <a href="https://github.com" title="Github" rel="nofollow" target="_blank">
            <i class="iconfont fab fa-github"></i>
        </a>
    
</div>

<!-- sidebar social end -->
        <!-- info -->
<div class="trm-divider trm-mb-40 trm-mt-40"></div>
<ul class="trm-table trm-mb-20">
    
        <li>
            <div class="trm-label">
                Residence:
            </div>
            <div class="trm-label trm-label-light">
                Mars
            </div>
        </li>
    
</ul>
<!-- info end -->

        
    </div>
</div>
                    <!-- main card end -->
                </div>
            
            <div class="trm-page-content col-lg-8">
                <div id="trm-content" class="trm-content">
                    <div class="trm-post-info row hidden-sm">
    <div class="col-sm-4">
        <div class="trm-card trm-label trm-label-light text-center">
            <i class="iconfont far fa-calendar-alt trm-icon"></i><br>
            09/04
        </div>
    </div>
    <div class="col-sm-4">
        <div class="trm-card trm-label trm-label-light text-center">
            <i class="iconfont far fa-clock trm-icon"></i><br>
            09:26
        </div>
    </div>
    <div class="col-sm-4">
        <div id="post-author" class="trm-card trm-label trm-label-light text-center">
            <i class="iconfont far fa-user trm-icon"></i><br>
            John Doe
        </div>
    </div>
</div>
<div class="trm-card ">
    <article id="article-container" class="trm-publication">
    <h1 id="目的"><a href="#目的" class="headerlink" title="目的"></a>目的</h1><p>实现colocate的minibatch pipline，然后做资源调度，实现rollout和其他stage的计算通信重叠。<br>9&#x2F;17：minibatch pipline pipline是实现了，调度还没想好怎么做，有些复杂。</p>
<h1 id="实验"><a href="#实验" class="headerlink" title="实验"></a>实验</h1><h2 id="all-to-all"><a href="#all-to-all" class="headerlink" title="all-to-all"></a>all-to-all</h2><ul>
<li><strong>Qwen1.5-MoE-A2.7B-Chat</strong></li>
<li>VLLM: TP&#x3D;2</li>
<li>node:4 </li>
<li>GPU per node: 6</li>
<li>**train_batch_size&#x3D;1024 **</li>
<li>max_prompt_length&#x3D;256 </li>
<li>max_response_length&#x3D;1024 </li>
<li><strong>mini_batch_size&#x3D;256</strong></li>
<li>micro_batch_size_per_gpu&#x3D;8</li>
<li><strong>rollout_n&#x3D;3</strong></li>
<li>param_offload&#x3D;False </li>
<li>grad_offload&#x3D;False</li>
<li>optimizer_offload&#x3D;False</li>
</ul>
<table>
<thead>
<tr>
<th>megatron并行策略</th>
<th>all-to-all占比</th>
<th>rollout占比</th>
</tr>
</thead>
<tbody><tr>
<td>(ep&#x3D;6,tp&#x3D;2)</td>
<td>51.01%</td>
<td>15.54%</td>
</tr>
<tr>
<td>(ep&#x3D;6,tp&#x3D;4)</td>
<td>51.80%</td>
<td>15.44%</td>
</tr>
<tr>
<td>(ep&#x3D;12,tp&#x3D;2)</td>
<td>39.32%</td>
<td>27.16%</td>
</tr>
</tbody></table>
<h2 id="overlap-methods"><a href="#overlap-methods" class="headerlink" title="overlap methods"></a>overlap methods</h2><img src="/2025/09/04/verl-moe-all2all-2/Users\25490\Desktop\blog\source\_posts\verl-moe-all2all-2\methods.png" alt="image-20250918142158375" style="zoom: 50%;"  data-tag='post-image' onload='this.onload=null;this.style.opacity=1;' loading="lazy" onerror='this.onerror=null;this.src="/img/404.jpg"'>


<h2 id="four-GPUs："><a href="#four-GPUs：" class="headerlink" title="four GPUs："></a>four GPUs：</h2><ul>
<li><strong>Qwen3-30B-A3B-Instruct-2507</strong>，隐藏层仅保留2层</li>
<li>actor_update: Megatron(EP&#x3D;4, TP&#x3D;1)</li>
<li>rollout: VLLM(TP&#x3D;1)</li>
</ul>
<h3 id="实验1："><a href="#实验1：" class="headerlink" title="实验1："></a>实验1：</h3><ul>
<li>**train_batch_size&#x3D;128 **</li>
<li>max_prompt_length&#x3D;256 </li>
<li>max_response_length&#x3D;1024 </li>
<li><strong>mini_batch_size&#x3D;32</strong></li>
<li>micro_batch_size_per_gpu&#x3D;2</li>
<li><strong>rollout_n&#x3D;2</strong></li>
<li>param_offload&#x3D;True </li>
<li>grad_offload&#x3D;True</li>
<li>optimizer_offload&#x3D;True</li>
</ul>
<table>
<thead>
<tr>
<th>type</th>
<th>time per batch</th>
<th>归一化</th>
</tr>
</thead>
<tbody><tr>
<td>baseline</td>
<td>34.62s</td>
<td>1</td>
</tr>
<tr>
<td>minibatch</td>
<td>70.77s</td>
<td>1.93</td>
</tr>
<tr>
<td>minibatch_pipline</td>
<td>69.34s</td>
<td>1.89</td>
</tr>
<tr>
<td>stream_minibatch_pipline</td>
<td>60.02s</td>
<td>1.63</td>
</tr>
</tbody></table>
<h3 id="实验2："><a href="#实验2：" class="headerlink" title="实验2："></a>实验2：</h3><ul>
<li><strong>train_batch_size&#x3D;1024</strong></li>
<li>max_prompt_length&#x3D;256 </li>
<li>max_response_length&#x3D;1024 </li>
<li><strong>mini_batch_size&#x3D;256</strong></li>
<li>micro_batch_size_per_gpu&#x3D;2</li>
<li><strong>rollout_n&#x3D;2</strong></li>
<li>param_offload&#x3D;True </li>
<li>grad_offload&#x3D;True</li>
<li>optimizer_offload&#x3D;True</li>
</ul>
<table>
<thead>
<tr>
<th>type</th>
<th>time per batch</th>
<th>归一化</th>
</tr>
</thead>
<tbody><tr>
<td>baseline</td>
<td>92.73s</td>
<td>1</td>
</tr>
<tr>
<td>minibatch</td>
<td>138.17s</td>
<td>1.49</td>
</tr>
<tr>
<td>minibatch_pipline</td>
<td>142.57s</td>
<td>1.54</td>
</tr>
<tr>
<td>stream_minibatch_pipline</td>
<td>118.77s</td>
<td>1.28</td>
</tr>
</tbody></table>
<h3 id="实验3："><a href="#实验3：" class="headerlink" title="实验3："></a>实验3：</h3><ul>
<li><strong>train_batch_size&#x3D;256</strong></li>
<li>max_prompt_length&#x3D;256 </li>
<li>max_response_length&#x3D;1024 </li>
<li><strong>mini_batch_size&#x3D;64</strong></li>
<li>micro_batch_size_per_gpu&#x3D;2</li>
<li><strong>rollout_n&#x3D;4</strong></li>
<li>param_offload&#x3D;True </li>
<li>grad_offload&#x3D;True</li>
<li>optimizer_offload&#x3D;True</li>
</ul>
<table>
<thead>
<tr>
<th>type</th>
<th>time per batch</th>
<th>归一化</th>
</tr>
</thead>
<tbody><tr>
<td>baseline</td>
<td>58.21s</td>
<td>1</td>
</tr>
<tr>
<td>minibatch</td>
<td>102.46s</td>
<td>1.76</td>
</tr>
<tr>
<td>minibatch_pipline</td>
<td>118.39s</td>
<td>2.03</td>
</tr>
<tr>
<td>stream_minibatch_pipline</td>
<td>84.79s</td>
<td>1.46</td>
</tr>
</tbody></table>
<h3 id="实验4："><a href="#实验4：" class="headerlink" title="实验4："></a>实验4：</h3><ul>
<li><strong>train_batch_size&#x3D;512</strong></li>
<li>max_prompt_length&#x3D;256 </li>
<li>max_response_length&#x3D;1024 </li>
<li><strong>mini_batch_size&#x3D;128</strong></li>
<li>micro_batch_size_per_gpu&#x3D;2</li>
<li><strong>rollout_n&#x3D;4</strong></li>
<li>param_offload&#x3D;True </li>
<li>grad_offload&#x3D;True</li>
<li>optimizer_offload&#x3D;True</li>
</ul>
<table>
<thead>
<tr>
<th>type</th>
<th>time per batch</th>
<th>归一化</th>
</tr>
</thead>
<tbody><tr>
<td>baseline</td>
<td>92.73s</td>
<td>1</td>
</tr>
<tr>
<td>minibatch</td>
<td>137.89s</td>
<td>1.49</td>
</tr>
<tr>
<td>minibatch_pipline</td>
<td>144.55s</td>
<td>1.56</td>
</tr>
<tr>
<td>stream_minibatch_pipline</td>
<td>120.45s</td>
<td>1.30</td>
</tr>
</tbody></table>
<h3 id="实验5："><a href="#实验5：" class="headerlink" title="实验5："></a>实验5：</h3><ul>
<li><strong>train_batch_size&#x3D;1024</strong></li>
<li>max_prompt_length&#x3D;256 </li>
<li>max_response_length&#x3D;1024 </li>
<li><strong>mini_batch_size&#x3D;256</strong></li>
<li>micro_batch_size_per_gpu&#x3D;2</li>
<li><strong>rollout_n&#x3D;4</strong></li>
<li>param_offload&#x3D;True </li>
<li>grad_offload&#x3D;True</li>
<li>optimizer_offload&#x3D;True</li>
</ul>
<table>
<thead>
<tr>
<th>type</th>
<th>time per batch</th>
<th>归一化</th>
</tr>
</thead>
<tbody><tr>
<td>baseline</td>
<td>161.54s</td>
<td>1</td>
</tr>
<tr>
<td>minibatch</td>
<td>213.52s</td>
<td>1.32</td>
</tr>
<tr>
<td>minibatch_pipline</td>
<td>213.27s</td>
<td>1.32</td>
</tr>
<tr>
<td>stream_minibatch_pipline</td>
<td>188.68s</td>
<td>1.17</td>
</tr>
</tbody></table>
<h3 id="batchsize的影响"><a href="#batchsize的影响" class="headerlink" title="batchsize的影响"></a>batchsize的影响</h3><p><img src="/2025/09/04/verl-moe-all2all-2/Users\25490\Desktop\blog\source_posts\verl-moe-all2all-2\four_method.png" alt="test"  data-tag='post-image' onload='this.onload=null;this.style.opacity=1;' loading="lazy" onerror='this.onerror=null;this.src="/img/404.jpg"'></p>
<ul>
<li><p>minibatch：</p>
<p>相比baseline耗时更长。</p>
<ul>
<li>每个minibatch都有长度较长的samples,长尾效应更加严重,相比于baseline在rollout未饱和的情况下(batchsize较大时,vllm也会内部切分多次推理),时间为$\frac{batch}{minibatch} \times longest sample$</li>
<li>param、grad和optimizer在baseline中只需要offload一次, 在minibatch中需要offload $\frac{batch}{minibatch}$次，与模型大小有关。</li>
<li>dirver process与rank process进程间通过ray.remote和ray.get交换数据，baseline中数据传输一次，在minibatch中需要 $\frac{batch}{minibatch}$次（尽管数据总量不变），但多次传输耗时仍更大。</li>
<li>minibatch在rollout时GPU利用率更低，bachsize越小越明显。</li>
<li>其他由于对batch切分而引起的多次cpu-gpu、进程间等通信开销。</li>
</ul>
<p>以上时间消耗随batchsize的变化增加较小（45.44s、45.16s、51.73s），minibatch_pipline和stream_minibatch_pipline同样有这部分原因。</p>
</li>
<li><p>minibatch_pipline</p>
<p>比minibatch耗时稍大，直接overlap会导致竞争,导致overlap的stage耗时都增长,总耗时和串行接近.</p>
</li>
<li><p>stream_minibatch_pipline</p>
</li>
</ul>
<p>​	相较于minibatch和minibatch_pipline,缓解了长尾效应的影响.</p>
<img src="/2025/09/04/verl-moe-all2all-2/Users\25490\Desktop\blog\source\_posts\verl-moe-all2all-2\lengths_bin.png" alt="lengths_bin" style="zoom:67%;"  data-tag='post-image' onload='this.onload=null;this.style.opacity=1;' loading="lazy" onerror='this.onerror=null;this.src="/img/404.jpg"'>

<ul>
<li>baseline:</li>
</ul>
<table>
<thead>
<tr>
<th>stage</th>
<th>时间</th>
</tr>
</thead>
<tbody><tr>
<td>gen</td>
<td>62s</td>
</tr>
<tr>
<td>update_actor</td>
<td>163s</td>
</tr>
<tr>
<td>old_prob</td>
<td>49s</td>
</tr>
<tr>
<td>ref_prob</td>
<td>42s</td>
</tr>
</tbody></table>
<p><img src="/2025/09/04/verl-moe-all2all-2/Users\25490\Desktop\blog\source_posts\verl-moe-all2all-2\baseline_timeline.png" alt="image-20250918143914555"  data-tag='post-image' onload='this.onload=null;this.style.opacity=1;' loading="lazy" onerror='this.onerror=null;this.src="/img/404.jpg"'></p>
<ul>
<li>minibatch:</li>
</ul>
<table>
<thead>
<tr>
<th>stage</th>
<th>时间</th>
</tr>
</thead>
<tbody><tr>
<td>gen</td>
<td>60s</td>
</tr>
<tr>
<td>update_actor</td>
<td>170s</td>
</tr>
<tr>
<td>old_prob</td>
<td>57s</td>
</tr>
<tr>
<td>ref_prob</td>
<td>47s</td>
</tr>
</tbody></table>
<p><img src="/2025/09/04/verl-moe-all2all-2/Users\25490\AppData\Roaming\Typora\typora-user-images\minibatch_timeline.png" alt="image-20250918143823783"  data-tag='post-image' onload='this.onload=null;this.style.opacity=1;' loading="lazy" onerror='this.onerror=null;this.src="/img/404.jpg"'></p>
<ul>
<li>minibatch pipline: gen与old_prob重叠,但总耗时与串行相比接近.</li>
</ul>
<table>
<thead>
<tr>
<th>stage</th>
<th>时间</th>
</tr>
</thead>
<tbody><tr>
<td>gen</td>
<td>117s</td>
</tr>
<tr>
<td>update_actor</td>
<td>170s</td>
</tr>
<tr>
<td>old_prob</td>
<td>109s</td>
</tr>
<tr>
<td>ref_prob</td>
<td>47s</td>
</tr>
</tbody></table>
<p><img src="/2025/09/04/verl-moe-all2all-2/Users\25490\AppData\Roaming\Typora\typora-user-images\minibatch_pipline.png" alt="image-20250918144116108"  data-tag='post-image' onload='this.onload=null;this.style.opacity=1;' loading="lazy" onerror='this.onerror=null;this.src="/img/404.jpg"'></p>
<ul>
<li>stream minibatch pipline:</li>
</ul>
<table>
<thead>
<tr>
<th>stage</th>
<th>时间</th>
</tr>
</thead>
<tbody><tr>
<td>gen</td>
<td>81s</td>
</tr>
<tr>
<td>update_actor</td>
<td>175s</td>
</tr>
<tr>
<td>old_prob</td>
<td>88.5s</td>
</tr>
<tr>
<td>ref_prob</td>
<td>46.5s</td>
</tr>
</tbody></table>
<p><img src="/2025/09/04/verl-moe-all2all-2/Users\25490\AppData\Roaming\Typora\typora-user-images\stream_minibatch_pipline_timeline.png" alt="image-20250918144318163"  data-tag='post-image' onload='this.onload=null;this.style.opacity=1;' loading="lazy" onerror='this.onerror=null;this.src="/img/404.jpg"'></p>
<p>直接overlap后,会有一些计算和通信重叠,但粗粒度的overlap收益很小:</p>
<p><img src="/2025/09/04/verl-moe-all2all-2/Users\25490\AppData\Roaming\Typora\typora-user-images\direct_overlap_kernel.png" alt="image-20250918145245930"  data-tag='post-image' onload='this.onload=null;this.style.opacity=1;' loading="lazy" onerror='this.onerror=null;this.src="/img/404.jpg"'></p>
<h2 id="batchsize大小对推理时间的影响"><a href="#batchsize大小对推理时间的影响" class="headerlink" title="batchsize大小对推理时间的影响"></a>batchsize大小对推理时间的影响</h2><ul>
<li>硬件：1*a6000</li>
<li>数据集：gsm8k</li>
<li>模型：Qwen2.5-0.5B-Instruct</li>
<li>推理|训练：推理使用VLLM，训练为Megatron</li>
<li>放置方式：colocated</li>
</ul>
<table>
<thead>
<tr>
<th>batchsize * n</th>
<th>时间</th>
</tr>
</thead>
<tbody><tr>
<td>8*3</td>
<td>4.08s</td>
</tr>
<tr>
<td>16*3</td>
<td>4.23s</td>
</tr>
<tr>
<td>32*3</td>
<td>4.63s</td>
</tr>
<tr>
<td>64*3</td>
<td>5.39s</td>
</tr>
<tr>
<td>128*3</td>
<td>7.95s</td>
</tr>
<tr>
<td>256*3</td>
<td>14.26s</td>
</tr>
<tr>
<td>512*3</td>
<td>29.51s</td>
</tr>
<tr>
<td>1024*3</td>
<td>60.23s</td>
</tr>
<tr>
<td>2048*3</td>
<td>126.37s</td>
</tr>
</tbody></table>
<ul>
<li>硬件：8*a6000</li>
<li>数据集：gsm8k</li>
<li>模型：qwen_2_5_3B_Instruct（2层）</li>
<li>推理|训练：推理使用VLLM（tp&#x3D;2），训练为Megatron（tp&#x3D;2）</li>
<li>放置方式：colocated</li>
</ul>
<table>
<thead>
<tr>
<th>batchsize * n</th>
<th>时间</th>
</tr>
</thead>
<tbody><tr>
<td>256*3</td>
<td>8.38s</td>
</tr>
<tr>
<td>512*3</td>
<td>10.10s</td>
</tr>
<tr>
<td>1024*3</td>
<td>18.00s</td>
</tr>
<tr>
<td>2048*3</td>
<td>31.23s</td>
</tr>
</tbody></table>
<h2 id="minibatch切分对推理效率的影响"><a href="#minibatch切分对推理效率的影响" class="headerlink" title="minibatch切分对推理效率的影响"></a>minibatch切分对推理效率的影响</h2><ul>
<li>硬件：8*a6000</li>
<li>数据集：gsm8k</li>
<li>模型：qwen_2_5_3B_Instruct（2层）</li>
<li>推理|训练：VLLM（tp&#x3D;2），训练为Megatron（ep&#x3D;8）</li>
<li>放置方式：colocated</li>
</ul>
<h3 id="baseline"><a href="#baseline" class="headerlink" title="baseline"></a>baseline</h3><p>batchsize: 1024<br>minibatchsize: 128<br>max_prompt_length: 256<br>max_response_length: 256</p>
<p>时间: 57.78s</p>
<h3 id="minibath"><a href="#minibath" class="headerlink" title="minibath"></a>minibath</h3><table>
<thead>
<tr>
<th>minibatch_size</th>
<th>时间</th>
</tr>
</thead>
<tbody><tr>
<td>128</td>
<td>79.87s</td>
</tr>
<tr>
<td>256</td>
<td>68.23s</td>
</tr>
<tr>
<td>512</td>
<td>显存全部占满，一个GPU计算拉满，卡住</td>
</tr>
<tr>
<td>1024</td>
<td>null</td>
</tr>
</tbody></table>
<h1 id="后续工作"><a href="#后续工作" class="headerlink" title="后续工作"></a>后续工作</h1><ol>
<li>解决overlap时VLLM TP&gt;1情况下NCCL死锁的问题.</li>
<li>想一个好的调度策略,使overlap不增加训练时间.</li>
</ol>
<h1 id="工作记录"><a href="#工作记录" class="headerlink" title="工作记录"></a>工作记录</h1><p>9&#x2F;4：今天技术评估，想到两个idea，一个是<a target="_blank" rel="noopener" href="https://github.com/volcengine/verl/pull/2200">reorder rollout for tackling long-tail generation problem</a>，一个是根据history sample时间粗糙估计长样本分布，总体都是减少rollout时间。</p>
<p>9&#x2F;5：大概需要实现三个版本：minibatch、minibatch pipline、stream minibatch pipline，从以上三个版本中选一个好的再实现bubble的减少。</p>
<p>9&#x2F;6：实现minibatch，没有overlap的流程</p>
<p>9&#x2F;7:   实现异步LLM,非在线服务,没必要.</p>
<p>9&#x2F;8：实现一个流式的LLM,</p>
<p>9&#x2F;9：实现多线程（但是随机状态好像没有考虑?）</p>
<p>马克思：补交课堂工作，回答问题</p>
<p>管理：分组</p>
<p>先进人工智能：思考和讨论的问题：如果让你展开充分的想象，你研究的领域在10年、50年或100年后会变成什么样？用一个故事来描述一下这个场景</p>
<p>9&#x2F;10：终于实现了基于VLLM的LLM接口的minibatch pipline，但是耗时由72s增加到90多秒，不应该这么大的，可能是driver process和worker process之间频繁通信的结果，尝试只传数据的量，不传原始数据，通过桶策略后，rollout恢复正常了，但是training的时间好长啊，后来发现注释掉aggressive_empty_cache(force_sync&#x3D;True)后多出来的时间减少了很多，多出来的一点大概是访存问题吧</p>
<p>9&#x2F;11：发现actor_update的时间确实有些长，有点崩溃，准备好好profile一下</p>
<p>9&#x2F;12：重构了相关代码，不然维护起来太麻烦了，发现前几天多出来的时间不是scale up的</p>
<p>9&#x2F;13：开始上四台机器了，遇到GLOO的问题，指定ib端口就OK了；遇到ray突然主动退出，（grep -r “67379” &#x2F;tmp&#x2F;ray&#x2F;session_latest&#x2F;logs&#x2F;）那是因为代码有bug，绑定的方法名字写错了，用环境变量传参不太行啊，还是在config中绑定了；终于跑起来qwen30b了，但是OOM，换deepseek7b-moe也不行，megatron不支持啊，问问翔哥；实验最新的minibatch pipline，max_response_length一旦增大到1024，就会把batch打满直接卡死，爆出一大堆类似的错误：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">(WorkerDict pid=272892, ip=192.158.0.14) frame <span class="comment">#1: &lt;unknown function&gt; + 0x11b4a6e (0x7ff2872a8a6e in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cuda.so)</span></span><br><span class="line">(WorkerDict pid=272892, ip=192.158.0.14) frame <span class="comment">#2: &lt;unknown function&gt; + 0xe07bed (0x7ff286efbbed in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cuda.so)</span></span><br><span class="line">(WorkerDict pid=272892, ip=192.158.0.14) frame <span class="comment">#3: &lt;unknown function&gt; + 0xdc253 (0x7ff8e821e253 in /lib/x86_64-linux-gnu/libstdc++.so.6)</span></span><br><span class="line">(WorkerDict pid=272892, ip=192.158.0.14) frame <span class="comment">#4: &lt;unknown function&gt; + 0x94ac3 (0x7ff8ecddaac3 in /lib/x86_64-linux-gnu/libc.so.6)</span></span><br><span class="line">(WorkerDict pid=272892, ip=192.158.0.14) frame <span class="comment">#5: &lt;unknown function&gt; + 0x126850 (0x7ff8ece6c850 in /lib/x86_64-linux-gnu/libc.so.6)</span></span><br><span class="line">(WorkerDict pid=272892, ip=192.158.0.14) </span><br><span class="line">(WorkerDict pid=272892, ip=192.158.0.14) [2025-09-13 12:51:44,295 E 272892 275408] logging.cc:119: Stack trace: </span><br><span class="line">(WorkerDict pid=272892, ip=192.158.0.14)  /usr/local/lib/python3.10/dist-packages/ray/_raylet.so(+0x1464c1a) [0x7ff8e97d2c1a] ray::operator&lt;&lt;()</span><br><span class="line">(WorkerDict pid=272892, ip=192.158.0.14) /usr/local/lib/python3.10/dist-packages/ray/_raylet.so(+0x14681f2) [0x7ff8e97d61f2] ray::TerminateHandler()</span><br><span class="line">(WorkerDict pid=272892, ip=192.158.0.14) /lib/x86_64-linux-gnu/libstdc++.so.6(+0xae20c) [0x7ff8e81f020c]</span><br><span class="line">(WorkerDict pid=272892, ip=192.158.0.14) /lib/x86_64-linux-gnu/libstdc++.so.6(+0xae277) [0x7ff8e81f0277]</span><br><span class="line">(WorkerDict pid=272892, ip=192.158.0.14) /lib/x86_64-linux-gnu/libstdc++.so.6(+0xae1fe) [0x7ff8e81f01fe]</span><br><span class="line">(WorkerDict pid=272892, ip=192.158.0.14) /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cuda.so(+0xe07c7b) [0x7ff286efbc7b] c10d::ProcessGroupNCCL::ncclCommWatchdog()</span><br><span class="line">(WorkerDict pid=272892, ip=192.158.0.14) /lib/x86_64-linux-gnu/libstdc++.so.6(+0xdc253) [0x7ff8e821e253]</span><br><span class="line">(WorkerDict pid=272892, ip=192.158.0.14) /lib/x86_64-linux-gnu/libc.so.6(+0x94ac3) [0x7ff8ecddaac3]</span><br><span class="line">(WorkerDict pid=272892, ip=192.158.0.14) /lib/x86_64-linux-gnu/libc.so.6(+0x126850) [0x7ff8ece6c850]</span><br><span class="line">(WorkerDict pid=272892, ip=192.158.0.14) </span><br></pre></td></tr></table></figure>

<p>在四台机器上跑更大的模型，即便max_response_length达到256也不行了，应该是OOM</p>
<p>9&#x2F;14：粗暴的overlap会导致OOM。</p>
<p>9&#x2F;15：只要计算资源和内存够，还是有收益的。</p>
<p>9&#x2F;16: 卡死了，但不是显存的问题,是通信的问题。</p>
<p><img src="/2025/09/04/verl-moe-all2all-2/Users\25490\Desktop\blog\source_posts\verl-moe-all2all-2\NCCL_stuck1.png" alt="image-20250916093720578"  data-tag='post-image' onload='this.onload=null;this.style.opacity=1;' loading="lazy" onerror='this.onerror=null;this.src="/img/404.jpg"'></p>
<p><img src="/2025/09/04/verl-moe-all2all-2/Users\25490\Desktop\blog\source_posts\verl-moe-all2all-2\NCCL_stuck2.png" alt="image-20250916104906045"  data-tag='post-image' onload='this.onload=null;this.style.opacity=1;' loading="lazy" onerror='this.onerror=null;this.src="/img/404.jpg"'></p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[rank4]:[E913 11:47:58.292915725 ProcessGroupNCCL.cpp:632] [Rank 0] Watchdog caught collective operation <span class="built_in">timeout</span>: WorkNCCL(SeqNum=609, OpType=_ALLGATHER_BASE, NumelIn=21043136, NumelOut=42086272, Timeout(ms)=600000) ran <span class="keyword">for</span> 600034 milliseconds before timing out.</span><br></pre></td></tr></table></figure>

<p>NCCL死锁了</p>
<p>我自己写的schedule_test里面，如果并行的两个小worker都有计算和通信，也会发生这个问题</p>
<p>9&#x2F;17：机器崩了，后面几天只有一张卡能用了, 整理报告。</p>
<h1 id="Tips"><a href="#Tips" class="headerlink" title="Tips"></a>Tips</h1><h2 id="Bug"><a href="#Bug" class="headerlink" title="Bug"></a>Bug</h2><h2 id="知识"><a href="#知识" class="headerlink" title="知识"></a>知识</h2><h3 id="多线程"><a href="#多线程" class="headerlink" title="多线程"></a>多线程</h3><ul>
<li>不join会怎么样？守护线程会随主线程退出，非守护线程会一直跑</li>
<li>其他线程raise会怎么样？主进程会退出</li>
</ul>
<h3 id="VLLM的推理机制"><a href="#VLLM的推理机制" class="headerlink" title="VLLM的推理机制"></a>VLLM的推理机制</h3><h3 id="Verl的训练过程"><a href="#Verl的训练过程" class="headerlink" title="Verl的训练过程"></a>Verl的训练过程</h3><img src="/2025/09/04/verl-moe-all2all-2/Users\25490\Desktop\blog\source\_posts\verl-moe-all2all-2\GRPO.png" alt="GRPO" style="zoom: 67%;"  data-tag='post-image' onload='this.onload=null;this.style.opacity=1;' loading="lazy" onerror='this.onerror=null;this.src="/img/404.jpg"'> 

<img src="/2025/09/04/verl-moe-all2all-2/Users\25490\Desktop\blog\source\_posts\verl-moe-all2all-2\bach_size.png" alt="ref_log_prob" style="zoom:80%;"  data-tag='post-image' onload='this.onload=null;this.style.opacity=1;' loading="lazy" onerror='this.onerror=null;this.src="/img/404.jpg"'>

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">@GPUMemoryLogger(<span class="params">role=<span class="string">&quot;dp actor&quot;</span>, logger=logger</span>)</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">update_policy</span>(<span class="params">self, data: DataProto</span>):</span><br><span class="line">    <span class="comment"># make sure we are in training mode</span></span><br><span class="line">    <span class="variable language_">self</span>.actor_module.train()</span><br><span class="line"></span><br><span class="line">    temperature = data.meta_info[<span class="string">&quot;temperature&quot;</span>]  <span class="comment"># temperature must be in the data.meta_info to avoid silent error</span></span><br><span class="line"></span><br><span class="line">    select_keys = [</span><br><span class="line">        <span class="string">&quot;responses&quot;</span>,</span><br><span class="line">        <span class="string">&quot;response_mask&quot;</span>,</span><br><span class="line">        <span class="string">&quot;input_ids&quot;</span>,</span><br><span class="line">        <span class="string">&quot;attention_mask&quot;</span>,</span><br><span class="line">        <span class="string">&quot;position_ids&quot;</span>,</span><br><span class="line">        <span class="string">&quot;old_log_probs&quot;</span>,</span><br><span class="line">        <span class="string">&quot;advantages&quot;</span>,</span><br><span class="line">    ]</span><br><span class="line">    <span class="keyword">if</span> <span class="variable language_">self</span>.config.use_kl_loss:</span><br><span class="line">        select_keys.append(<span class="string">&quot;ref_log_prob&quot;</span>)</span><br><span class="line">    <span class="keyword">if</span> <span class="variable language_">self</span>.config.tis_imp_ratio_cap &gt; <span class="number">0</span>:</span><br><span class="line">        <span class="keyword">assert</span> <span class="string">&quot;rollout_log_probs&quot;</span> <span class="keyword">in</span> data.batch.keys(), (</span><br><span class="line">            <span class="string">&quot;Truncated Importance Sampling (TIS) requires to configure &quot;</span></span><br><span class="line">            <span class="string">&quot;`actor_rollout_ref.rollout.calculate_log_probs=True` &quot;</span></span><br><span class="line">            <span class="string">&quot;and is not currently supported in Server mode (agent loop).&quot;</span></span><br><span class="line">        )</span><br><span class="line">        select_keys.append(<span class="string">&quot;rollout_log_probs&quot;</span>)</span><br><span class="line"></span><br><span class="line">    has_multi_modal_inputs = <span class="string">&quot;multi_modal_inputs&quot;</span> <span class="keyword">in</span> data.non_tensor_batch.keys()</span><br><span class="line">    non_tensor_select_keys = [<span class="string">&quot;multi_modal_inputs&quot;</span>] <span class="keyword">if</span> has_multi_modal_inputs <span class="keyword">else</span> []</span><br><span class="line"></span><br><span class="line">    data = data.select(batch_keys=select_keys, non_tensor_batch_keys=non_tensor_select_keys)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Split to make minibatch iterator for updating the actor</span></span><br><span class="line">    <span class="comment"># See PPO paper for details. https://arxiv.org/abs/1707.06347</span></span><br><span class="line">    mini_batches = data.split(<span class="variable language_">self</span>.config.ppo_mini_batch_size)</span><br><span class="line"></span><br><span class="line">    on_policy = <span class="built_in">len</span>(mini_batches) == <span class="number">1</span> <span class="keyword">and</span> <span class="variable language_">self</span>.config.ppo_epochs == <span class="number">1</span></span><br><span class="line"></span><br><span class="line">    metrics = &#123;&#125;</span><br><span class="line">    <span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(<span class="variable language_">self</span>.config.ppo_epochs):</span><br><span class="line">        <span class="keyword">for</span> batch_idx, mini_batch <span class="keyword">in</span> <span class="built_in">enumerate</span>(mini_batches):</span><br><span class="line">            <span class="keyword">if</span> <span class="variable language_">self</span>.config.use_dynamic_bsz:</span><br><span class="line">                max_token_len = <span class="variable language_">self</span>.config.ppo_max_token_len_per_gpu * <span class="variable language_">self</span>.ulysses_sequence_parallel_size</span><br><span class="line">                micro_batches, _ = prepare_dynamic_batch(mini_batch, max_token_len=max_token_len)</span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                <span class="variable language_">self</span>.gradient_accumulation = (</span><br><span class="line">                    <span class="variable language_">self</span>.config.ppo_mini_batch_size // <span class="variable language_">self</span>.config.ppo_micro_batch_size_per_gpu</span><br><span class="line">                )</span><br><span class="line">                micro_batches = mini_batch.split(<span class="variable language_">self</span>.config.ppo_micro_batch_size_per_gpu)</span><br><span class="line"></span><br><span class="line">            <span class="variable language_">self</span>.actor_optimizer.zero_grad()</span><br><span class="line"></span><br><span class="line">            <span class="keyword">for</span> micro_batch <span class="keyword">in</span> micro_batches:</span><br><span class="line">                micro_batch = micro_batch.to(get_device_id())</span><br><span class="line">                micro_batch_metrics = &#123;&#125;</span><br><span class="line">                model_inputs = &#123;**micro_batch.batch, **micro_batch.non_tensor_batch&#125;</span><br><span class="line">                response_mask = model_inputs[<span class="string">&quot;response_mask&quot;</span>]</span><br><span class="line">                old_log_prob = model_inputs[<span class="string">&quot;old_log_probs&quot;</span>]</span><br><span class="line">                rollout_log_probs = model_inputs[<span class="string">&quot;rollout_log_probs&quot;</span>] <span class="keyword">if</span> <span class="variable language_">self</span>.config.tis_imp_ratio_cap &gt; <span class="number">0</span> <span class="keyword">else</span> <span class="literal">None</span></span><br><span class="line">                advantages = model_inputs[<span class="string">&quot;advantages&quot;</span>]</span><br><span class="line"></span><br><span class="line">                entropy_coeff = <span class="variable language_">self</span>.config.entropy_coeff</span><br><span class="line">                loss_agg_mode = <span class="variable language_">self</span>.config.loss_agg_mode</span><br><span class="line"></span><br><span class="line">                <span class="keyword">if</span> <span class="variable language_">self</span>.config.use_dynamic_bsz:</span><br><span class="line">                    loss_scale_factor = response_mask.shape[<span class="number">0</span>] / <span class="variable language_">self</span>.config.ppo_mini_batch_size</span><br><span class="line">                <span class="keyword">else</span>:</span><br><span class="line">                    loss_scale_factor = <span class="number">1</span> / <span class="variable language_">self</span>.gradient_accumulation</span><br><span class="line"></span><br><span class="line">                <span class="comment"># all return: (bsz, response_length)</span></span><br><span class="line">                calculate_entropy = <span class="literal">False</span></span><br><span class="line">                <span class="keyword">if</span> entropy_coeff != <span class="number">0</span>:</span><br><span class="line">                    calculate_entropy = <span class="literal">True</span></span><br><span class="line">                entropy, log_prob = <span class="variable language_">self</span>._forward_micro_batch(</span><br><span class="line">                    model_inputs, temperature=temperature, calculate_entropy=calculate_entropy</span><br><span class="line">                )</span><br><span class="line"></span><br><span class="line">                <span class="keyword">if</span> on_policy:</span><br><span class="line">                    old_log_prob = log_prob.detach()</span><br><span class="line">                <span class="keyword">else</span>:</span><br><span class="line">                    old_log_prob = model_inputs[<span class="string">&quot;old_log_probs&quot;</span>]</span><br><span class="line"></span><br><span class="line">                loss_mode = <span class="variable language_">self</span>.config.policy_loss.get(<span class="string">&quot;loss_mode&quot;</span>, <span class="string">&quot;vanilla&quot;</span>)</span><br><span class="line">                <span class="comment"># vanilla -&gt; verl.trainer.ppo.core_algos.compute_policy_loss_vanilla</span></span><br><span class="line">                <span class="comment"># gpg -&gt; verl.trainer.ppo.core_algos.compute_policy_loss_gpg</span></span><br><span class="line">                <span class="comment"># clip_cov -&gt; verl.trainer.ppo.core_algos.compute_policy_loss_clip_cov</span></span><br><span class="line">                policy_loss_fn = get_policy_loss_fn(loss_mode)</span><br><span class="line">                pg_loss, pg_clipfrac, ppo_kl, pg_clipfrac_lower = policy_loss_fn(</span><br><span class="line">                    old_log_prob=old_log_prob,</span><br><span class="line">                    log_prob=log_prob,</span><br><span class="line">                    advantages=advantages,</span><br><span class="line">                    response_mask=response_mask,</span><br><span class="line">                    loss_agg_mode=loss_agg_mode,</span><br><span class="line">                    config=<span class="variable language_">self</span>.config,</span><br><span class="line">                    rollout_log_probs=rollout_log_probs,</span><br><span class="line">                )</span><br><span class="line"></span><br><span class="line">                <span class="keyword">if</span> entropy_coeff != <span class="number">0</span>:</span><br><span class="line">                    entropy_loss = agg_loss(loss_mat=entropy, loss_mask=response_mask, loss_agg_mode=loss_agg_mode)</span><br><span class="line"></span><br><span class="line">                    <span class="comment"># compute policy loss</span></span><br><span class="line">                    policy_loss = pg_loss - entropy_loss * entropy_coeff</span><br><span class="line">                <span class="keyword">else</span>:</span><br><span class="line">                    policy_loss = pg_loss</span><br><span class="line"></span><br><span class="line">                <span class="keyword">if</span> <span class="variable language_">self</span>.config.use_kl_loss:</span><br><span class="line">                    ref_log_prob = model_inputs[<span class="string">&quot;ref_log_prob&quot;</span>]</span><br><span class="line">                    <span class="comment"># compute kl loss</span></span><br><span class="line">                    kld = kl_penalty(</span><br><span class="line">                        logprob=log_prob, ref_logprob=ref_log_prob, kl_penalty=<span class="variable language_">self</span>.config.kl_loss_type</span><br><span class="line">                    )</span><br><span class="line">                    kl_loss = agg_loss(loss_mat=kld, loss_mask=response_mask, loss_agg_mode=loss_agg_mode)</span><br><span class="line"></span><br><span class="line">                    policy_loss = policy_loss + kl_loss * <span class="variable language_">self</span>.config.kl_loss_coef</span><br><span class="line">                    micro_batch_metrics[<span class="string">&quot;actor/kl_loss&quot;</span>] = kl_loss.detach().item() * loss_scale_factor</span><br><span class="line">                    micro_batch_metrics[<span class="string">&quot;actor/kl_coef&quot;</span>] = <span class="variable language_">self</span>.config.kl_loss_coef</span><br><span class="line"></span><br><span class="line">                <span class="keyword">if</span> <span class="variable language_">self</span>.config.use_dynamic_bsz:</span><br><span class="line">                    <span class="comment"># relative to the dynamic bsz</span></span><br><span class="line">                    loss = policy_loss * loss_scale_factor</span><br><span class="line">                <span class="keyword">else</span>:</span><br><span class="line">                    loss = policy_loss * loss_scale_factor</span><br><span class="line">                loss.backward()</span><br><span class="line"></span><br><span class="line">                micro_batch_metrics.update(</span><br><span class="line">                    &#123;</span><br><span class="line">                        <span class="string">&quot;actor/pg_loss&quot;</span>: pg_loss.detach().item() * loss_scale_factor,</span><br><span class="line">                        <span class="string">&quot;actor/pg_clipfrac&quot;</span>: pg_clipfrac.detach().item(),</span><br><span class="line">                        <span class="string">&quot;actor/ppo_kl&quot;</span>: ppo_kl.detach().item(),</span><br><span class="line">                        <span class="string">&quot;actor/pg_clipfrac_lower&quot;</span>: pg_clipfrac_lower.detach().item(),</span><br><span class="line">                    &#125;</span><br><span class="line">                )</span><br><span class="line">                append_to_dict(metrics, micro_batch_metrics)</span><br><span class="line"></span><br><span class="line">            grad_norm = <span class="variable language_">self</span>._optimizer_step()</span><br><span class="line">            mini_batch_metrics = &#123;<span class="string">&quot;actor/grad_norm&quot;</span>: grad_norm.detach().item()&#125;</span><br><span class="line">            append_to_dict(metrics, mini_batch_metrics)</span><br><span class="line">    <span class="variable language_">self</span>.actor_optimizer.zero_grad()</span><br><span class="line">    <span class="keyword">return</span> metrics</span><br></pre></td></tr></table></figure>
</article>
    
    

</div>
<div class="trm-post-next-prev row">
    <div class="col-lg-12">
        <!-- title -->
        <h5 class="trm-title-with-divider">
            其他文章
            <span data-number="02"></span>
        </h5>
    </div>
    
        <div class="col-lg-6">
    <div class="trm-blog-card trm-scroll-animation">
        <a href="/2025/09/14/verl-moe-all2all-3/" class="trm-cover-frame trm-anima-link">
            
            
                <img alt="cover" class="no-fancybox" src="https://www.logosc.cn/uploads/resources/2018/11/29/1543459457_thumb.jpg">
            
        </a>
        
        <div class="trm-card-descr">
            <div class="trm-label trm-category trm-mb-20">
                <a href=" /categories/RLHF/">
                    RLHF
                </a>
            </div>
            <h5>
                <a href="/2025/09/14/verl-moe-all2all-3/" class="trm-anima-link">
                    verl改进（一）-all-to-all实验分析
                </a>
            </h5>
            <div class="trm-divider trm-mb-20 trm-mt-20"></div>
            <ul class="trm-card-data trm-label">
                <li>25/09/14</li>
                <li>16:57</li>
                
                
            </ul>
        </div>
    </div>
</div>
    
    
        <div class="col-lg-6">
    <div class="trm-blog-card trm-scroll-animation">
        <a href="/2025/09/01/paper-review820/" class="trm-cover-frame trm-anima-link">
            
            
                <img alt="cover" class="no-fancybox" src="https://www.logosc.cn/uploads/resources/2018/11/29/1543459457_thumb.jpg">
            
        </a>
        
        <div class="trm-card-descr">
            <div class="trm-label trm-category trm-mb-20">
                <a href=" /categories/%E6%96%87%E7%8C%AE%E6%80%BB%E7%BB%93/">
                    文献总结
                </a>
            </div>
            <h5>
                <a href="/2025/09/01/paper-review820/" class="trm-anima-link">
                    文献总结（2025-08-20-2025-09-01）
                </a>
            </h5>
            <div class="trm-divider trm-mb-20 trm-mt-20"></div>
            <ul class="trm-card-data trm-label">
                <li>25/09/01</li>
                <li>11:02</li>
                
                
            </ul>
        </div>
    </div>
</div>
    
</div>

    



                    <div class="trm-divider footer-divider"></div>

                    <!-- footer -->
                    <footer class="trm-footer-card trm-scroll-animation">

    

    

    
        <div class="trm-footer-item">
            <span>
                由 <a href="https://hexo.io" target="_blank" rel="noopener">Hexo</a> 驱动 v7.3.0
            </span>
            <span class="footer-separator" data-separator=" | "></span>
            <span> 
                主题 - 
                <a rel="noopener" href='https://github.com/MaLuns/hexo-theme-async' target='_blank'>Async</a>
                v2.2.6
            </span>
        </div>
      

     

     
</footer>
                    <!-- footer end -->

                </div>
            </div>
        </div>
    </div>
</div>
            <!-- body end -->

            

            
<div class="trm-fixed-container">
    
    
        <div class="trm-fixed-btn" data-title="阅读模式" onclick="asyncFun.switchReadMode()">
            <i class="iconfont fas fa-book-reader"></i>
        </div>
    
    
    <div id="trm-back-top" class="trm-fixed-btn" data-title="回到顶部">
        <i class="iconfont fas fa-arrow-up"></i>
    </div>
</div>
        </div>
      </div>
      <!-- scroll container end -->
  </div>
  <!-- app wrapper end -->

  
  <!-- Plugin -->




    
    
<script src="https://unpkg.com/@fancyapps/ui@4.0/dist/fancybox.umd.js"></script>

    

    

    

    <!-- 数学公式 -->
    

    <!-- 评论插件 -->
    
        

        
    

		




    <!-- Service Worker -->
    
    <!-- baidu push -->
    


<script id="async-script" src="/js/main.js?v=2.2.6"></script>

<!-- CDN -->


    

    

    



</body>

</html>