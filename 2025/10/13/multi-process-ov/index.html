<!DOCTYPE html>
<html lang="zh-Hans">

<head>
  <meta http-equiv="content-type" content="text/html; charset=utf-8">
<meta name="x5-fullscreen" content="true">
<meta name="full-screen" content="yes">
<meta name="theme-color" content="#317EFB" />
<meta content="width=device-width, initial-scale=1.0, maximum-scale=5.0, user-scalable=0" name="viewport">
<meta name="description" content="上周工作 之前遇到的问题：GIL锁  除了跨进程的参数同步，其他已完成，主要工作在将之前多线程改为多进程，并处理ray框架和进程通信问题。   目的 让推理在通信期间执行，实现 通信–计算重叠，提高 GPU 利用率并降低空闲时间  Rollout与actor_update并行效率分析。   方法一个GPU绑定两个进程并行，actor_upadte 、ref_policy等任务在一个进程，rollo">
<meta property="og:type" content="article">
<meta property="og:title" content="multi_process_ov">
<meta property="og:url" content="http://example.com/2025/10/13/multi-process-ov/index.html">
<meta property="og:site_name" content="Hexo">
<meta property="og:description" content="上周工作 之前遇到的问题：GIL锁  除了跨进程的参数同步，其他已完成，主要工作在将之前多线程改为多进程，并处理ray框架和进程通信问题。   目的 让推理在通信期间执行，实现 通信–计算重叠，提高 GPU 利用率并降低空闲时间  Rollout与actor_update并行效率分析。   方法一个GPU绑定两个进程并行，actor_upadte 、ref_policy等任务在一个进程，rollo">
<meta property="og:locale">
<meta property="og:image" content="http://example.com/img/404.jpg">
<meta property="og:image" content="http://example.com/img/404.jpg">
<meta property="og:image" content="http://example.com/img/404.jpg">
<meta property="og:image" content="http://example.com/img/404.jpg">
<meta property="og:image" content="http://example.com/img/404.jpg">
<meta property="og:image" content="http://example.com/img/404.jpg">
<meta property="og:image" content="http://example.com/img/404.jpg">
<meta property="article:published_time" content="2025-10-13T11:45:23.000Z">
<meta property="article:modified_time" content="2025-10-30T12:59:28.809Z">
<meta property="article:author" content="John Doe">
<meta property="article:tag" content="Verl框架">
<meta property="article:tag" content="性能分析">
<meta property="article:tag" content="多进程">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="http://example.com/img/404.jpg">

    <meta name="keywords" content="RLHF,Verl框架,性能分析">


<title >multi_process_ov</title>

<!-- Favicon -->

    <link href='/img/favicon.svg?v=2.2.6' rel='icon' type='image/png' sizes='16x16' ></link>


    <link href='/img/favicon.svg?v=2.2.6' rel='icon' type='image/png' sizes='32x32' ></link>




<!-- Plugin -->




    
<link rel="stylesheet" href="/css/plugins/bootstrap.row.css">

    
<link rel="stylesheet" href="https://unpkg.com/@fancyapps/ui@4.0/dist/fancybox.css">

    
    




<!-- Icon -->

    
<link rel="stylesheet" href="/css/plugins/font-awesome.min.css">




<!-- Variable -->
<script>window.ASYNC_CONFIG = {"hostname":"example.com","author":"John Doe","root":"/","typed_text":null,"theme_version":"2.2.6","theme":{"switch":true,"default":"style-light"},"favicon":{"logo":"/img/favicon.svg","icon16":"/img/favicon.svg","icon32":"/img/favicon.svg","apple_touch_icon":null,"webmanifest":null,"visibilitychange":false,"hidden":"/failure.ico","show_text":"(/≧▽≦/)咦！又好了！","hide_text":"(●—●)喔哟，崩溃啦！"},"i18n":{"placeholder":"搜索文章...","empty":"找不到您查询的内容: ${query}","hits":"找到 ${hits} 条结果","hits_time":"找到 ${hits} 条结果（用时 ${time} 毫秒）","author":"本文作者：","copyright_link":"本文链接：","copyright_license_title":"版权声明：","copyright_license_content":"本博客所有文章除特别声明外，均默认采用 undefined 许可协议。","copy_success":"复制成功","copy_failure":"复制失败","open_read_mode":"进入阅读模式","exit_read_mode":"退出阅读模式","notice_outdate_message":"距离上次更新已经 undefined 天了, 文章内容可能已经过时。","sticky":"置顶","just":"刚刚","min":"分钟前","hour":"小时前","day":"天前","month":"个月前"},"swup":false,"plugin":{"flickr_justified_gallery":"https://unpkg.com/flickr-justified-gallery@latest/dist/fjGallery.min.js"},"icons":{"sun":"far fa-sun","moon":"far fa-moon","play":"fas fa-play","email":"far fa-envelope","next":"fas fa-arrow-right","calendar":"far fa-calendar-alt","clock":"far fa-clock","user":"far fa-user","back_top":"fas fa-arrow-up","close":"fas fa-times","search":"fas fa-search","reward":"fas fa-hand-holding-usd","toc_tag":"fas fa-th-list","read":"fas fa-book-reader","arrows":"fas fa-arrows-alt-h","double_arrows":"fas fa-angle-double-down","copy":"fas fa-copy"},"icontype":"font","highlight":{"plugin":"highlighjs","theme":true,"copy":true,"lang":true,"title":"default","height_limit":false},"toc":{"post_title":true},"live_time":{"start_time":"","prefix":"博客已萌萌哒运行 undefined 天"},"danmu":{"enable":false,"el":".trm-banner"}};</script>
<script id="async-page-config">window.PAGE_CONFIG = {"isPost":true,"isHome":false,"postUpdate":"2025-10-30 20:59:28"};</script>

<!-- Theme mode css -->
<link data-swup-theme rel="stylesheet" href="/css/index.css?v=2.2.6" id="trm-switch-style">
<script>
    let defaultMode = ASYNC_CONFIG.theme.default !=='auto' ?  ASYNC_CONFIG.theme.default : (window.matchMedia("(prefers-color-scheme: light)").matches ? 'style-light' : 'style-dark')
    let catchMode = localStorage.getItem('theme-mode') || defaultMode;
    let type = catchMode === 'style-dark' ? 'add' : 'remove';
    document.documentElement.classList[type]('dark')
</script>

<!-- CDN -->


    
    



<!-- Site Analytics -->

 
<meta name="generator" content="Hexo 7.3.0"></head>

<body>

  <!-- app wrapper -->
  <div class="trm-app-frame">

    <!-- page preloader -->
    <div class="trm-preloader">
    <div class="trm-holder">
        <div class="preloader">
            <div></div>
            <div></div>
            <div></div>
            <div></div>
            <div></div>
            <div></div>
            <div></div>
            <div></div>
            <div></div>
            <div></div>
        </div>
    </div>
</div>
    <!-- page preloader end -->

    <!-- change mode preloader -->
    <div class="trm-mode-swich-animation-frame">
    <div class="trm-mode-swich-animation">
        <i class="i-sun"><i class="iconfont far fa-sun"></i></i>
        <div class="trm-horizon"></div>
        <i class="i-moon"><i class="iconfont far fa-moon"></i></i>
    </div>
</div>
    <!-- change mode preloader end -->

      <!-- scroll container -->
      <div id="trm-dynamic-content" class="trm-swup-animation">
        <div id="trm-scroll-container" class="trm-scroll-container" style="opacity: 0">
            <!-- top bar -->
            <header class="trm-top-bar">
	<div class="container">
		<div class="trm-left-side">
			<!-- logo -->
<a href="/" class="trm-logo-frame trm-anima-link">
    
        <img alt="logo" src="/img/favicon.svg">
    
    
        <div class="trm-logo-text">
            Async<span>zhiqiang</span>
        </div>
    
</a>
<!-- logo end -->
		</div>
		<div class="trm-right-side">
			<!-- menu -->
<div class="trm-menu">
    <nav>
        <ul>
            
            <li class="menu-item-has-children ">
                <a  href="/" target="">
                    首页
                </a>
                
            </li>
            
            <li class="menu-item-has-children ">
                <a  href="/archives/" target="">
                    时间线
                </a>
                
            </li>
            
            <li class="menu-item-has-children ">
                <a  href="/tags/" target="">
                    标签
                </a>
                
            </li>
            
            <li class="menu-item-has-children ">
                <a  href="/categories" target="">
                    分类
                </a>
                
            </li>
            
        </ul>
    </nav>
</div>
<!-- menu end -->
			
    <!-- mode switcher place -->
    <div class="trm-mode-switcher-place">
        <div class="trm-mode-switcher">
            <i class="iconfont far fa-sun"></i>
            <input class="tgl tgl-light" id="trm-swich" type="checkbox">
            <label class="trm-swich" for="trm-swich"></label>
            <i class="iconfont far fa-moon"></i>
        </div>
    </div>
    <!-- mode switcher place end -->

			
		</div>
		<div class="trm-menu-btn">
			<span></span>
		</div>
	</div>
</header>
            <!-- top bar end -->

            <!-- body -->
            
<div class="trm-content-start">
    <!-- banner -->
    <div class="trm-banner">
    
    <!-- banner cover -->
    <img style="object-position:top;object-fit:cover;" alt="banner" class="trm-banner-cover" src="https://pic1.zhimg.com/v2-b3c2c6745b9421a13a3c4706b19223b3_r.jpg">
    <!-- banner cover end -->
    

    <!-- banner content -->
    <div class="trm-banner-content trm-overlay">
        <div class="container">
            <div class="row">
                
                <div class="col-lg-4"></div>
                
                <div class="col-lg-8">

                    <!-- banner title -->
                    <div class="trm-banner-text ">
                        <div class="trm-label trm-mb-20">
                            Hi my new friend!
                        </div>
                        <h1 class="trm-mb-30 trm-hsmb-font">
                            multi_process_ov
                        </h1>

                        
                            <ul class="trm-breadcrumbs trm-label">
                                <li>
                                    <a href="/" class="trm-anima-link">Home</a>
                                </li>
                                <li>
                                    <span>
                                        2025
                                    </span>
                                </li>
                            </ul>
                        
                    </div>
                    <!-- banner title end -->

                    <!-- scroll hint -->
                    <span id="scroll-triger" class="trm-scroll-hint-frame">
                        <div class="trm-scroll-hint"></div>
                        <span class="trm-label">Scroll down</span>
                    </span>
                    <!-- scroll hint end -->

                </div>
            </div>
        </div>
    </div>
    <!-- banner content end -->
</div>
    <!-- banner end -->
    <div class="container">
        <div class="row">
            
                <div class="trm-page-sidebar col-lg-4 hidden-sm">
                    <!-- main card -->
                    <div class="trm-main-card-frame trm-sidebar">
    <div class="trm-main-card"> 
        <!-- card header -->
<div class="trm-mc-header">
    <div class="trm-avatar-frame trm-mb-20">
        <img alt="Avatar" class="trm-avatar" src="/img/avatar.jpg">
    </div>
    <h5 class="trm-name trm-mb-15">
        看雪
    </h5>
    
</div>
<!-- card header end -->
        <!-- sidebar social -->

<div class="trm-divider trm-mb-40 trm-mt-40"></div>
<div class="trm-social">
    
        <a href="https://github.com" title="Github" rel="nofollow" target="_blank">
            <i class="iconfont fab fa-github"></i>
        </a>
    
</div>

<!-- sidebar social end -->
        <!-- info -->
<div class="trm-divider trm-mb-40 trm-mt-40"></div>
<ul class="trm-table trm-mb-20">
    
        <li>
            <div class="trm-label">
                Residence:
            </div>
            <div class="trm-label trm-label-light">
                Mars
            </div>
        </li>
    
</ul>
<!-- info end -->

        
    </div>
</div>
                    <!-- main card end -->
                </div>
            
            <div class="trm-page-content col-lg-8">
                <div id="trm-content" class="trm-content">
                    <div class="trm-post-info row hidden-sm">
    <div class="col-sm-4">
        <div class="trm-card trm-label trm-label-light text-center">
            <i class="iconfont far fa-calendar-alt trm-icon"></i><br>
            10/13
        </div>
    </div>
    <div class="col-sm-4">
        <div class="trm-card trm-label trm-label-light text-center">
            <i class="iconfont far fa-clock trm-icon"></i><br>
            19:45
        </div>
    </div>
    <div class="col-sm-4">
        <div id="post-author" class="trm-card trm-label trm-label-light text-center">
            <i class="iconfont far fa-user trm-icon"></i><br>
            John Doe
        </div>
    </div>
</div>
<div class="trm-card ">
    <article id="article-container" class="trm-publication">
    <h1 id="上周工作"><a href="#上周工作" class="headerlink" title="上周工作"></a>上周工作</h1><ul>
<li><p>之前遇到的问题：GIL锁</p>
</li>
<li><p>除了跨进程的参数同步，其他已完成，主要工作在将之前多线程改为多进程，并处理ray框架和进程通信问题。</p>
</li>
</ul>
<h1 id="目的"><a href="#目的" class="headerlink" title="目的"></a>目的</h1><ul>
<li><p>让推理在通信期间执行，实现 <strong>通信–计算重叠</strong>，提高 GPU 利用率并降低空闲时间</p>
</li>
<li><p>Rollout与actor_update并行效率分析。</p>
</li>
</ul>
<h1 id="方法"><a href="#方法" class="headerlink" title="方法"></a>方法</h1><p>一个GPU绑定两个进程并行，actor_upadte 、ref_policy等任务在一个进程，rollout单独在一个进程。<br>绑定在一个GPU上的进程通过共享内存通信，通过比较训练和推理经过的”检查点“次数控制overlap频率。</p>
<ul>
<li><strong>Sensitive Zone（并行窗口）</strong>：允许进程并行运行的时间片。</li>
<li><strong>Scheduler（调度器）</strong>：通过共享内存和条件变量控制两个进程的同步节奏。</li>
<li><strong>关键状态变量：</strong><ul>
<li><code>zone</code>：是否处于并行区（1&#x3D;开启窗口，0&#x3D;关闭窗口）</li>
<li><code>allow</code>：训练进程已完成的步骤个数（train-done）</li>
<li><code>check</code>：推理线程已完成的步骤个数（infer-done）</li>
</ul>
</li>
</ul>
<table>
<thead>
<tr>
<th>动作</th>
<th>执行方</th>
<th>说明</th>
</tr>
</thead>
<tbody><tr>
<td><code>sensitive_zone_start()</code></td>
<td>训练&#x2F;推理</td>
<td>开启并行窗口 (<code>zone=1</code>)</td>
</tr>
<tr>
<td><code>train_done()</code></td>
<td>训练</td>
<td>allow+&#x3D;1，执行完一次训练步</td>
</tr>
<tr>
<td><code>train_check_point()</code></td>
<td>训练</td>
<td>比较allow和check，若推理滞后自旋等待</td>
</tr>
<tr>
<td><code>infer_check_point()</code></td>
<td>推理</td>
<td>比较allow和check，若训练滞后自旋等待</td>
</tr>
<tr>
<td><code>infer_done()</code></td>
<td>推理</td>
<td>check+&#x3D;1，执行完一次推理步</td>
</tr>
<tr>
<td><code>senstive_zone_end_notify_train()</code></td>
<td>训练</td>
<td>关闭窗口并结束训练进程自旋</td>
</tr>
<tr>
<td><code>senstive_zone_end_notify_infer()</code></td>
<td>推理</td>
<td>关闭窗口并结束推理进程自旋</td>
</tr>
<tr>
<td><code>v.infer_steps</code> &#x2F; <code>v.train_steps</code></td>
<td>调度参数</td>
<td>控制推理与训练的步频比（如 1:2等）</td>
</tr>
</tbody></table>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">sequenceDiagram</span><br><span class="line">    participant Train as 训练流(A2A所在进程)</span><br><span class="line">    participant Infer as 推理流(rollout/vLLM)</span><br><span class="line">    participant Sched as 调度器(共享内存)</span><br><span class="line">    participant GPU as GPU</span><br><span class="line"></span><br><span class="line">    Note over Train,Infer: 开始调度</span><br><span class="line">    Train-&gt;&gt;GPU: launch all_to_all(async_op=True)</span><br><span class="line">    Train--&gt;&gt;Sched: train_done()  </span><br><span class="line">    par </span><br><span class="line">        Sched--&gt;&gt;Infer: infer_check_point()</span><br><span class="line">    and overlap start</span><br><span class="line">    	GPU-&gt;&gt;GPU: vllm step</span><br><span class="line">        GPU-&gt;&gt;GPU:  all-to-all kernels</span><br><span class="line">    and overlap end</span><br><span class="line">    	Infer--&gt;&gt;Sched: infer_done()</span><br><span class="line">    	Sched--&gt;&gt;Train: train_check_point()</span><br><span class="line">        Train-&gt;&gt;GPU: handle.wait() </span><br><span class="line">    end</span><br><span class="line">    Note over Train,Infer: 调度结束</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<h1 id="实验"><a href="#实验" class="headerlink" title="实验"></a>实验</h1><ul>
<li><strong>Qwen3-30B-A3B-Instruct-2507</strong>，隐藏层仅保留2层</li>
</ul>
<h2 id="8-GPUs"><a href="#8-GPUs" class="headerlink" title="8 GPUs"></a>8 GPUs</h2><ul>
<li><p>actor_update: Megatron(EP&#x3D;8, TP&#x3D;1)</p>
</li>
<li><p>rollout: VLLM(TP&#x3D;4)</p>
</li>
<li><p><strong>train_batch_size&#x3D;512</strong></p>
</li>
<li><p>max_prompt_length&#x3D;512 </p>
</li>
<li><p>max_response_length&#x3D;512</p>
</li>
<li><p><strong>mini_batch_size&#x3D;256</strong></p>
</li>
<li><p>micro_batch_size_per_gpu&#x3D;2</p>
</li>
<li><p><strong>rollout_n&#x3D;4</strong></p>
</li>
<li><p>param_offload&#x3D;False</p>
</li>
<li><p>grad_offload&#x3D;True</p>
</li>
<li><p>optimizer_offload&#x3D;True</p>
</li>
</ul>
<table>
<thead>
<tr>
<th>type</th>
<th>time per batch</th>
<th>归一化</th>
</tr>
</thead>
<tbody><tr>
<td>baseline</td>
<td>59.77s</td>
<td>1</td>
</tr>
<tr>
<td>one-step-off-policy-colocated</td>
<td>55.47s</td>
<td>0.93</td>
</tr>
<tr>
<td>one-step-off-policy-colocated-schedule</td>
<td>53.17s</td>
<td>0.89</td>
</tr>
<tr>
<td>minibatch</td>
<td>61.19s</td>
<td>1.024</td>
</tr>
<tr>
<td>minibatch-pipline</td>
<td>60.74s</td>
<td>1.016</td>
</tr>
<tr>
<td>minibatch-pipline-schedule</td>
<td>59.38s</td>
<td>0.993</td>
</tr>
</tbody></table>
<p><img src="/2025/10/13/multi-process-ov/Users\25490\AppData\Roaming\Typora\typora-user-images\image-20251014162958363.png" alt="image-20251014162958363"  data-tag='post-image' onload='this.onload=null;this.style.opacity=1;' loading="lazy" onerror='this.onerror=null;this.src="/img/404.jpg"'></p>
<h2 id="16-GPUs（需要再测）"><a href="#16-GPUs（需要再测）" class="headerlink" title="16 GPUs（需要再测）"></a>16 GPUs（需要再测）</h2><ul>
<li><p>actor_update: Megatron(EP&#x3D;16, TP&#x3D;1)</p>
</li>
<li><p>rollout: VLLM(TP&#x3D;4)</p>
</li>
<li><p><strong>train_batch_size&#x3D;512</strong></p>
</li>
<li><p>max_prompt_length&#x3D;512 </p>
</li>
<li><p>max_response_length&#x3D;512</p>
</li>
<li><p><strong>mini_batch_size&#x3D;256</strong></p>
</li>
<li><p>micro_batch_size_per_gpu&#x3D;2</p>
</li>
<li><p><strong>rollout_n&#x3D;4</strong></p>
</li>
<li><p>param_offload&#x3D;False</p>
</li>
<li><p>grad_offload&#x3D;True</p>
</li>
<li><p>optimizer_offload&#x3D;True</p>
</li>
</ul>
<table>
<thead>
<tr>
<th>type</th>
<th>time per batch</th>
<th>归一化</th>
</tr>
</thead>
<tbody><tr>
<td>baseline</td>
<td>42.03s</td>
<td>1</td>
</tr>
<tr>
<td>one-step-off-policy-colocated</td>
<td>36.86s</td>
<td>0.877</td>
</tr>
<tr>
<td>one-step-off-policy-colocated-schedule</td>
<td>37.29s</td>
<td>0.887</td>
</tr>
<tr>
<td>minibatch</td>
<td>42.91s</td>
<td>1.021</td>
</tr>
<tr>
<td>minibatch-pipline</td>
<td>41.69s</td>
<td>0.992</td>
</tr>
<tr>
<td>minibatch-pipline-schedule</td>
<td>42.45s</td>
<td>1.010</td>
</tr>
</tbody></table>
<p><img src="/2025/10/13/multi-process-ov/Users\25490\Desktop\blog\source_posts\multi-process-ov\16GOU_time.png" alt="image-20251014152748764"  data-tag='post-image' onload='this.onload=null;this.style.opacity=1;' loading="lazy" onerror='this.onerror=null;this.src="/img/404.jpg"'></p>
<ul>
<li>相比于切minibatch，one-step-off-policy算法上rollout与actor_update重叠区域最大。</li>
<li>直接重叠比调度（目前一个all2all一个step）收益更大，也可能是调度(1:1)不够合理？。</li>
<li>16GPU用的time测量，可能不如8GPU用Nsight测量profile准确。</li>
</ul>
<h3 id="8GPU的重叠情况如下："><a href="#8GPU的重叠情况如下：" class="headerlink" title="8GPU的重叠情况如下："></a>8GPU的重叠情况如下：</h3><p><img src="/2025/10/13/multi-process-ov/Users\25490\AppData\Roaming\Typora\typora-user-images\image-20251014154158634.png" alt="image-20251014154158634"  data-tag='post-image' onload='this.onload=null;this.style.opacity=1;' loading="lazy" onerror='this.onerror=null;this.src="/img/404.jpg"'></p>
<ul>
<li>A2A时间如果太短，无法有效利用，如第二个A2A</li>
</ul>
<h2 id="不做调度直接重叠"><a href="#不做调度直接重叠" class="headerlink" title="不做调度直接重叠"></a>不做调度直接重叠</h2><p><img src="/2025/10/13/multi-process-ov/Users\25490\AppData\Roaming\Typora\typora-user-images\image-20251014161518751.png" alt="image-20251014161518751"  data-tag='post-image' onload='this.onload=null;this.style.opacity=1;' loading="lazy" onerror='this.onerror=null;this.src="/img/404.jpg"'></p>
<ul>
<li>会把一个VLLM step拉得很长，跨几个all-to-all</li>
</ul>
<h3 id="计划"><a href="#计划" class="headerlink" title="计划"></a>计划</h3><p><img src="/2025/10/13/multi-process-ov/Users\25490\AppData\Roaming\Typora\typora-user-images\image-20251014162601224.png" alt="image-20251014162601224"  data-tag='post-image' onload='this.onload=null;this.style.opacity=1;' loading="lazy" onerror='this.onerror=null;this.src="/img/404.jpg"'></p>
<img src="/2025/10/13/multi-process-ov/Users\25490\Desktop\blog\source\_posts\verl-moe-all2all-2\methods.png" alt="image-20250918142158375" style="zoom: 50%;"  data-tag='post-image' onload='this.onload=null;this.style.opacity=1;' loading="lazy" onerror='this.onerror=null;this.src="/img/404.jpg"'>



<h3 id="TODO："><a href="#TODO：" class="headerlink" title="TODO："></a>TODO：</h3><ol>
<li><p>再测量16GPUs的case</p>
</li>
<li><p>跨进程参数同步，计划通过共享内存传递地址</p>
</li>
<li><p>对all-to-all和step的比例等参数建模</p>
</li>
<li><p>与disagg的one-step-off-policy比较； 实现stream minibatch pipline，与baseline比较</p>
</li>
<li><p>尝试结合disagg和colocate（并行用于disagg的case）</p>
</li>
</ol>
<h1 id="Tips"><a href="#Tips" class="headerlink" title="Tips"></a>Tips</h1><p>如何理解batch_size等各种参数？</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># normalize config</span></span><br><span class="line"><span class="comment"># [Note]这里为什么要用and呢</span></span><br><span class="line"><span class="keyword">if</span> <span class="variable language_">self</span>._is_actor <span class="keyword">and</span> <span class="variable language_">self</span>._is_rollout:</span><br><span class="line">    <span class="variable language_">self</span>.config.actor.ppo_mini_batch_size *= <span class="variable language_">self</span>.config.rollout.n</span><br><span class="line">    <span class="variable language_">self</span>.config.actor.ppo_mini_batch_size //= mpu.get_data_parallel_world_size()</span><br><span class="line">    <span class="keyword">if</span> <span class="variable language_">self</span>.config.actor.get(<span class="string">&quot;ppo_micro_batch_size&quot;</span>, <span class="literal">None</span>):</span><br><span class="line">        <span class="variable language_">self</span>.config.actor.ppo_micro_batch_size //= mpu.get_data_parallel_world_size()</span><br><span class="line">        <span class="variable language_">self</span>.config.rollout.log_prob_micro_batch_size //= mpu.get_data_parallel_world_size()</span><br><span class="line">        <span class="variable language_">self</span>.config.actor.ppo_micro_batch_size_per_gpu = <span class="variable language_">self</span>.config.actor.ppo_micro_batch_size</span><br><span class="line">        <span class="variable language_">self</span>.config.rollout.log_prob_micro_batch_size_per_gpu = <span class="variable language_">self</span>.config.rollout.log_prob_micro_batch_size</span><br><span class="line"></span><br><span class="line">    <span class="variable language_">self</span>._is_offload_param = <span class="variable language_">self</span>.config.actor.megatron.get(<span class="string">&quot;param_offload&quot;</span>, <span class="literal">False</span>)</span><br><span class="line">    <span class="variable language_">self</span>._is_offload_grad = <span class="variable language_">self</span>.config.actor.megatron.get(<span class="string">&quot;grad_offload&quot;</span>, <span class="literal">False</span>)</span><br><span class="line">    <span class="variable language_">self</span>._is_offload_optimizer = <span class="variable language_">self</span>.config.actor.megatron.get(<span class="string">&quot;optimizer_offload&quot;</span>, <span class="literal">False</span>)</span><br><span class="line"><span class="keyword">elif</span> <span class="variable language_">self</span>._is_actor:</span><br><span class="line">    <span class="variable language_">self</span>.config.actor.ppo_mini_batch_size *= <span class="variable language_">self</span>.config.rollout.n</span><br><span class="line">    <span class="variable language_">self</span>.config.actor.ppo_mini_batch_size //= mpu.get_data_parallel_world_size()</span><br><span class="line">    <span class="keyword">if</span> <span class="variable language_">self</span>.config.actor.get(<span class="string">&quot;ppo_micro_batch_size&quot;</span>, <span class="literal">None</span>):</span><br><span class="line">        <span class="variable language_">self</span>.config.actor.ppo_micro_batch_size //= mpu.get_data_parallel_world_size()</span><br><span class="line">        <span class="variable language_">self</span>.config.rollout.log_prob_micro_batch_size //= mpu.get_data_parallel_world_size()</span><br><span class="line">        <span class="variable language_">self</span>.config.actor.ppo_micro_batch_size_per_gpu = <span class="variable language_">self</span>.config.actor.ppo_micro_batch_size</span><br><span class="line">        <span class="variable language_">self</span>.config.rollout.log_prob_micro_batch_size_per_gpu = <span class="variable language_">self</span>.config.rollout.log_prob_micro_batch_size</span><br><span class="line"></span><br><span class="line">    <span class="variable language_">self</span>._is_offload_param = <span class="variable language_">self</span>.config.actor.megatron.get(<span class="string">&quot;param_offload&quot;</span>, <span class="literal">False</span>)</span><br><span class="line">    <span class="variable language_">self</span>._is_offload_grad = <span class="variable language_">self</span>.config.actor.megatron.get(<span class="string">&quot;grad_offload&quot;</span>, <span class="literal">False</span>)</span><br><span class="line">    <span class="variable language_">self</span>._is_offload_optimizer = <span class="variable language_">self</span>.config.actor.megatron.get(<span class="string">&quot;optimizer_offload&quot;</span>, <span class="literal">False</span>)                          </span><br><span class="line"><span class="keyword">elif</span> <span class="variable language_">self</span>._is_ref:</span><br><span class="line">    <span class="keyword">if</span> <span class="variable language_">self</span>.config.ref.get(<span class="string">&quot;log_prob_micro_batch_size&quot;</span>, <span class="literal">None</span>):</span><br><span class="line">        <span class="variable language_">self</span>.config.ref.log_prob_micro_batch_size //= mpu.get_data_parallel_world_size()</span><br><span class="line">        <span class="variable language_">self</span>.config.ref.log_prob_micro_batch_size_per_gpu = <span class="variable language_">self</span>.config.ref.log_prob_micro_batch_size</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        <span class="keyword">assert</span> <span class="variable language_">self</span>.config.ref.get(<span class="string">&quot;log_prob_micro_batch_size_per_gpu&quot;</span>, <span class="literal">None</span>) <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>, (</span><br><span class="line">            <span class="string">&quot;Please note that in the ref policy configuration, `log_prob_micro_batch_size_per_gpu` and &quot;</span></span><br><span class="line">            <span class="string">&quot;`log_prob_micro_batch_size` should not be None at the same time.&quot;</span></span><br><span class="line">        )</span><br><span class="line">    <span class="variable language_">self</span>._ref_is_offload_param = <span class="variable language_">self</span>.config.ref.megatron.get(<span class="string">&quot;param_offload&quot;</span>, <span class="literal">False</span>)</span><br></pre></td></tr></table></figure>

<p><img src="/2025/10/13/multi-process-ov/Users\25490\AppData\Roaming\Typora\typora-user-images\image-20251016095447032.png" alt="image-20251016095447032"  data-tag='post-image' onload='this.onload=null;this.style.opacity=1;' loading="lazy" onerror='this.onerror=null;this.src="/img/404.jpg"'></p>
<ul>
<li>train_batch_size：一次给多少个batch</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">make_minibatch_iterator</span>(<span class="params">self, data: DataProto</span>) -&gt; Iterable[DataProto]:</span><br><span class="line">    <span class="string">&quot;&quot;&quot;Make minibatch iterator for updating the actor</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Args:</span></span><br><span class="line"><span class="string">        data (DataProto): a DataProto containing keys</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">            ``input_ids``: tensor of shape [batch_size, sequence_length]. torch.int64, where</span></span><br><span class="line"><span class="string">            ``sequence_length = prompt_length + response_length``</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">            ``attention_mask``: tensor of shape [batch_size, sequence_length]. torch.int64</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">            ``position_ids``: tensor of shape [batch_size, sequence_length]. torch.int64</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">            ``responses``: tensor of shape [batch_size, response_length]. torch.int64. Note that</span></span><br><span class="line"><span class="string">            responses = input_ids[:, -response_length:]</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">            ``old_log_probs``: tensor of shape [batch_size, response_length]. torch.float32. The log probability</span></span><br><span class="line"><span class="string">            of responses.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">            ``advantages``: tensor of shape [batch_size, response_length]. torch.float32. The advantages of</span></span><br><span class="line"><span class="string">            responses.</span></span><br><span class="line"><span class="string">            See PPO paper for details. https://arxiv.org/abs/1707.06347</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    select_keys = [</span><br><span class="line">        <span class="string">&quot;responses&quot;</span>,</span><br><span class="line">        <span class="string">&quot;input_ids&quot;</span>,</span><br><span class="line">        <span class="string">&quot;attention_mask&quot;</span>,</span><br><span class="line">        <span class="string">&quot;response_mask&quot;</span>,</span><br><span class="line">        <span class="string">&quot;position_ids&quot;</span>,</span><br><span class="line">        <span class="string">&quot;old_log_probs&quot;</span>,</span><br><span class="line">        <span class="string">&quot;advantages&quot;</span>,</span><br><span class="line">    ]</span><br><span class="line">    <span class="keyword">if</span> <span class="variable language_">self</span>.config.use_kl_loss:</span><br><span class="line">        select_keys.append(<span class="string">&quot;ref_log_prob&quot;</span>)</span><br><span class="line">    <span class="variable language_">self</span>.has_multi_modal_inputs = <span class="string">&quot;multi_modal_inputs&quot;</span> <span class="keyword">in</span> data.non_tensor_batch.keys()</span><br><span class="line">    <span class="keyword">if</span> <span class="variable language_">self</span>.has_multi_modal_inputs:</span><br><span class="line">        data = data.select(select_keys, [<span class="string">&quot;multi_modal_inputs&quot;</span>])</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        data = data.select(batch_keys=select_keys)</span><br><span class="line">    <span class="comment"># breakpoint()</span></span><br><span class="line">    <span class="keyword">return</span> data.make_iterator(</span><br><span class="line">        mini_batch_size=<span class="variable language_">self</span>.config.ppo_mini_batch_size, <span class="comment"># have *n//word_size</span></span><br><span class="line">        epochs=<span class="variable language_">self</span>.config.ppo_epochs,</span><br><span class="line">        seed=<span class="variable language_">self</span>.config.data_loader_seed,</span><br><span class="line">        dataloader_kwargs=&#123;<span class="string">&quot;shuffle&quot;</span>: <span class="variable language_">self</span>.config.shuffle&#125;,</span><br><span class="line">    )</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<ul>
<li>ppo_mini_batch_size：一个batch做F-B显存压力太大了，所以切成mini_batch_size，每mini_batch_size更新梯度，因此实际上update_actor以mini_batch_size为单位（每张卡上实际处理数据batch大小为*n&#x2F;&#x2F;dp_world_size），在actor_update中标注的minibatch就是以这个为单位的</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># batch should be a list of batches inside micro-batches</span></span><br><span class="line">batch_generator = make_batch_generator(micro_batches, vpp_size=<span class="built_in">len</span>(<span class="variable language_">self</span>.actor_module))</span><br></pre></td></tr></table></figure>

<ul>
<li>ppo_micro_batch_size_per_gpu：显存压力还是大，为防止OOM，minibatch为基本单位更新参数，micro_batch为基本单位做F-B算loss，获取优化器参数</li>
</ul>
<p>我们需要注意，actor_update和compute_log_prob还有compute_ref_prob不一样，后两者没有minibatch的概念，但是有micro_batch_size:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">        <span class="comment"># we should always recompute old_log_probs when it is HybridEngine</span></span><br><span class="line">        <span class="comment"># [TODO] 这里用rollout的参数有些奇怪？引擎也是megatron的,参数真是乱飞,这是compute_log_prob的</span></span><br><span class="line">        data.meta_info[<span class="string">&quot;micro_batch_size&quot;</span>] = <span class="variable language_">self</span>.config.rollout.log_prob_micro_batch_size_per_gpu</span><br><span class="line">        data.meta_info[<span class="string">&quot;max_token_len&quot;</span>] = <span class="variable language_">self</span>.config.rollout.log_prob_max_token_len_per_gpu</span><br><span class="line">        data.meta_info[<span class="string">&quot;use_dynamic_bsz&quot;</span>] = <span class="variable language_">self</span>.config.rollout.log_prob_use_dynamic_bsz</span><br><span class="line">        data.meta_info[<span class="string">&quot;temperature&quot;</span>] = <span class="variable language_">self</span>.config.rollout.temperature</span><br><span class="line"><span class="comment"># ref model</span></span><br><span class="line">        micro_batch_size = <span class="variable language_">self</span>.config.ref.log_prob_micro_batch_size_per_gpu</span><br><span class="line">        data.meta_info[<span class="string">&quot;micro_batch_size&quot;</span>] = micro_batch_size</span><br><span class="line">        data.meta_info[<span class="string">&quot;max_token_len&quot;</span>] = <span class="variable language_">self</span>.config.ref.log_prob_max_token_len_per_gpu</span><br><span class="line">        data.meta_info[<span class="string">&quot;use_dynamic_bsz&quot;</span>] = <span class="variable language_">self</span>.config.ref.log_prob_use_dynamic_bsz</span><br><span class="line">        data.meta_info[<span class="string">&quot;temperature&quot;</span>] = <span class="variable language_">self</span>.config.rollout.temperature</span><br></pre></td></tr></table></figure>


<ul>
<li>global_steps：总共需要训练的次数，为total_epochs*len(dataset)&#x2F;&#x2F;data.train_batch_size，是否向上取整看对数据集的处理方式</li>
</ul>

</article>
    
    

</div>
<div class="trm-post-next-prev row">
    <div class="col-lg-12">
        <!-- title -->
        <h5 class="trm-title-with-divider">
            其他文章
            <span data-number="02"></span>
        </h5>
    </div>
    
        <div class="col-lg-6">
    <div class="trm-blog-card trm-scroll-animation">
        <a href="/2025/10/16/parellel_report1/" class="trm-cover-frame trm-anima-link">
            
            
                <img alt="cover" class="no-fancybox" src="/img/block.jpg">
            
        </a>
        
        <div class="trm-card-descr">
            <div class="trm-label trm-category trm-mb-20">
                <a href=" #.">
                    未分类
                </a>
            </div>
            <h5>
                <a href="/2025/10/16/parellel_report1/" class="trm-anima-link">
                    
                </a>
            </h5>
            <div class="trm-divider trm-mb-20 trm-mt-20"></div>
            <ul class="trm-card-data trm-label">
                <li>25/10/16</li>
                <li>16:48</li>
                
                
            </ul>
        </div>
    </div>
</div>
    
    
        <div class="col-lg-6">
    <div class="trm-blog-card trm-scroll-animation">
        <a href="/2025/10/01/why-overlap-slower/" class="trm-cover-frame trm-anima-link">
            
            
                <img alt="cover" class="no-fancybox" src="https://www.logosc.cn/uploads/resources/2018/11/29/1543459457_thumb.jpg">
            
        </a>
        
        <div class="trm-card-descr">
            <div class="trm-label trm-category trm-mb-20">
                <a href=" /categories/RLHF/">
                    RLHF
                </a>
            </div>
            <h5>
                <a href="/2025/10/01/why-overlap-slower/" class="trm-anima-link">
                    why_overlap_slower
                </a>
            </h5>
            <div class="trm-divider trm-mb-20 trm-mt-20"></div>
            <ul class="trm-card-data trm-label">
                <li>25/10/01</li>
                <li>10:16</li>
                
                
            </ul>
        </div>
    </div>
</div>
    
</div>

    



                    <div class="trm-divider footer-divider"></div>

                    <!-- footer -->
                    <footer class="trm-footer-card trm-scroll-animation">

    

    

    
        <div class="trm-footer-item">
            <span>
                由 <a href="https://hexo.io" target="_blank" rel="noopener">Hexo</a> 驱动 v7.3.0
            </span>
            <span class="footer-separator" data-separator=" | "></span>
            <span> 
                主题 - 
                <a rel="noopener" href='https://github.com/MaLuns/hexo-theme-async' target='_blank'>Async</a>
                v2.2.6
            </span>
        </div>
      

     

     
</footer>
                    <!-- footer end -->

                </div>
            </div>
        </div>
    </div>
</div>
            <!-- body end -->

            

            
<div class="trm-fixed-container">
    
    
        <div class="trm-fixed-btn" data-title="阅读模式" onclick="asyncFun.switchReadMode()">
            <i class="iconfont fas fa-book-reader"></i>
        </div>
    
    
    <div id="trm-back-top" class="trm-fixed-btn" data-title="回到顶部">
        <i class="iconfont fas fa-arrow-up"></i>
    </div>
</div>
        </div>
      </div>
      <!-- scroll container end -->
  </div>
  <!-- app wrapper end -->

  
  <!-- Plugin -->




    
    
<script src="https://unpkg.com/@fancyapps/ui@4.0/dist/fancybox.umd.js"></script>

    

    

    

    <!-- 数学公式 -->
    

    <!-- 评论插件 -->
    
        

        
    

		




    <!-- Service Worker -->
    
    <!-- baidu push -->
    


<script id="async-script" src="/js/main.js?v=2.2.6"></script>

<!-- CDN -->


    

    

    



</body>

</html>