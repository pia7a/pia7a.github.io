<!DOCTYPE html>
<html lang="zh-Hans">

<head>
  <meta http-equiv="content-type" content="text/html; charset=utf-8">
<meta name="x5-fullscreen" content="true">
<meta name="full-screen" content="yes">
<meta name="theme-color" content="#317EFB" />
<meta content="width=device-width, initial-scale=1.0, maximum-scale=5.0, user-scalable=0" name="viewport">
<meta name="description" content="目的达到预期加速目的 实验1[MCP](1. Introduction — Multi-Process Service): 启用后允许多进程共享同一GPU, 不启用的话需要在不同进程的cuda上下文之间切换,启用后共享同一上下文. server-client模式, 所有进程都将调用请求发给MCP-server, 由其统一管理调度.  MPS对单进程GPU低利用率情况有[加速]LithOS: An">
<meta property="og:type" content="article">
<meta property="og:title" content="multi-process-ov2">
<meta property="og:url" content="http://example.com/2025/10/29/multi-process-ov2/index.html">
<meta property="og:site_name" content="Hexo">
<meta property="og:description" content="目的达到预期加速目的 实验1[MCP](1. Introduction — Multi-Process Service): 启用后允许多进程共享同一GPU, 不启用的话需要在不同进程的cuda上下文之间切换,启用后共享同一上下文. server-client模式, 所有进程都将调用请求发给MCP-server, 由其统一管理调度.  MPS对单进程GPU低利用率情况有[加速]LithOS: An">
<meta property="og:locale">
<meta property="og:image" content="http://example.com/img/404.jpg">
<meta property="og:image" content="http://example.com/img/404.jpg">
<meta property="og:image" content="http://example.com/img/404.jpg">
<meta property="og:image" content="http://example.com/img/404.jpg">
<meta property="og:image" content="http://example.com/img/404.jpg">
<meta property="og:image" content="http://example.com/img/404.jpg">
<meta property="og:image" content="http://example.com/img/404.jpg">
<meta property="og:image" content="http://example.com/img/404.jpg">
<meta property="og:image" content="http://example.com/img/404.jpg">
<meta property="og:image" content="http://example.com/img/404.jpg">
<meta property="og:image" content="http://example.com/img/404.jpg">
<meta property="article:published_time" content="2025-10-29T13:06:05.000Z">
<meta property="article:modified_time" content="2025-11-20T02:16:56.035Z">
<meta property="article:author" content="John Doe">
<meta property="article:tag" content="Verl框架">
<meta property="article:tag" content="性能分析">
<meta property="article:tag" content="多进程">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="http://example.com/img/404.jpg">

    <meta name="keywords" content="RLHF,Verl框架,性能分析">


<title >multi-process-ov2</title>

<!-- Favicon -->

    <link href='/img/favicon.svg?v=2.2.6' rel='icon' type='image/png' sizes='16x16' ></link>


    <link href='/img/favicon.svg?v=2.2.6' rel='icon' type='image/png' sizes='32x32' ></link>




<!-- Plugin -->




    
<link rel="stylesheet" href="/css/plugins/bootstrap.row.css">

    
<link rel="stylesheet" href="https://unpkg.com/@fancyapps/ui@4.0/dist/fancybox.css">

    
    




<!-- Icon -->

    
<link rel="stylesheet" href="/css/plugins/font-awesome.min.css">




<!-- Variable -->
<script>window.ASYNC_CONFIG = {"hostname":"example.com","author":"John Doe","root":"/","typed_text":null,"theme_version":"2.2.6","theme":{"switch":true,"default":"style-light"},"favicon":{"logo":"/img/favicon.svg","icon16":"/img/favicon.svg","icon32":"/img/favicon.svg","apple_touch_icon":null,"webmanifest":null,"visibilitychange":false,"hidden":"/failure.ico","show_text":"(/≧▽≦/)咦！又好了！","hide_text":"(●—●)喔哟，崩溃啦！"},"i18n":{"placeholder":"搜索文章...","empty":"找不到您查询的内容: ${query}","hits":"找到 ${hits} 条结果","hits_time":"找到 ${hits} 条结果（用时 ${time} 毫秒）","author":"本文作者：","copyright_link":"本文链接：","copyright_license_title":"版权声明：","copyright_license_content":"本博客所有文章除特别声明外，均默认采用 undefined 许可协议。","copy_success":"复制成功","copy_failure":"复制失败","open_read_mode":"进入阅读模式","exit_read_mode":"退出阅读模式","notice_outdate_message":"距离上次更新已经 undefined 天了, 文章内容可能已经过时。","sticky":"置顶","just":"刚刚","min":"分钟前","hour":"小时前","day":"天前","month":"个月前"},"swup":false,"plugin":{"flickr_justified_gallery":"https://unpkg.com/flickr-justified-gallery@latest/dist/fjGallery.min.js"},"icons":{"sun":"far fa-sun","moon":"far fa-moon","play":"fas fa-play","email":"far fa-envelope","next":"fas fa-arrow-right","calendar":"far fa-calendar-alt","clock":"far fa-clock","user":"far fa-user","back_top":"fas fa-arrow-up","close":"fas fa-times","search":"fas fa-search","reward":"fas fa-hand-holding-usd","toc_tag":"fas fa-th-list","read":"fas fa-book-reader","arrows":"fas fa-arrows-alt-h","double_arrows":"fas fa-angle-double-down","copy":"fas fa-copy"},"icontype":"font","highlight":{"plugin":"highlighjs","theme":true,"copy":true,"lang":true,"title":"default","height_limit":false},"toc":{"post_title":true},"live_time":{"start_time":"","prefix":"博客已萌萌哒运行 undefined 天"},"danmu":{"enable":false,"el":".trm-banner"}};</script>
<script id="async-page-config">window.PAGE_CONFIG = {"isPost":true,"isHome":false,"postUpdate":"2025-11-20 10:16:56"};</script>

<!-- Theme mode css -->
<link data-swup-theme rel="stylesheet" href="/css/index.css?v=2.2.6" id="trm-switch-style">
<script>
    let defaultMode = ASYNC_CONFIG.theme.default !=='auto' ?  ASYNC_CONFIG.theme.default : (window.matchMedia("(prefers-color-scheme: light)").matches ? 'style-light' : 'style-dark')
    let catchMode = localStorage.getItem('theme-mode') || defaultMode;
    let type = catchMode === 'style-dark' ? 'add' : 'remove';
    document.documentElement.classList[type]('dark')
</script>

<!-- CDN -->


    
    



<!-- Site Analytics -->

 
<meta name="generator" content="Hexo 7.3.0"></head>

<body>

  <!-- app wrapper -->
  <div class="trm-app-frame">

    <!-- page preloader -->
    <div class="trm-preloader">
    <div class="trm-holder">
        <div class="preloader">
            <div></div>
            <div></div>
            <div></div>
            <div></div>
            <div></div>
            <div></div>
            <div></div>
            <div></div>
            <div></div>
            <div></div>
        </div>
    </div>
</div>
    <!-- page preloader end -->

    <!-- change mode preloader -->
    <div class="trm-mode-swich-animation-frame">
    <div class="trm-mode-swich-animation">
        <i class="i-sun"><i class="iconfont far fa-sun"></i></i>
        <div class="trm-horizon"></div>
        <i class="i-moon"><i class="iconfont far fa-moon"></i></i>
    </div>
</div>
    <!-- change mode preloader end -->

      <!-- scroll container -->
      <div id="trm-dynamic-content" class="trm-swup-animation">
        <div id="trm-scroll-container" class="trm-scroll-container" style="opacity: 0">
            <!-- top bar -->
            <header class="trm-top-bar">
	<div class="container">
		<div class="trm-left-side">
			<!-- logo -->
<a href="/" class="trm-logo-frame trm-anima-link">
    
        <img alt="logo" src="/img/favicon.svg">
    
    
        <div class="trm-logo-text">
            Async<span>zhiqiang</span>
        </div>
    
</a>
<!-- logo end -->
		</div>
		<div class="trm-right-side">
			<!-- menu -->
<div class="trm-menu">
    <nav>
        <ul>
            
            <li class="menu-item-has-children ">
                <a  href="/" target="">
                    首页
                </a>
                
            </li>
            
            <li class="menu-item-has-children ">
                <a  href="/archives/" target="">
                    时间线
                </a>
                
            </li>
            
            <li class="menu-item-has-children ">
                <a  href="/tags/" target="">
                    标签
                </a>
                
            </li>
            
            <li class="menu-item-has-children ">
                <a  href="/categories" target="">
                    分类
                </a>
                
            </li>
            
        </ul>
    </nav>
</div>
<!-- menu end -->
			
    <!-- mode switcher place -->
    <div class="trm-mode-switcher-place">
        <div class="trm-mode-switcher">
            <i class="iconfont far fa-sun"></i>
            <input class="tgl tgl-light" id="trm-swich" type="checkbox">
            <label class="trm-swich" for="trm-swich"></label>
            <i class="iconfont far fa-moon"></i>
        </div>
    </div>
    <!-- mode switcher place end -->

			
		</div>
		<div class="trm-menu-btn">
			<span></span>
		</div>
	</div>
</header>
            <!-- top bar end -->

            <!-- body -->
            
<div class="trm-content-start">
    <!-- banner -->
    <div class="trm-banner">
    
    <!-- banner cover -->
    <img style="object-position:top;object-fit:cover;" alt="banner" class="trm-banner-cover" src="https://pic1.zhimg.com/v2-b3c2c6745b9421a13a3c4706b19223b3_r.jpg">
    <!-- banner cover end -->
    

    <!-- banner content -->
    <div class="trm-banner-content trm-overlay">
        <div class="container">
            <div class="row">
                
                <div class="col-lg-4"></div>
                
                <div class="col-lg-8">

                    <!-- banner title -->
                    <div class="trm-banner-text ">
                        <div class="trm-label trm-mb-20">
                            Hi my new friend!
                        </div>
                        <h1 class="trm-mb-30 trm-hsmb-font">
                            multi-process-ov2
                        </h1>

                        
                            <ul class="trm-breadcrumbs trm-label">
                                <li>
                                    <a href="/" class="trm-anima-link">Home</a>
                                </li>
                                <li>
                                    <span>
                                        2025
                                    </span>
                                </li>
                            </ul>
                        
                    </div>
                    <!-- banner title end -->

                    <!-- scroll hint -->
                    <span id="scroll-triger" class="trm-scroll-hint-frame">
                        <div class="trm-scroll-hint"></div>
                        <span class="trm-label">Scroll down</span>
                    </span>
                    <!-- scroll hint end -->

                </div>
            </div>
        </div>
    </div>
    <!-- banner content end -->
</div>
    <!-- banner end -->
    <div class="container">
        <div class="row">
            
                <div class="trm-page-sidebar col-lg-4 hidden-sm">
                    <!-- main card -->
                    <div class="trm-main-card-frame trm-sidebar">
    <div class="trm-main-card"> 
        <!-- card header -->
<div class="trm-mc-header">
    <div class="trm-avatar-frame trm-mb-20">
        <img alt="Avatar" class="trm-avatar" src="/img/avatar.jpg">
    </div>
    <h5 class="trm-name trm-mb-15">
        看雪
    </h5>
    
</div>
<!-- card header end -->
        <!-- sidebar social -->

<div class="trm-divider trm-mb-40 trm-mt-40"></div>
<div class="trm-social">
    
        <a href="https://github.com" title="Github" rel="nofollow" target="_blank">
            <i class="iconfont fab fa-github"></i>
        </a>
    
</div>

<!-- sidebar social end -->
        <!-- info -->
<div class="trm-divider trm-mb-40 trm-mt-40"></div>
<ul class="trm-table trm-mb-20">
    
        <li>
            <div class="trm-label">
                Residence:
            </div>
            <div class="trm-label trm-label-light">
                Mars
            </div>
        </li>
    
</ul>
<!-- info end -->

        
    </div>
</div>
                    <!-- main card end -->
                </div>
            
            <div class="trm-page-content col-lg-8">
                <div id="trm-content" class="trm-content">
                    <div class="trm-post-info row hidden-sm">
    <div class="col-sm-4">
        <div class="trm-card trm-label trm-label-light text-center">
            <i class="iconfont far fa-calendar-alt trm-icon"></i><br>
            10/29
        </div>
    </div>
    <div class="col-sm-4">
        <div class="trm-card trm-label trm-label-light text-center">
            <i class="iconfont far fa-clock trm-icon"></i><br>
            21:06
        </div>
    </div>
    <div class="col-sm-4">
        <div id="post-author" class="trm-card trm-label trm-label-light text-center">
            <i class="iconfont far fa-user trm-icon"></i><br>
            John Doe
        </div>
    </div>
</div>
<div class="trm-card ">
    <article id="article-container" class="trm-publication">
    <h1 id="目的"><a href="#目的" class="headerlink" title="目的"></a>目的</h1><p>达到预期加速目的</p>
<h1 id="实验1"><a href="#实验1" class="headerlink" title="实验1"></a>实验1</h1><p>[MCP](<a target="_blank" rel="noopener" href="https://docs.nvidia.com/deploy/mps/index.html#the-benefits-of-mps">1. Introduction — Multi-Process Service</a>): 启用后允许多进程共享同一GPU, 不启用的话需要在不同进程的cuda上下文之间切换,启用后共享同一上下文. server-client模式, 所有进程都将调用请求发给MCP-server, 由其统一管理调度.</p>
<p><img src="https://docs.nvidia.com/deploy/mps/_images/image1.png" alt="_images&#x2F;image1.png"  data-tag='post-image' onload='this.onload=null;this.style.opacity=1;' loading="lazy" onerror='this.onerror=null;this.src="/img/404.jpg"'></p>
<p>MPS对单进程GPU低利用率情况有[加速]<a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2504.15465">LithOS: An Operating Systemfor Efficient Machine Learning on GPUs</a>:</p>
<p><img src="/2025/10/29/multi-process-ov2/Users\25490\AppData\Roaming\Typora\typora-user-images\image-20251030214338057.png" alt="image-20251030214338057"  data-tag='post-image' onload='this.onload=null;this.style.opacity=1;' loading="lazy" onerror='this.onerror=null;this.src="/img/404.jpg"'></p>
<p>实验参数:</p>
<ul>
<li><strong>Qwen3-30B-A3B-Instruct-2507</strong>，隐藏层为2层</li>
</ul>
<h2 id="8-GPUs"><a href="#8-GPUs" class="headerlink" title="8 GPUs"></a>8 GPUs</h2><ul>
<li><p>actor_update: Megatron(EP&#x3D;8, TP&#x3D;1)</p>
</li>
<li><p>rollout: VLLM(TP&#x3D;4)</p>
</li>
<li><p>train_batch_size&#x3D;xx</p>
</li>
<li><p>max_prompt_length&#x3D;256</p>
</li>
<li><p>max_response_length&#x3D;1024</p>
</li>
<li><p><strong>mini_batch_size&#x3D;256</strong></p>
</li>
<li><p>micro_batch_size_per_gpu&#x3D;2</p>
</li>
<li><p><strong>rollout_n&#x3D;4</strong></p>
</li>
<li><p>param_offload&#x3D;False</p>
</li>
<li><p>grad_offload&#x3D;True</p>
</li>
<li><p>optimizer_offload&#x3D;True</p>
</li>
</ul>
<table>
<thead>
<tr>
<th>batch</th>
<th>baseline</th>
<th>$\frac{1}{4}$ minibath+pipline</th>
<th>$\frac{1}{4}$ minibath+pipline+mps</th>
<th>通信</th>
</tr>
</thead>
<tbody><tr>
<td>512</td>
<td>72.27s</td>
<td>76.50s</td>
<td>64.94s (1.11$\times$)</td>
<td></td>
</tr>
<tr>
<td>1024</td>
<td>140.28s</td>
<td></td>
<td>116.42s(1.20$\times$)</td>
<td></td>
</tr>
</tbody></table>
<table>
<thead>
<tr>
<th>batch</th>
<th>baseline</th>
<th>$\frac{1}{8}$ minibath+pipline</th>
<th>$\frac{1}{8}$ minibath+pipline+mps</th>
<th>通信(baselines)</th>
</tr>
</thead>
<tbody><tr>
<td>1024</td>
<td>141.05s</td>
<td></td>
<td>124.03s(1.14$\times$)</td>
<td>37.96s（all2all:21.35s all_gather:13.87s all_reduce:2.73s）</td>
</tr>
<tr>
<td>2048</td>
<td>286.07s</td>
<td></td>
<td>220.26s(1.31$\times$)</td>
<td>76.43s（all2all:42.87s all_gather:26.23s all_reduce:7.33s）</td>
</tr>
</tbody></table>
<h4 id="batch-1024"><a href="#batch-1024" class="headerlink" title="batch&#x3D;1024"></a>batch&#x3D;1024</h4><p>baseline：</p>
<table>
<thead>
<tr>
<th>阶段</th>
<th>耗时 (s)</th>
</tr>
</thead>
<tbody><tr>
<td>rollout</td>
<td>64.25</td>
</tr>
<tr>
<td>training</td>
<td>67.06</td>
</tr>
<tr>
<td>others</td>
<td>9.93</td>
</tr>
</tbody></table>
<p>$\frac{1}{8}$ minibath+pipline+mps:</p>
<table>
<thead>
<tr>
<th>阶段</th>
<th>耗时 (s)</th>
</tr>
</thead>
<tbody><tr>
<td>rollout</td>
<td>71.94</td>
</tr>
<tr>
<td>training</td>
<td>65.304</td>
</tr>
<tr>
<td>others</td>
<td>24.89</td>
</tr>
</tbody></table>
<h4 id="batch-2048"><a href="#batch-2048" class="headerlink" title="batch&#x3D;2048"></a>batch&#x3D;2048</h4><p>baseline：</p>
<table>
<thead>
<tr>
<th>阶段</th>
<th>耗时 (s)</th>
</tr>
</thead>
<tbody><tr>
<td>rollout</td>
<td>141.14</td>
</tr>
<tr>
<td>training</td>
<td>131.04</td>
</tr>
<tr>
<td>others</td>
<td>14.52</td>
</tr>
</tbody></table>
<p> $\frac{1}{8}$ minibath+pipline+mps：</p>
<table>
<thead>
<tr>
<th>阶段</th>
<th>耗时 (s)</th>
</tr>
</thead>
<tbody><tr>
<td>rollout</td>
<td>137.9s</td>
</tr>
<tr>
<td>training</td>
<td>127.91</td>
</tr>
<tr>
<td>others</td>
<td>26.62</td>
</tr>
</tbody></table>
<p><img src="/2025/10/29/multi-process-ov2/Users\25490\AppData\Roaming\Typora\typora-user-images\image-20251102143556425.png" alt="image-20251102143556425"  data-tag='post-image' onload='this.onload=null;this.style.opacity=1;' loading="lazy" onerror='this.onerror=null;this.src="/img/404.jpg"'></p>
<p>当推理和训练的时间接近时，可以获得较大收益</p>
<ul>
<li><p>为什么batch增大后加速比提升?</p>
<p>minibatch切分后引入的load&#x2F;offload开销固定,而mps对训练和推理之间做计算-通信overlap省下的时间比例不变,所以batch增大后加速更接近overlap的收益.</p>
</li>
<li><p>8卡的all-to-all占比约10%,为什么收益超过10%?</p>
<p>all-to-all之外的其他通信等也重叠<img src="/2025/10/29/multi-process-ov2/Users\25490\AppData\Roaming\Typora\typora-user-images\image-20251030212944572.png" alt="image-20251030212944572"  data-tag='post-image' onload='this.onload=null;this.style.opacity=1;' loading="lazy" onerror='this.onerror=null;this.src="/img/404.jpg"'></p>
</li>
<li><p>actor_update: Megatron(EP&#x3D;8, TP&#x3D;1)</p>
</li>
<li><p>rollout: VLLM(TP&#x3D;1)</p>
</li>
<li><p>train_batch_size&#x3D;xx</p>
</li>
<li><p>max_prompt_length&#x3D;256</p>
</li>
<li><p>max_response_length&#x3D;1024</p>
</li>
<li><p><strong>mini_batch_size&#x3D;256</strong></p>
</li>
<li><p>micro_batch_size_per_gpu&#x3D;2</p>
</li>
<li><p><strong>rollout_n&#x3D;4</strong></p>
</li>
<li><p>param_offload&#x3D;False</p>
</li>
<li><p>grad_offload&#x3D;True</p>
</li>
<li><p>optimizer_offload&#x3D;True</p>
</li>
</ul>
<table>
<thead>
<tr>
<th>batch</th>
<th>baseline</th>
<th>$\frac{1}{8}$ minibath+pipline+mps</th>
<th>通信</th>
</tr>
</thead>
<tbody><tr>
<td>1024</td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>2048</td>
<td>187.14s</td>
<td>194.37s(0.96$\times$)</td>
<td></td>
</tr>
</tbody></table>
<p>没有加速的原因：</p>
<p>以batch&#x3D;2956为例，baseline为：</p>
<table>
<thead>
<tr>
<th>阶段</th>
<th>耗时 (s)</th>
</tr>
</thead>
<tbody><tr>
<td>rollout</td>
<td>38.34</td>
</tr>
<tr>
<td>training</td>
<td>130.00</td>
</tr>
<tr>
<td>others</td>
<td>19.14</td>
</tr>
</tbody></table>
<p>本方法为：</p>
<table>
<thead>
<tr>
<th>阶段</th>
<th>耗时 (s)</th>
</tr>
</thead>
<tbody><tr>
<td>rollout</td>
<td>67.54</td>
</tr>
<tr>
<td>training</td>
<td>128.68</td>
</tr>
<tr>
<td>others</td>
<td>27.167</td>
</tr>
</tbody></table>
<ol>
<li><p>Rollout改成TP&#x3D;1后，时间从38.34s增加到67.54s，推理没打满（batch减少与时间非线性）</p>
</li>
<li><p>offload等其他时间增加</p>
</li>
<li><p>推理时间相比于TP&#x3D;4更小</p>
</li>
</ol>
<p><img src="/2025/10/29/multi-process-ov2/Users\25490\AppData\Roaming\Typora\typora-user-images\image-20251105131132724.png" alt="image-20251105131132724"  data-tag='post-image' onload='this.onload=null;this.style.opacity=1;' loading="lazy" onerror='this.onerror=null;this.src="/img/404.jpg"'></p>
<p><img src="/2025/10/29/multi-process-ov2/Users\25490\AppData\Roaming\Typora\typora-user-images\image-20251105132350609.png" alt="image-20251105132350609"  data-tag='post-image' onload='this.onload=null;this.style.opacity=1;' loading="lazy" onerror='this.onerror=null;this.src="/img/404.jpg"'></p>
<img src="/2025/10/29/multi-process-ov2/Users\25490\AppData\Roaming\Typora\typora-user-images\image-20251105132302034.png" alt="image-20251105132302034" style="zoom: 80%;"  data-tag='post-image' onload='this.onload=null;this.style.opacity=1;' loading="lazy" onerror='this.onerror=null;this.src="/img/404.jpg"'>



<h1 id="思路"><a href="#思路" class="headerlink" title="思路"></a>思路</h1><p>STEP1: offline profiling</p>
<p>分别独立测量一个minibatch的训练或推理的gpu api</p>
<p><img src="/2025/10/29/multi-process-ov2/Users\25490\AppData\Roaming\Typora\typora-user-images\image-20251105095302912.png" alt="image-20251105095302912"  data-tag='post-image' onload='this.onload=null;this.style.opacity=1;' loading="lazy" onerror='this.onerror=null;this.src="/img/404.jpg"'></p>
<p><img src="/2025/10/29/multi-process-ov2/Users\25490\AppData\Roaming\Typora\typora-user-images\image-20251105095804739.png" alt="image-20251105095804739"  data-tag='post-image' onload='this.onload=null;this.style.opacity=1;' loading="lazy" onerror='this.onerror=null;this.src="/img/404.jpg"'></p>
<img src="/2025/10/29/multi-process-ov2/Users\25490\AppData\Roaming\Typora\typora-user-images\image-20251105095854984.png" alt="image-20251105095854984" style="zoom:80%;"  data-tag='post-image' onload='this.onload=null;this.style.opacity=1;' loading="lazy" onerror='this.onerror=null;this.src="/img/404.jpg"'>

<h1 id="paper"><a href="#paper" class="headerlink" title="paper"></a>paper</h1><ol>
<li><p>[Orion: Interference-aware, Fine-grained GPU Sharing for ML Applications](<a target="_blank" rel="noopener" href="https://dl.acm.org/doi/pdf/10.1145/3627703.3629578">Orion: Interference-aware, Fine-grained GPU Sharing for ML Applications</a>)</p>
</li>
<li><p><a target="_blank" rel="noopener" href="https://jamesthez.github.io/files/bless-eurosys25.pdf">Improving GPU Sharing Performance through  Adaptive Bubbleless Spatial-Temporal Sharing</a></p>
</li>
</ol>
<h2 id="还能做什么"><a href="#还能做什么" class="headerlink" title="还能做什么?"></a>还能做什么?</h2><ul>
<li><p>在mps基础上看一看有哪些优化方案</p>
</li>
<li><p>还有哪些类似mps的工具\文章,他们又做了哪些优化,可以结合RL系统看一看</p>
</li>
</ul>
<h1 id="一、资源占用估计"><a href="#一、资源占用估计" class="headerlink" title="一、资源占用估计"></a>一、资源占用估计</h1><h2 id="1-公式（含-warp-对齐修正）"><a href="#1-公式（含-warp-对齐修正）" class="headerlink" title="1. 公式（含 warp 对齐修正）"></a>1. 公式（含 warp 对齐修正）</h2><p>令：</p>
<ul>
<li>$\displaystyle W_{\rm size} &#x3D; 32$  （假设 warp 大小为 32 线程）</li>
<li>$\displaystyle T_{\rm block}$ &#x3D; 每 block 的线程数</li>
<li>$\displaystyle W_{\rm block} &#x3D; \left\lceil \frac{T_{\rm block}}{W_{\rm size}} \right\rceil$ &#x3D; 每 block 所含 warp 数，经 warp 对齐向上取整</li>
<li>硬件上每 SM 参数：<ul>
<li>$\displaystyle W_{\rm max}$ &#x3D; 每 SM 可驻留的最大 warp 数</li>
<li>$\displaystyle T_{\rm max} &#x3D; W_{\rm max} \times W_{\rm size}$ &#x3D; 每 SM 可驻留的最大线程数</li>
<li>$\displaystyle R_{\rm SM}$ &#x3D; 每 SM 可用的寄存器总数</li>
<li>$\displaystyle S_{\rm SM}$ &#x3D; 每 SM 可用的共享内存总量</li>
<li>$\displaystyle B_{\rm blocks_max}$ &#x3D; 每 SM 支持的最大 block 数</li>
</ul>
</li>
<li>kernel 使用情况：<ul>
<li>$\displaystyle R_{\rm thread}$ &#x3D; 每线程使用的寄存器数</li>
<li>$\displaystyle S_{\rm block}$ &#x3D; 每 block 使用的共享内存／动态共享内存总量（字节）</li>
</ul>
</li>
</ul>
<p>首先计算各资源对驻留 block 数的限制：<br>$$<br>B_{\rm thread} &#x3D; \left\lfloor \frac{T_{\rm max}}{T_{\rm block}} \right\rfloor<br>$$<br>然后可驻留的 block 数：<br>$$<br>B_{\rm reside} &#x3D; \min{B_{\rm thread},, B_{\rm reg},, B_{\rm smem},, B_{\rm blocks_max}}<br>$$<br>接着驻留的 warp 数：<br>$$<br>W_{\rm reside} &#x3D; B_{\rm reside} \times W_{\rm block}<br>$$<br>最后估算 occupancy：<br>$$<br>\text{Occupancy} &#x3D; \frac{W_{\rm reside}}{W_{\rm max}}<br>$$<br>因为我们向上取整了 warp 数（$W_{\rm block}$），这种做法较为保守（即可能低估实际 occupancy ）。</p>
<p>即：<br>$$<br>\text{Occupancy}<br>&#x3D; \frac{ \min\big(\lfloor \tfrac{T_{\rm max}}{T_{\rm block}} \rfloor,;\lfloor \tfrac{R_{\rm SM}}{R_{\rm thread} \cdot T_{\rm block}} \rfloor,;\lfloor \tfrac{S_{\rm SM}}{S_{\rm block}} \rfloor,; B_{\rm blocks_max} \big);\cdot ;\lceil \tfrac{T_{\rm block}}{W_{\rm size}}\rceil }{W_{\rm max}}<br>$$<br>（其中若 $S_{\rm block}&#x3D;0$ 则忽略第三项）。</p>
<hr>
<h2 id="参考文献"><a href="#参考文献" class="headerlink" title="参考文献"></a>参考文献</h2><ul>
<li>CUDA Occupancy Calculator — 官方文档指出 “the multiprocessor occupancy is the ratio of active warps to the maximum number of warps supported on a multiprocessor of the GPU.” <a target="_blank" rel="noopener" href="https://docs.nvidia.com/cuda/archive/12.2.1/cuda-occupancy-calculator/index.html?utm_source=chatgpt.com">NVIDIA Docs+2NVIDIA Docs+2</a></li>
<li>Lei Mao. “CUDA Occupancy Calculation” 博客，解释寄存器、线程、warp、对齐计算关系。 <a target="_blank" rel="noopener" href="https://leimao.github.io/blog/CUDA-Occupancy-Calculation/?utm_source=chatgpt.com">Lei Mao’s Log Book</a></li>
<li>NVIDIA CUDA Runtime API: “Occupancy” 章节，说明 cudaOccupancyMaxActiveBlocksPerMultiprocessor 等。 <a target="_blank" rel="noopener" href="https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__OCCUPANCY.html?utm_source=chatgpt.com">NVIDIA Docs</a></li>
</ul>
<h2 id="说明"><a href="#说明" class="headerlink" title="说明"></a>说明</h2><p>本模型假设 kernel 独占 GPU，且无其他进程或 kernel 干扰。它综合考虑了三个主要资源瓶颈：线程／warp 驻留数、寄存器数、共享内存数。若 T_block 不是 warp 大小（32）整倍数，则 warp 数向上取整，从而更为保守估算 occupancy。<br>该估算可作为 kernel 在理论最优资源情境下的 occupancy 上界，但实际 occupancy 可能受 其他瓶颈（如内存带宽、竞态、调度延迟）影响而更低。<br>参考 NVIDIA 官方 “multiprocessor occupancy is the ratio of active warps to the maximum number of warps supported on a multiprocessor” 定义。 :contentReference[oaicite:0]{index&#x3D;0}  </p>
<h2 id><a href="#" class="headerlink" title></a></h2><p>可以直接从Nsight获取：</p>
<p><img src="/2025/10/29/multi-process-ov2/Users\25490\AppData\Roaming\Typora\typora-user-images\image-20251107154734824.png" alt="image-20251107154734824"  data-tag='post-image' onload='this.onload=null;this.style.opacity=1;' loading="lazy" onerror='this.onerror=null;this.src="/img/404.jpg"'></p>
<p>这样，我们可以获取任意时间段$(t_i,t_{i+1})$的所占据的活跃wraps，带宽等资源。</p>
<h1 id="二、重叠时间估计"><a href="#二、重叠时间估计" class="headerlink" title="二、重叠时间估计"></a>二、重叠时间估计</h1><p>对于推理进程，某一段区间$(t^{I}<em>{i},t^{I}</em>{i+1})$占有资源$R^{I}<em>{i}&#x3D;(S^{I}</em>{i},B^{I}<em>{i})$,其中，$S^{I}</em>{i} \in [0,1]$表示SM平均占用率，$B^{I}<em>{i} \in [0,1]$表示带宽的平均占用率, $\Delta t^{I}&#x3D;t^{I}</em>{i+1}-t^{I}_{i}$.</p>
<p>类似，训练进程，某一段区间$(t^{R}<em>{i},t^{R}</em>{i+1})$占有资源$R^{T}<em>{i}&#x3D;(S^{T}</em>{i},B^{T}<em>{i})$,$\Delta t^{T}&#x3D;t^{T}</em>{i+1}-t^{T}_{i}$</p>
<p>考虑部分重叠，$t^{I}<em>{i}&lt;t^{R}</em>{i}&lt;t^{I}<em>{i+1}&lt;t^{R}</em>{i+1}$, $\Delta t&#x3D;t^{I}<em>{i+1}-t^{R}</em>{i}$为理想情况下overlap掉的时间</p>
<p>实际可能因为资源限制，没办法全部overlap掉，则实际overlap的比例可近似表示为：</p>
<p>$\gamma_i &#x3D; 1-max(0,\alpha (S^{I}<em>{i}+S^{T}</em>{i})-1,\alpha( B^{I}<em>{i}+B^{T}</em>{i})-1)$，即如果计算($S$)和带宽($B$)都不是瓶颈的条件下为1，否则多出来的部分就需要考虑，最坏情况下为0，串行执行.</p>
<p>重叠后的总时间为：$\Delta t^{I}+\Delta t^{T}-\gamma_i \Delta t + C$</p>
<p>$\alpha \in [0,1]$, 表示底层的调度策略，当$\alpha接近0$时，说明底层难以重叠（如未启用mps），接近1时说明底层调度较好（如mps），$C$为重叠的固定开销</p>
<p>重叠后推理区间调整为$(t^{I}<em>{i},t^{I}</em>{i+1}+(1-\gamma_i) \Delta t+C)$</p>
<p>重叠后训练区间调整为$(t^{T}<em>{i},t^{T}</em>{i+1}+(1-\gamma_i) \Delta t+C)$</p>
<p>通过采集profile数据，可以确定超参数的值，这个模型可以拓展等，我查阅相关资料发现多数做法是查表，建模的较少</p>
<h1 id="三、静态调度"><a href="#三、静态调度" class="headerlink" title="三、静态调度"></a>三、静态调度</h1><p>profile获取推理进程的任务队列$W^I&#x3D;{t^{I}<em>{i},R^I_i}</em>{i&#x3D;0}^{N}$，训练进程的任务队列$W^T&#x3D;{t^{T}<em>{i},R^T_i}</em>{i&#x3D;0}^{M}$,表示一系列任务（kernel group）在$t_i$开始，$t_{i+1}$结束，占据资源$R_i$。</p>
<p>约束：</p>
<p>对于任意的$i \in {0,1,2,…}$，有$t_i&lt;t_{i+1}$</p>
<p>可在队列中任意位置插入空资源任务，即在$t_i$后插入$\Delta t_i&#x3D;t^{’}_j-t_i$，$t^{’}_j$为另一队列的任务起始时间且$t^{’}_j&gt;t_i$，，期间不占用任何资源，插入后，$R_i$的起始时间调整为$t_i+\Delta t_i$，且对于$j&gt;i$,有$t_j&#x3D;t_j+\Delta t_i$</p>
<p>对于推理训练并行后的新队列，重叠后的时间根据第二部分公式估计，并以此调整$t_j,j&#x3D;i+1,…$</p>
<p>通过某种算法，插入空资源任务调整重叠区域，达到$min{max(t_M^T，t_N^I)-min(t_0^T，t_0^I)}$的新队列总时间的目的</p>
<p>有没有类似问题的解法？</p>
<p>可以使用动态规划。</p>
<h1 id="四、调度算法"><a href="#四、调度算法" class="headerlink" title="四、调度算法"></a>四、调度算法</h1><h2 id="状态类型"><a href="#状态类型" class="headerlink" title="状态类型"></a>状态类型</h2><p>记推理区间为 $I_1..I_N$，训练区间为 $T_1..T_M$，每段不可中断。</p>
<p>我们只在以下三类情形定义 DP 状态（决策点）：</p>
<ol>
<li><p><strong>S0：两边都在边界（同时可启动）</strong></p>
<p>记为：<br>$$<br>F(i,j)<br>$$<br>含义：<br> $I_1..I_{i-1}$、$T_1..T_{j-1}$ 已完成，<br> 推理在 $I_i$ 起点（或 $i&#x3D;N$ 已结束），训练在 $T_j$ 起点（或 $j&#x3D;M$ 已结束）。</p>
</li>
<li><p><strong>S1：推理在边界，训练在中途</strong></p>
<p>记为：<br>$$<br>G(i, j, r_T)<br>$$<br>含义：</p>
<ul>
<li>推理：已完成到 $I_{i-1}$，当前停在 $I_i$ 起点（或 $i&#x3D;N$ 已结束），无剩余；</li>
<li>训练：正在执行 $T_j$ 的中间，还有 $r_T&gt;0$ 未完成；</li>
<li>这是“<strong>推理可以决策继续还是 idle，训练只能往前跑</strong>”的状态。</li>
</ul>
</li>
<li><p><strong>S2：训练在边界，推理在中途</strong></p>
<p>记为：<br>$$<br>H(i, j, r_I)<br>$$<br>含义与上对称：</p>
<ul>
<li>训练在 $T_j$ 起点（或结束），推理在 $I_i$ 中间，还有 $r_I&gt;0$；</li>
<li>此时训练可以决策，推理只能继续。</li>
</ul>
</li>
</ol>
<p><strong>没有</strong>“两边都在中途 (推理有剩余 &amp; 训练有剩余)”的 DP 状态：</p>
<ul>
<li>那种情况说明我们已经进入 overlap 段中间，<strong>两边都还没到自己的边界，谁都不能停，也不能决策</strong>；</li>
<li>这一段是<strong>强制执行</strong>，我们直接把它跑到下一个“某一方到边界”的时刻，再落回 S1 &#x2F; S2 &#x2F; S0 之一。</li>
</ul>
<hr>
<h2 id="重叠代价"><a href="#重叠代价" class="headerlink" title="重叠代价"></a>重叠代价</h2><p>对一对区间 $I_i$、$T_j$：<br>$$<br>\gamma_{ij}<br>&#x3D; 1 - \max\bigl(0,; \alpha (S^I_i + S^T_j) - 1,; \alpha (B^I_i + B^T_j) - 1\bigr)<br>\in [0,1].<br>$$<br>在这两段 overlap 的时长为 $x$ 的片段中：<br>$$<br>\text{cost}<em>\text{ov}(i,j,x)<br>&#x3D; \bigl(2 - \gamma</em>{ij}\bigr),x<br>$$<br>如果是从非 overlap 状态新开这段 overlap，可以另外加一次固定开销 $C$（这里不展开）。</p>
<p>solo 时：<br>$$<br>\text{cost}_\text{solo}(x)&#x3D;x.<br>$$<br>总 cost 就是所有片段的 cost 之和。</p>
<hr>
<h2 id="DP-转移（按三种状态写）"><a href="#DP-转移（按三种状态写）" class="headerlink" title="DP 转移（按三种状态写）"></a>DP 转移（按三种状态写）</h2><h3 id="1-状态-S0：-F-i-j-（两边在边界）"><a href="#1-状态-S0：-F-i-j-（两边在边界）" class="headerlink" title="1. 状态 S0：$F(i,j)$（两边在边界）"></a>1. 状态 S0：$F(i,j)$（两边在边界）</h3><p>可选动作：</p>
<ol>
<li><p><strong>I-solo</strong>（只跑下一段推理）</p>
<p>若 $i &lt; N$：<br>$$<br>F(i,j)<br>&#x3D; \min\Bigl(F(i,j),\<br>\Delta t^I_i + F(i+1, j)\Bigr)<br>$$</p>
</li>
<li><p><strong>T-solo</strong>（只跑下一段训练）</p>
<p>若 $j &lt; M$：<br>$$<br>F(i,j)<br>&#x3D; \min\Bigl(F(i,j),<br>\Delta t^T_j + F(i, j+1)\Bigr)<br>$$</p>
</li>
<li><p><strong>Overlap-start</strong>（同时启动 $I_i$ 和 $T_j$）</p>
<p>若 $i &lt; N, j &lt; M$，设：<br>$$<br>a &#x3D; \Delta t^I_i,\quad b &#x3D; \Delta t^T_j,\quad x &#x3D; \min(a,b)<br>$$<br>先跑一段 overlap 长度 $x$，代价：<br>$$<br>c &#x3D; \text{cost}<em>\text{ov}(i,j,x) + C</em>{\text{(可选)}}<br>$$<br>然后：</p>
<ul>
<li><p>若 $a &#x3D; b$：两者同时到边界 → 回到 S0：<br>$$<br>F(i,j) &#x3D; \min\bigl(F(i,j),\ c + F(i+1, j+1)\bigr)<br>$$</p>
</li>
<li><p>若 $a &lt; b$：推理先结束，训练剩余 $b-a$：</p>
<p>进入 S1（推理在边界，训练在中途）：<br>$$<br>F(i,j) &#x3D; \min\bigl(F(i,j),\ c + G(i+1, j, b-a)\bigr)<br>$$</p>
</li>
<li><p>若 $a &gt; b$：训练先结束：</p>
<p>进入 S2：<br>$$<br>F(i,j) &#x3D; \min\bigl(F(i,j),\ c + H(i, j+1, a-b)\bigr)<br>$$</p>
</li>
</ul>
</li>
</ol>
<hr>
<h3 id="2-状态-S1：-G-i-j-r-T-（推理在边界，训练在中途）"><a href="#2-状态-S1：-G-i-j-r-T-（推理在边界，训练在中途）" class="headerlink" title="2. 状态 S1：$G(i,j,r_T)$（推理在边界，训练在中途）"></a>2. 状态 S1：$G(i,j,r_T)$（推理在边界，训练在中途）</h3><p>这里只有训练在跑当前 $T_j$，推理可决策：</p>
<p><strong>动作 A：推理 idle（不启动下一段）</strong></p>
<ul>
<li>训练必须把剩余 $r_T$ 跑完（solo）：<br>$$<br>\text{cost} &#x3D; r_T + F(i, j+1)<br>$$</li>
</ul>
<p><strong>动作 B：推理立刻启动下一段，继续 overlap</strong></p>
<p>若 $i &lt; N$，启动 $I_i$ 与训练剩余 overlap：</p>
<ul>
<li><p>令 $a &#x3D; \Delta t^I_i,\ x &#x3D; \min(a, r_T)$</p>
</li>
<li><p>overlap 段代价：<br>$$<br>c &#x3D; \text{cost}_\text{ov}(i,j,x)<br>$$</p>
</li>
</ul>
<p>然后分三种：</p>
<ol>
<li><p>若 $x &#x3D; r_T &lt; a$：训练先到边界，推理还有 $a-x$：</p>
<p>→ 进入 S2（训练在边界，推理中途）：<br>$$<br>G(i,j,r_T)<br>&#x3D; \min\Bigl(<br> G(i,j,r_T),<br> c + H(i, j+1, a-x)<br>  \Bigr)<br>$$</p>
</li>
<li><p>若 $x &#x3D; a &lt; r_T$：推理先到新边界，训练还有剩余 $r_T - x$：</p>
<p>→ 回到 S1（新一段推理边界，训练仍在中途）：<br>$$<br>G(i,j,r_T)<br>&#x3D; \min\Bigl(<br> G(i,j,r_T),<br> c + G(i+1, j, r_T - x)<br>  \Bigr)<br>$$</p>
</li>
<li><p>若 $x &#x3D; a &#x3D; r_T$：两者同时到边界：<br>$$<br>G(i,j,r_T)<br>&#x3D; \min\Bigl(<br> G(i,j,r_T),<br> c + F(i+1, j+1)<br>  \Bigr)<br>$$</p>
</li>
</ol>
<p>取 A&#x2F;B 的最小值作为 $G(i,j,r_T)$。</p>
<hr>
<h3 id="3-状态-S2：-H-i-j-r-I-（训练在边界，推理在中途）"><a href="#3-状态-S2：-H-i-j-r-I-（训练在边界，推理在中途）" class="headerlink" title="3. 状态 S2：$H(i,j,r_I)$（训练在边界，推理在中途）"></a>3. 状态 S2：$H(i,j,r_I)$（训练在边界，推理在中途）</h3><p>与 S1 完全对称，交换 I&#x2F;T 即可：</p>
<ul>
<li><p>动作 A：训练 idle，让推理把剩余 $r_I$ solo 跑完：<br>$$<br>\text{cost} &#x3D; r_I + F(i+1, j)<br>$$</p>
</li>
<li><p>动作 B：训练启动下一段 $T_j$，与推理剩余 overlap：</p>
<ul>
<li>设 $b &#x3D; \Delta t^T_j,\ x &#x3D; \min(b, r_I)$</li>
<li>代价 $c &#x3D; \text{cost}_\text{ov}(i,j,x)$</li>
<li>再根据谁先结束，转到 H &#x2F; G &#x2F; F 中相应状态。</li>
</ul>
</li>
</ul>
<hr>
<h3 id="4-终止条件"><a href="#4-终止条件" class="headerlink" title="4. 终止条件"></a>4. 终止条件</h3><ul>
<li><p>当 $i &#x3D; N, j &#x3D; M$ 且无剩余时：<br>$$<br>F(N,M) &#x3D; 0<br>$$</p>
</li>
<li><p>当只剩一边队列（比如 $i &#x3D; N$ 但训练还有）时，S1&#x2F;S2 退化为连续 solo，DP 会自动给出：<br>$$<br>G(N,j,r_T) &#x3D; r_T + \sum_{k&gt;j} \Delta t^T_k<br>$$<br>等价地，对应到 F 的递推中。</p>
</li>
</ul>
<p>起始状态：<br>$$<br>T_{\text{opt}} &#x3D; F(0,0)<br>$$<br>回溯各状态中选过的动作（I-solo &#x2F; T-solo &#x2F; overlap &#x2F; idle），即可得到一条最优调度时间线。</p>
<h2 id="指标"><a href="#指标" class="headerlink" title="指标"></a>指标</h2><ol>
<li><p>调度后的总时间：时间越短越好，是我们的主要优化目标</p>
</li>
<li><p>平均资源利用率：对于$S$和$B$，需要计算调度后的平均值，方法为累加各类型（推理 or 训练）各时间片的$Time \times Resouse$,再除以总时间</p>
</li>
</ol>
<p>工作负载不变的情况下，总时间越短，资源利用率越高。</p>
<h2 id="基线"><a href="#基线" class="headerlink" title="基线"></a>基线</h2><ol>
<li><p>基线1：不做任何调度，即推理和训练均不间断执行，直至结束。</p>
</li>
<li><p>基线2：先做推理，再做训练，不重叠，直至结束。</p>
</li>
</ol>
<h2 id="展示"><a href="#展示" class="headerlink" title="展示"></a>展示</h2><p>对于基线1，2和我们DP后的最优方案，均给出：</p>
<ol>
<li><p>指标：包括总时间和平均资源利用率。</p>
</li>
<li><p>调度图示：横轴是时间，纵轴分两行（推理和训练），上下两行均由扁平长方形（时间轴为长边）对齐短边拼接而成。推理执行时用蓝色表示，训练执行时用红色表示，空闲时均用白色表示，长方形上标注占用的资源情况($S，B$)。这里展示的是考虑了cost的时间线，即如果重叠区域的cost大于1，则拉长重叠区域。</p>
</li>
</ol>
<h1 id="工程实现"><a href="#工程实现" class="headerlink" title="工程实现"></a>工程实现</h1><h2 id="系统设计"><a href="#系统设计" class="headerlink" title="系统设计"></a>系统设计</h2><h3 id="1-数据收集"><a href="#1-数据收集" class="headerlink" title="1. 数据收集"></a>1. 数据收集</h3><p>在推理引擎(vllm)或训练引擎(megatron)中插桩，记录${time,name}$表示下面一段代码（任务）的起始时间与任务名称。</p>
<p>由此，我们可以采用基线2收集推理进程与训练进程单独执行时各任务阶段的时间；通过基线1收集推理和训练不做调度并行后各任务的时间</p>
<p>对于资源占用信息$R$，可以通过Nsight收集，或者依据任务名称$name$直接指定（如分为计算${S&#x3D;1,B&#x3D;0}$或iO通信${S&#x3D;0,B&#x3D;1}$）</p>
<h3 id="2-参数估计"><a href="#2-参数估计" class="headerlink" title="2. 参数估计"></a>2. 参数估计</h3><p>基于1收集的数据（包含独立的推理\训练，重叠的推理\训练），通过回归的方式估计超参数$\alpha,C$，也可以对于任务的超参数$S,B$估计，可以使用EM算法迭代估计</p>
<h3 id="3-调度优化"><a href="#3-调度优化" class="headerlink" title="3. 调度优化"></a>3. 调度优化</h3><p>基于DP算法，得出最优的调度策略与理论最大加速空间。给出：</p>
<ol>
<li><p>推理在每个决策点根据当前推理$i$和训练$j$，决定挂起(idle)或执行</p>
</li>
<li><p>训练在每个决策点根据当前推理$i$和训练$j$，决定挂起(idle)或执行</p>
</li>
</ol>
<p>推理和训练共享一份状态，以共享内存的形式存在。</p>
<p>如果采用任务名称直接指定的形式，可以做到在线收集信息，当偏离理论最大加速空间时，周期性执行1,2,3，不断调整调度策略，收集数据的时间很小（只要记录即可），而参数估计和DP算法可以与RL算法并行，不会阻塞。</p>
<h2 id="问题"><a href="#问题" class="headerlink" title="问题"></a>问题</h2><p>当多机多卡环境调度时，有可能出现死锁问题：</p>
<p>A进程决定idle或继续的决策根据调度策略，依据目前A进程的进度（容易获取）与B进程的进度（需通过共享内存获取），然而，不同GPU之间相同进程也需要NCCL通信，如果各个GPU之间的推理和训练进度不一致，有可能出现0号GPU的A进程在等待1号GPU的A进程通信，而1号GPU的A进程在等待本GPU的B进程达到某一进度，1号GPU的B进程却在等待0号GPU的B进程通信，而0号B进程在等待本GPU的通信。</p>
<p>解决方案：</p>
<p>只需要固定时间较长的进程，不让它任何时候idle，而仅调度较短的进程，让较长进程永不死锁，则较短的进程也不会死锁，它可以稳定获取本进程的进度和较长进程的进度。但是需要证明这种调度方案与DP的最优方案等价或近似最优。</p>
<h3 id="证明："><a href="#证明：" class="headerlink" title="证明："></a>证明：</h3><p>如果有：</p>
<ol>
<li><p>对任意一对区间 $(i,j)$ 和 overlap 长度 $x$，都有：<br>$$<br>\text{cost}<em>{overlap}(i,j,x) \le \text{cost}</em>{solo}(i,x) + \text{cost}_{solo}(j,x)<br>$$<br>也就是“重叠不会比把这两段串行跑更贵”（否则我们本来也不该重叠它们）；</p>
</li>
<li><p>cost 只依赖于配对本身，和时间位置无关（当前模型是这样）；</p>
</li>
<li><p>不考虑奇怪的二级目标（比如专门给某方让路导致延迟奖励之类）。</p>
</li>
</ol>
<p>那么可以用经典的 <strong>non-delay &#x2F; dominant schedule</strong> 思路证明：</p>
<blockquote>
<p>对于以总完成时间为目标的两作业调度问题，总存在一个最优解，使得最终成为 critical 的那条作业不会被“策略性 idle”（即有事情做时不被调度器强行停住）。</p>
</blockquote>
<p>证明的关键是：</p>
<ul>
<li><p>如果 critical 那条在某个时间段被强制 idle，而另一条在干活，</p>
</li>
<li><p>通常可以把它后面的部分“拉”过来（作为 solo 或 overlap）填进去，使总时间不变或更小；</p>
</li>
<li><p>在「重叠永不更差」这个前提下，这种变换不会变坏。</p>
</li>
</ul>
<p>因此，这种调度方案与DP的最优等价，当C很小时，近似等价</p>
<p>即：</p>
<p>取一任意最优调度 $S^*$，其 makespan 为 $T^*$。<br> 设在 $S^*$ 中，作业 $L \in {I,T}$ 是关键作业：$C_L &#x3D; T^*$，另一条记为 $S$。</p>
<p>若 $L$ 在 $S^*$ 中从未出现“有未完成工作却被调度 idle”的情况，则结论成立。</p>
<p>否则，取时间轴上最早的一个区间 $[a,b)$，满足：</p>
<ul>
<li>在 $[a,b)$ 内，$L$ 仍有后续段尚未执行完；</li>
<li>$L$ 在整个 $[a,b)$ 内未执行（被 idle）；</li>
<li>区间内 GPU 可能在跑 $S$ 或空闲。</li>
</ul>
<p>由于 $L$ 后面还有执行，必存在某个后续区间 $[c,d)$（$c \ge b$）中 $L$ 实际被执行。<br> 从这些后续执行中截取总长度为 $\Delta &#x3D; b-a$ 的前缀，记为“L-工作块”。</p>
<p>构造新调度 $\tilde{S}$：</p>
<ul>
<li>将这段 “L-工作块”(长度 $\Delta$) 从 $[c,d)$ 等后续位置抽出，平移到 $[a,b)$；</li>
<li>原本在 $[a,b)$ 的 S-执行（若存在）整体顺延到 $[c’,d’)$ 等更靠后位置，保持 $S$ 内部顺序；</li>
<li>在平移&#x2F;重排过程中：<ul>
<li>不改变任一作业内部的段顺序；</li>
<li>在有需要时允许把部分 L 与 S 重叠执行（即把原本串行的一部分改为 overlap）。</li>
</ul>
</li>
</ul>
<p>由于我们的关键假设是：对于任意局部长度 $x$，overlap 的代价不大于串行代价 $2x$，<br> 上述局部变换可以做到：</p>
<ul>
<li>受影响时间区间的总长度不增加；</li>
<li>$L$ 的完成时间不变或提前（因为其部分执行被提前到了 $[a,b)$）；</li>
<li>$S$ 的完成时间不早于原先（可能被略微后移），但只要 $L$ 是关键作业，即 $C_S \le C_L$，这种微调不会推高 makespan。</li>
</ul>
<p>因为我们选择的是“最早的 idle 区间 $[a,b)$”，该变换不会制造新的更早 idle，且严格减少了“L 在有剩余工作时被 idle 的总时长”。</p>
<p>重复上述步骤有限次后，我们得到一个新的调度 $\tilde{S}$：</p>
<ul>
<li><p>makespan 仍为 $T^*$（不可能更大，因为 $S^*$ 已是最优）；</p>
</li>
<li><p>关键作业 $L$ 再也不会在“有工作可做”时被主动 idle。</p>
</li>
</ul>
<p>相当于对原算法做了剪枝</p>
<h3 id="数据记录"><a href="#数据记录" class="headerlink" title="数据记录"></a>数据记录</h3><p>在RL同步算法中，即便训推并行，一般也是推理-并行…-并行-训练，因此给我们记录推理和训练solo与并行数据的条件</p>
<p>但是，各进程需要在记录的起始点保持严格的同步，不然无法获得有效的overlap区间。</p>
<p>或者，我们可以加入绝对时间，允许DP算法中infer和train并非同一时间起始；只记录它们起始时间差值也是合理的</p>
<p>需要把数据记录的代码与之前控制推理\训练空闲或继续执行的代码合并，具体思路为：<br>通过共享内存做同一GPU的信息交换媒介，交换起始时间，任务执行进度。</p>
<p>当DP获得最优调度方案后，可让推理与训练进程按照调度方案挂起或执行</p>
<p>允许控制是否执行数据记录或调度</p>
<h3 id="回归参数-alpha"><a href="#回归参数-alpha" class="headerlink" title="回归参数$\alpha$"></a>回归参数$\alpha$</h3><p>我们的cost函数为：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">gamma_ij</span>(<span class="params">si: Seg, sj: Seg, alpha:<span class="built_in">float</span></span>) -&gt; <span class="built_in">float</span>:</span><br><span class="line">    <span class="keyword">if</span> si.S*sj.S == <span class="number">0.0</span>:</span><br><span class="line">        <span class="keyword">return</span> <span class="number">1.0</span></span><br><span class="line">    <span class="keyword">return</span> alpha</span><br></pre></td></tr></table></figure>

<p>我们通过数据采样获取了infer.json和train.json，其中包含：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">out: <span class="type">Dict</span>[<span class="built_in">str</span>, <span class="type">Any</span>] = &#123;</span><br><span class="line">    <span class="string">&quot;solo_infer&quot;</span>: [asdict(s) <span class="keyword">for</span> s <span class="keyword">in</span> <span class="variable language_">self</span>.solo_infer_avg],</span><br><span class="line">    <span class="string">&quot;solo_train&quot;</span>: [asdict(s) <span class="keyword">for</span> s <span class="keyword">in</span> <span class="variable language_">self</span>.solo_train_avg],</span><br><span class="line">    <span class="string">&quot;parallel_infer&quot;</span>: [asdict(s) <span class="keyword">for</span> s <span class="keyword">in</span> <span class="variable language_">self</span>.parallel_infer_avg],</span><br><span class="line">    <span class="string">&quot;parallel_train&quot;</span>: [asdict(s) <span class="keyword">for</span> s <span class="keyword">in</span> <span class="variable language_">self</span>.parallel_train_avg],</span><br><span class="line">&#125;</span><br><span class="line"><span class="comment"># ★ 写入 offset（并行时有效）</span></span><br><span class="line"><span class="keyword">if</span> <span class="variable language_">self</span>.parallel_infer_offset_avg <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">    out[<span class="string">&quot;parallel_infer_offset_ns&quot;</span>] = <span class="variable language_">self</span>.parallel_infer_offset_avg</span><br><span class="line"><span class="keyword">if</span> <span class="variable language_">self</span>.parallel_train_offset_avg <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">    out[<span class="string">&quot;parallel_train_offset_ns&quot;</span>] = <span class="variable language_">self</span>.parallel_train_offset_avg</span><br><span class="line">    </span><br><span class="line"><span class="keyword">with</span> <span class="built_in">open</span>(filepath, <span class="string">&quot;w&quot;</span>) <span class="keyword">as</span> f:</span><br><span class="line">    json.dump(out, f, indent=<span class="number">2</span>, ensure_ascii=<span class="literal">False</span>)</span><br><span class="line"><span class="variable language_">self</span>.reset()</span><br></pre></td></tr></table></figure>

<p>数据样例：</p>
<figure class="highlight json"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line">  <span class="attr">&quot;solo_infer&quot;</span><span class="punctuation">:</span> <span class="punctuation">[</span></span><br><span class="line">    <span class="punctuation">&#123;</span></span><br><span class="line">      <span class="attr">&quot;name&quot;</span><span class="punctuation">:</span> <span class="string">&quot;IO_cpu_to_gpu&quot;</span><span class="punctuation">,</span></span><br><span class="line">      <span class="attr">&quot;dt&quot;</span><span class="punctuation">:</span> <span class="number">0.05300827126484364</span><span class="punctuation">,</span></span><br><span class="line">      <span class="attr">&quot;S&quot;</span><span class="punctuation">:</span> <span class="number">0.0</span><span class="punctuation">,</span></span><br><span class="line">      <span class="attr">&quot;B&quot;</span><span class="punctuation">:</span> <span class="number">0.0</span></span><br><span class="line">    <span class="punctuation">&#125;</span><span class="punctuation">,</span></span><br><span class="line">    <span class="punctuation">&#123;</span></span><br><span class="line">      <span class="attr">&quot;name&quot;</span><span class="punctuation">:</span> <span class="string">&quot;CP_infer_model_forward&quot;</span><span class="punctuation">,</span></span><br><span class="line">      <span class="attr">&quot;dt&quot;</span><span class="punctuation">:</span> <span class="number">0.008826685370877385</span><span class="punctuation">,</span></span><br><span class="line">      <span class="attr">&quot;S&quot;</span><span class="punctuation">:</span> <span class="number">1.0</span><span class="punctuation">,</span></span><br><span class="line">      <span class="attr">&quot;B&quot;</span><span class="punctuation">:</span> <span class="number">0.0</span></span><br><span class="line">    <span class="punctuation">&#125;</span><span class="punctuation">,</span></span><br><span class="line">      ...</span><br><span class="line">       <span class="attr">&quot;parallel_train&quot;</span><span class="punctuation">:</span> <span class="punctuation">[</span></span><br><span class="line">    <span class="punctuation">&#123;</span></span><br><span class="line">      <span class="attr">&quot;name&quot;</span><span class="punctuation">:</span> <span class="string">&quot;IO_prepare_data&quot;</span><span class="punctuation">,</span></span><br><span class="line">      <span class="attr">&quot;dt&quot;</span><span class="punctuation">:</span> <span class="number">0.0180000007385388</span><span class="punctuation">,</span></span><br><span class="line">      <span class="attr">&quot;S&quot;</span><span class="punctuation">:</span> <span class="number">0.0</span><span class="punctuation">,</span></span><br><span class="line">      <span class="attr">&quot;B&quot;</span><span class="punctuation">:</span> <span class="number">0.0</span></span><br><span class="line">    <span class="punctuation">&#125;</span><span class="punctuation">,</span></span><br><span class="line">    <span class="punctuation">&#123;</span></span><br><span class="line">      <span class="attr">&quot;name&quot;</span><span class="punctuation">:</span> <span class="string">&quot;CP_train&quot;</span><span class="punctuation">,</span></span><br><span class="line">      <span class="attr">&quot;dt&quot;</span><span class="punctuation">:</span> <span class="number">0.04141549684572965</span><span class="punctuation">,</span></span><br><span class="line">      <span class="attr">&quot;S&quot;</span><span class="punctuation">:</span> <span class="number">1.0</span><span class="punctuation">,</span></span><br><span class="line">      <span class="attr">&quot;B&quot;</span><span class="punctuation">:</span> <span class="number">0.0</span></span><br><span class="line">    <span class="punctuation">&#125;</span><span class="punctuation">,</span></span><br><span class="line">    <span class="punctuation">&#123;</span></span><br><span class="line">      <span class="attr">&quot;name&quot;</span><span class="punctuation">:</span> <span class="string">&quot;CN_alltoall&quot;</span><span class="punctuation">,</span></span><br><span class="line">      <span class="attr">&quot;dt&quot;</span><span class="punctuation">:</span> <span class="number">0.0003283898113295436</span><span class="punctuation">,</span></span><br><span class="line">      <span class="attr">&quot;S&quot;</span><span class="punctuation">:</span> <span class="number">0.0</span><span class="punctuation">,</span></span><br><span class="line">      <span class="attr">&quot;B&quot;</span><span class="punctuation">:</span> <span class="number">1.0</span></span><br><span class="line">    <span class="punctuation">&#125;</span><span class="punctuation">,</span></span><br><span class="line">...</span><br><span class="line"><span class="punctuation">]</span><span class="punctuation">,</span></span><br><span class="line">  <span class="attr">&quot;parallel_train_offset_ns&quot;</span><span class="punctuation">:</span> <span class="number">0.46194177175</span></span><br></pre></td></tr></table></figure>

<p>infer.json中只有infer的数据，包含”solo_infer”、”parallel_infer”和”parallel_infer_offset_ns”，其他项为空。</p>
<p>train.json中只有train的数据，包含”train_infer”、”parallel_train”和”parallel_train_offset_ns”，其他项为空。</p>
<p>希望通过比较solo和parallel，结合offset_ns（parallel_infer_offset_ns恒为0），估计衡量重叠效率的$\alpha$</p>
<p>再根据solo、$\alpha$和offset_ns，使用DP算法（受约束的剪枝band c++）给出timeline，然后回溯最优的调度方案（即policy），包括：</p>
<p>“状态 -&gt; 动作”的 policy，即不论对训练进程还是推理进程：</p>
<p>它们可以获取的信息有：</p>
<ol>
<li><p>当前是否在并行，不在并行就一直跑</p>
</li>
<li><p>在并行，根据本进程目前的进度（通过mark计数）与并行的另一个进程的进度（假设可以获取），决定是挂起还是继续执行</p>
</li>
<li><p>对于挂起，要根据另一个进程的进度终止</p>
</li>
</ol>
<p>对于长任务永不挂起的场景，只有短任务需要决策，保证安全性。</p>
<p>你需实现：</p>
<p>python代码读取数据估计超参数$\alpha$，估计误差（即估计的超参数估计的并行时间与实际测量的百分比误差）</p>
<p>使用我们之前的最新版DP算法（受约束的剪枝band c++）获取最优调度方案，并根据timeline给出此方案的policy，通过总时间给出与两个基线（solo和parallel，即采样数据）相比的加速比</p>
<h3 id="合并数据记录-logger-与调度器-scheduler）"><a href="#合并数据记录-logger-与调度器-scheduler）" class="headerlink" title="合并数据记录(logger)与调度器(scheduler）"></a>合并数据记录(logger)与调度器(scheduler）</h3><p>算法复杂度还是太高了，必须用启发式的算法</p>
<p>假设我们跳过估计$\alpha$这类超参数呢？</p>
<h2 id="新的想法"><a href="#新的想法" class="headerlink" title="新的想法"></a>新的想法</h2><p>我们的思路是推理的每个token计算步(step)插入到训练的非计算空隙（通信、IO等），最大化SM活跃率，缩短推理-训练并行时间。</p>
<h4 id="前提："><a href="#前提：" class="headerlink" title="前提："></a>前提：</h4><p>推理和训练的任务是稳态的，即batchsize、max_response_length等预先指定</p>
<h4 id="状态："><a href="#状态：" class="headerlink" title="状态："></a>状态：</h4><p>共享内存存储：</p>
<ol>
<li><p>event_handle: 训练进程python解释器当前执行位置上文最近的event的handle</p>
</li>
<li><p>status：训练进程python解释器当前执行区域的性质（允许插入或不允许插入）</p>
</li>
<li><p>train_steps：训练进程使用torch.cuda.event record的次数</p>
</li>
<li><p>infer_steps: 推理进程执行的次数</p>
</li>
</ol>
<h4 id="做法："><a href="#做法：" class="headerlink" title="做法："></a>做法：</h4><p>训练在区域的开始创建 torch.cuda.event(interprocess&#x3D;True), 并将idle_start.ipc_handle()写入共享内存的相应区域（初始值为0），同时写入区域的性质idle（1表示允许插入，0表示不允许插入，初始值为1），同时train_steps自增1:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">loggerscheduler = get_global_LoggerScheduler()</span><br><span class="line">loggerscheduler.mark(idle=<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">LoggerScheduler</span>:</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self,shem</span>):</span><br><span class="line">        init(shem)</span><br><span class="line">        <span class="variable language_">self</span>.event = torch.cuda.event(interprocess=<span class="literal">True</span>)</span><br><span class="line">        sekf.train_steps = <span class="number">0</span></span><br><span class="line">        <span class="variable language_">self</span>.idle = <span class="number">1</span></span><br><span class="line">        <span class="keyword">pass</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">mark</span>(<span class="params">self,idle:<span class="built_in">int</span> = <span class="number">1</span></span>):</span><br><span class="line">        <span class="variable language_">self</span>.idle = idle</span><br><span class="line">        event.record()</span><br><span class="line">        <span class="variable language_">self</span>.train_steps += <span class="number">1</span></span><br><span class="line">        write_to_shared_memory(<span class="variable language_">self</span>.event.ipc_handle(),<span class="variable language_">self</span>.train_steps,<span class="variable language_">self</span>.idle)</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">end</span>(<span class="params">self</span>):</span><br><span class="line">        init(shem)</span><br><span class="line">   	</span><br><span class="line">        </span><br></pre></td></tr></table></figure>



<p>推理在每次step前，检查ipc_handle的值：</p>
<p>为0：直接执行，infer_steps自增1; </p>
<p>不为0，表明训练已经开始：读入idle的值，进入循环</p>
<p>idle &#x3D; 1: event.wait() ,break，在idle区域，等待GPU执行到空闲区域就插入</p>
<p>idle &#x3D; 0: event.qurey(), False的时候break，真正当GPU执行到计算区域的时候才放弃发射kernel</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">LoggerScheduler</span>:</span><br><span class="line">    ...</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">check</span>(<span class="params">self</span>):</span><br><span class="line">        event_handle=get_shared_ipc() </span><br><span class="line">        <span class="keyword">while</span> event_handle!=<span class="number">0</span>:</span><br><span class="line">            event = torch.cuda.Event.from_ipc_handle(device=<span class="variable language_">self</span>.rank, handle=event_handle)</span><br><span class="line">            <span class="keyword">if</span> get_shared_idle==<span class="number">1</span>:</span><br><span class="line">                event.wait()</span><br><span class="line">                <span class="keyword">break</span></span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                <span class="keyword">if</span> !event.qurey():</span><br><span class="line">                    <span class="keyword">break</span></span><br></pre></td></tr></table></figure>







<h1 id="工作记录"><a href="#工作记录" class="headerlink" title="工作记录"></a>工作记录</h1><p>10.29: 实现了adaptive schedule,发现schedule总有GPU莫名奇妙sm 打满,换成不调度的发现是OOM,如果不调度的不OOM,那么调度也可以跑.另外,为什么双机时用一台机器会OOM而单机同样配置就不会?</p>
<p>10.30: mps开启后收益挺大的,比我个人手动在python层面调度大挺多,sad</p>

</article>
    
    

</div>
<div class="trm-post-next-prev row">
    <div class="col-lg-12">
        <!-- title -->
        <h5 class="trm-title-with-divider">
            其他文章
            <span data-number="02"></span>
        </h5>
    </div>
    
        <div class="col-lg-6">
    <div class="trm-blog-card trm-scroll-animation">
        <a href="/2025/11/04/parellel-report2/" class="trm-cover-frame trm-anima-link">
            
            
                <img alt="cover" class="no-fancybox" src="/img/block.jpg">
            
        </a>
        
        <div class="trm-card-descr">
            <div class="trm-label trm-category trm-mb-20">
                <a href=" #.">
                    未分类
                </a>
            </div>
            <h5>
                <a href="/2025/11/04/parellel-report2/" class="trm-anima-link">
                    
                </a>
            </h5>
            <div class="trm-divider trm-mb-20 trm-mt-20"></div>
            <ul class="trm-card-data trm-label">
                <li>25/11/04</li>
                <li>21:16</li>
                
                
            </ul>
        </div>
    </div>
</div>
    
    
        <div class="col-lg-6">
    <div class="trm-blog-card trm-scroll-animation">
        <a href="/2025/10/20/solve-extra-overhead/" class="trm-cover-frame trm-anima-link">
            
            
                <img alt="cover" class="no-fancybox" src="https://www.logosc.cn/uploads/resources/2018/11/29/1543459457_thumb.jpg">
            
        </a>
        
        <div class="trm-card-descr">
            <div class="trm-label trm-category trm-mb-20">
                <a href=" /categories/RLHF/">
                    RLHF
                </a>
            </div>
            <h5>
                <a href="/2025/10/20/solve-extra-overhead/" class="trm-anima-link">
                    solve_extra_overhead
                </a>
            </h5>
            <div class="trm-divider trm-mb-20 trm-mt-20"></div>
            <ul class="trm-card-data trm-label">
                <li>25/10/20</li>
                <li>13:55</li>
                
                
            </ul>
        </div>
    </div>
</div>
    
</div>

    



                    <div class="trm-divider footer-divider"></div>

                    <!-- footer -->
                    <footer class="trm-footer-card trm-scroll-animation">

    

    

    
        <div class="trm-footer-item">
            <span>
                由 <a href="https://hexo.io" target="_blank" rel="noopener">Hexo</a> 驱动 v7.3.0
            </span>
            <span class="footer-separator" data-separator=" | "></span>
            <span> 
                主题 - 
                <a rel="noopener" href='https://github.com/MaLuns/hexo-theme-async' target='_blank'>Async</a>
                v2.2.6
            </span>
        </div>
      

     

     
</footer>
                    <!-- footer end -->

                </div>
            </div>
        </div>
    </div>
</div>
            <!-- body end -->

            

            
<div class="trm-fixed-container">
    
    
        <div class="trm-fixed-btn" data-title="阅读模式" onclick="asyncFun.switchReadMode()">
            <i class="iconfont fas fa-book-reader"></i>
        </div>
    
    
    <div id="trm-back-top" class="trm-fixed-btn" data-title="回到顶部">
        <i class="iconfont fas fa-arrow-up"></i>
    </div>
</div>
        </div>
      </div>
      <!-- scroll container end -->
  </div>
  <!-- app wrapper end -->

  
  <!-- Plugin -->




    
    
<script src="https://unpkg.com/@fancyapps/ui@4.0/dist/fancybox.umd.js"></script>

    

    

    

    <!-- 数学公式 -->
    

    <!-- 评论插件 -->
    
        

        
    

		




    <!-- Service Worker -->
    
    <!-- baidu push -->
    


<script id="async-script" src="/js/main.js?v=2.2.6"></script>

<!-- CDN -->


    

    

    



</body>

</html>